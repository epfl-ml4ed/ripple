{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b09a6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c0b9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d6b2034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/../ML4ED/Raindrop/code/baselines\n"
     ]
    }
   ],
   "source": [
    "%cd Raindrop/code/baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a5c899a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py39/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "wandb = False\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix, average_precision_score, precision_score, recall_score, f1_score\n",
    "from models import TransformerModel, TransformerModel2, SEFT\n",
    "from utils_phy12 import *\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424f1655",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3aedcf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09e8532d",
   "metadata": {},
   "source": [
    "# SEFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2763473d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MOOCs_list = [\n",
    "#'villesafricaines_002.csv',\n",
    "# 'villesafricaines_003.csv',\n",
    "# 'microcontroleurs_004.csv',\n",
    "# 'dsp_004.csv',\n",
    " 'hwts_001.csv',\n",
    " 'dsp_001.csv',\n",
    " 'progfun_002.csv',\n",
    " 'microcontroleurs_003.csv',\n",
    " 'geomatique_003.csv',\n",
    " 'villesafricaines_001.csv',\n",
    " 'progfun_003.csv',\n",
    " 'dsp_002.csv',\n",
    " 'structures_002.csv',\n",
    " 'initprogcpp_001.csv',\n",
    " 'analysenumerique_003.csv',\n",
    " 'microcontroleurs_006.csv',\n",
    " 'dsp_005.csv',\n",
    " 'hwts_002.csv',\n",
    " 'dsp_006.csv',\n",
    " 'analysenumerique_002.csv',\n",
    " 'structures_003.csv',\n",
    " 'microcontroleurs_005.csv',\n",
    " 'venture_001.csv',\n",
    " 'analysenumerique_001.csv',\n",
    " 'cpp_fr_001.csv',\n",
    " 'structures_001.csv'\n",
    "]\n",
    "MOOCs_list = [i.replace(\"_\", \"-\").split('.')[0] for i in MOOCs_list]\n",
    "\n",
    "dims4 = [\n",
    "#12,\n",
    "# 12,\n",
    "# 13,\n",
    "# 12,\n",
    " 12,\n",
    " 6,\n",
    " 12,\n",
    " 13,\n",
    " 12,\n",
    " 12,\n",
    " 12,\n",
    " 12,\n",
    " 13,\n",
    " 13,\n",
    " 6,\n",
    " 13,\n",
    " 12,\n",
    " 12,\n",
    " 12,\n",
    " 12,\n",
    " 13,\n",
    " 13,\n",
    " 12,\n",
    " 6,\n",
    " 13,\n",
    " 12]\n",
    "\n",
    "dims6 = [\n",
    "#12,\n",
    "# 12,\n",
    "# 13,\n",
    "# 12,\n",
    " 12,\n",
    " 6,\n",
    " 12,\n",
    " 13,\n",
    " 12,\n",
    " 12,\n",
    " 12,\n",
    " 12,\n",
    " 13,\n",
    " 13,\n",
    " 6,\n",
    " 13,\n",
    " 12,\n",
    " 12,\n",
    " 13,\n",
    " 12,\n",
    " 13,\n",
    " 13,\n",
    " 12,\n",
    " 6,\n",
    " 13,\n",
    " 12]\n",
    "dims = {40: dims4, 60: dims6}\n",
    "data_path = '/../data'\n",
    "percentile = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60393a2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset used:  P12\n",
      "Split id: 1\n",
      "1074 138 136 1074 138 136\n",
      "[507] [64] [61]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1074, 1000, 24]) torch.Size([1074, 9]) torch.Size([1074, 1000, 1]) torch.Size([1074])\n",
      "- - Run 1 - -\n",
      "Stop epochs: 25, Batches/epoch: 8, Total batches: 200\n",
      "Validation: Epoch 0,  val_loss:0.6902, aupr_val: 46.38, auc_val: 48.12\n",
      "**[S] Epoch 0, aupr_val: 46.3777, auc_val: 48.1208 **\n",
      "Validation: Epoch 1,  val_loss:0.6915, aupr_val: 45.63, auc_val: 46.81\n",
      "Validation: Epoch 2,  val_loss:0.6924, aupr_val: 45.48, auc_val: 46.05\n",
      "Epoch 00003: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Validation: Epoch 3,  val_loss:0.6941, aupr_val: 45.49, auc_val: 46.07\n",
      "Validation: Epoch 4,  val_loss:0.6938, aupr_val: 45.55, auc_val: 46.11\n",
      "Epoch 00005: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Validation: Epoch 5,  val_loss:0.6937, aupr_val: 45.57, auc_val: 46.14\n",
      "Validation: Epoch 6,  val_loss:0.6937, aupr_val: 45.55, auc_val: 46.09\n",
      "Epoch 00007: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Validation: Epoch 7,  val_loss:0.6937, aupr_val: 45.55, auc_val: 46.09\n",
      "Validation: Epoch 8,  val_loss:0.6937, aupr_val: 45.55, auc_val: 46.09\n",
      "Epoch 00009: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Validation: Epoch 9,  val_loss:0.6937, aupr_val: 45.55, auc_val: 46.09\n",
      "Validation: Epoch 10,  val_loss:0.6937, aupr_val: 45.55, auc_val: 46.09\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Validation: Epoch 11,  val_loss:0.6937, aupr_val: 45.55, auc_val: 46.09\n",
      "Validation: Epoch 12,  val_loss:0.6937, aupr_val: 45.55, auc_val: 46.09\n",
      "Validation: Epoch 13,  val_loss:0.6937, aupr_val: 45.55, auc_val: 46.09\n",
      "Validation: Epoch 14,  val_loss:0.6937, aupr_val: 45.55, auc_val: 46.09\n",
      "Validation: Epoch 15,  val_loss:0.6937, aupr_val: 45.55, auc_val: 46.09\n",
      "Validation: Epoch 16,  val_loss:0.6937, aupr_val: 45.55, auc_val: 46.09\n",
      "Validation: Epoch 17,  val_loss:0.6937, aupr_val: 45.55, auc_val: 46.09\n",
      "Validation: Epoch 18,  val_loss:0.6937, aupr_val: 45.55, auc_val: 46.09\n",
      "Validation: Epoch 19,  val_loss:0.6937, aupr_val: 45.55, auc_val: 46.09\n",
      "Validation: Epoch 20,  val_loss:0.6937, aupr_val: 45.55, auc_val: 46.09\n",
      "Validation: Epoch 21,  val_loss:0.6937, aupr_val: 45.55, auc_val: 46.09\n",
      "Validation: Epoch 22,  val_loss:0.6937, aupr_val: 45.55, auc_val: 46.09\n",
      "Validation: Epoch 23,  val_loss:0.6937, aupr_val: 45.55, auc_val: 46.09\n",
      "Validation: Epoch 24,  val_loss:0.6937, aupr_val: 45.55, auc_val: 46.09\n",
      "Total Time elapsed: 0.893 mins\n",
      "     course  percentile       acc       bac       auc    auprc\n",
      "0  hwts-001          40  0.528571  0.502878  0.582545  0.51487\n",
      "     course  percentile       acc       bac       auc     auprc\n",
      "0  hwts-001          40  0.521429  0.487664  0.494243  0.462514\n",
      "Dataset used:  P12\n",
      "Split id: 1\n",
      "1104 139 140 1104 139 140\n",
      "[512] [64] [64]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1104, 1000, 24]) torch.Size([1104, 9]) torch.Size([1104, 1000, 1]) torch.Size([1104])\n",
      "- - Run 1 - -\n",
      "Stop epochs: 25, Batches/epoch: 9, Total batches: 225\n",
      "Validation: Epoch 0,  val_loss:0.6904, aupr_val: 46.93, auc_val: 50.33\n",
      "**[S] Epoch 0, aupr_val: 46.9319, auc_val: 50.3333 **\n",
      "Validation: Epoch 1,  val_loss:0.6904, aupr_val: 47.27, auc_val: 50.50\n",
      "**[S] Epoch 1, aupr_val: 47.2732, auc_val: 50.5000 **\n",
      "Validation: Epoch 2,  val_loss:0.6917, aupr_val: 48.18, auc_val: 51.33\n",
      "**[S] Epoch 2, aupr_val: 48.1775, auc_val: 51.3333 **\n",
      "Validation: Epoch 3,  val_loss:0.6895, aupr_val: 49.39, auc_val: 52.71\n",
      "**[S] Epoch 3, aupr_val: 49.3904, auc_val: 52.7083 **\n",
      "Validation: Epoch 4,  val_loss:0.6880, aupr_val: 50.19, auc_val: 54.23\n",
      "**[S] Epoch 4, aupr_val: 50.1866, auc_val: 54.2292 **\n",
      "Validation: Epoch 5,  val_loss:0.6841, aupr_val: 51.27, auc_val: 57.06\n",
      "**[S] Epoch 5, aupr_val: 51.2689, auc_val: 57.0625 **\n",
      "Validation: Epoch 6,  val_loss:0.6816, aupr_val: 52.52, auc_val: 59.50\n",
      "**[S] Epoch 6, aupr_val: 52.5160, auc_val: 59.5000 **\n",
      "Validation: Epoch 7,  val_loss:0.6778, aupr_val: 55.11, auc_val: 62.33\n",
      "**[S] Epoch 7, aupr_val: 55.1149, auc_val: 62.3333 **\n",
      "Validation: Epoch 8,  val_loss:0.6725, aupr_val: 57.25, auc_val: 64.44\n",
      "**[S] Epoch 8, aupr_val: 57.2486, auc_val: 64.4375 **\n",
      "Validation: Epoch 9,  val_loss:0.6662, aupr_val: 59.12, auc_val: 66.58\n",
      "**[S] Epoch 9, aupr_val: 59.1230, auc_val: 66.5833 **\n",
      "Validation: Epoch 10,  val_loss:0.6695, aupr_val: 59.63, auc_val: 67.29\n",
      "**[S] Epoch 10, aupr_val: 59.6294, auc_val: 67.2917 **\n",
      "Validation: Epoch 11,  val_loss:0.6649, aupr_val: 60.13, auc_val: 68.27\n",
      "**[S] Epoch 11, aupr_val: 60.1300, auc_val: 68.2708 **\n",
      "Validation: Epoch 12,  val_loss:0.6607, aupr_val: 59.68, auc_val: 68.06\n",
      "Validation: Epoch 13,  val_loss:0.6634, aupr_val: 58.70, auc_val: 67.12\n",
      "Epoch 00014: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Validation: Epoch 14,  val_loss:0.6642, aupr_val: 59.10, auc_val: 67.35\n",
      "Validation: Epoch 15,  val_loss:0.6660, aupr_val: 59.19, auc_val: 67.65\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Validation: Epoch 16,  val_loss:0.6656, aupr_val: 59.04, auc_val: 67.52\n",
      "Validation: Epoch 17,  val_loss:0.6651, aupr_val: 59.05, auc_val: 67.50\n",
      "Epoch 00018: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Validation: Epoch 18,  val_loss:0.6651, aupr_val: 59.04, auc_val: 67.48\n",
      "Validation: Epoch 19,  val_loss:0.6650, aupr_val: 59.08, auc_val: 67.50\n",
      "Epoch 00020: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Validation: Epoch 20,  val_loss:0.6650, aupr_val: 59.08, auc_val: 67.50\n",
      "Validation: Epoch 21,  val_loss:0.6650, aupr_val: 59.08, auc_val: 67.50\n",
      "Epoch 00022: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Validation: Epoch 22,  val_loss:0.6650, aupr_val: 59.08, auc_val: 67.50\n",
      "Validation: Epoch 23,  val_loss:0.6650, aupr_val: 59.08, auc_val: 67.50\n",
      "Validation: Epoch 24,  val_loss:0.6650, aupr_val: 59.08, auc_val: 67.50\n",
      "Total Time elapsed: 1.070 mins\n",
      "     course  percentile       acc       bac       auc     auprc\n",
      "0  hwts-001          60  0.678571  0.691612  0.767681  0.755583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4006876/2251726157.py:399: RuntimeWarning: overflow encountered in exp\n",
      "  denoms = np.sum(np.exp(out_val), axis=1).reshape((-1, 1))\n",
      "/tmp/ipykernel_4006876/2251726157.py:400: RuntimeWarning: overflow encountered in exp\n",
      "  probs = np.exp(out_val) / denoms\n",
      "/tmp/ipykernel_4006876/2251726157.py:400: RuntimeWarning: invalid value encountered in true_divide\n",
      "  probs = np.exp(out_val) / denoms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     course  percentile       acc       bac       auc     auprc\n",
      "0  hwts-001          60  0.657143  0.668174  0.669305  0.578704\n",
      "Dataset used:  P12\n",
      "Split id: 1\n",
      "4377 541 547 4377 541 547\n",
      "[1210] [151] [152]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4377, 1000, 12]) torch.Size([4377, 9]) torch.Size([4377, 1000, 1]) torch.Size([4377])\n",
      "- - Run 1 - -\n",
      "Stop epochs: 25, Batches/epoch: 49, Total batches: 1225\n",
      "Validation: Epoch 0,  val_loss:0.6581, aupr_val: 40.36, auc_val: 70.69\n",
      "**[S] Epoch 0, aupr_val: 40.3610, auc_val: 70.6911 **\n",
      "Validation: Epoch 1,  val_loss:0.6458, aupr_val: 56.16, auc_val: 79.25\n",
      "**[S] Epoch 1, aupr_val: 56.1585, auc_val: 79.2478 **\n",
      "Validation: Epoch 2,  val_loss:0.6176, aupr_val: 60.24, auc_val: 82.67\n",
      "**[S] Epoch 2, aupr_val: 60.2362, auc_val: 82.6660 **\n",
      "Validation: Epoch 3,  val_loss:0.6250, aupr_val: 63.18, auc_val: 83.93\n",
      "**[S] Epoch 3, aupr_val: 63.1813, auc_val: 83.9277 **\n",
      "Validation: Epoch 4,  val_loss:0.5973, aupr_val: 66.94, auc_val: 84.12\n",
      "**[S] Epoch 4, aupr_val: 66.9401, auc_val: 84.1229 **\n",
      "Validation: Epoch 5,  val_loss:0.6028, aupr_val: 68.79, auc_val: 85.30\n",
      "**[S] Epoch 5, aupr_val: 68.7866, auc_val: 85.2980 **\n",
      "Validation: Epoch 6,  val_loss:0.5823, aupr_val: 72.25, auc_val: 86.64\n",
      "**[S] Epoch 6, aupr_val: 72.2466, auc_val: 86.6446 **\n",
      "Validation: Epoch 7,  val_loss:0.5815, aupr_val: 69.86, auc_val: 85.69\n",
      "Validation: Epoch 8,  val_loss:0.5985, aupr_val: 72.14, auc_val: 86.73\n",
      "Epoch 00009: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Validation: Epoch 9,  val_loss:0.5902, aupr_val: 72.17, auc_val: 86.82\n",
      "Validation: Epoch 10,  val_loss:0.5908, aupr_val: 71.77, auc_val: 86.53\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Validation: Epoch 11,  val_loss:0.5881, aupr_val: 72.06, auc_val: 86.70\n",
      "Validation: Epoch 12,  val_loss:0.5872, aupr_val: 72.14, auc_val: 86.78\n",
      "Epoch 00013: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Validation: Epoch 13,  val_loss:0.5873, aupr_val: 72.15, auc_val: 86.78\n",
      "Validation: Epoch 14,  val_loss:0.5874, aupr_val: 72.15, auc_val: 86.78\n",
      "Epoch 00015: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Validation: Epoch 15,  val_loss:0.5873, aupr_val: 72.16, auc_val: 86.78\n",
      "Validation: Epoch 16,  val_loss:0.5874, aupr_val: 72.16, auc_val: 86.78\n",
      "Epoch 00017: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Validation: Epoch 17,  val_loss:0.5874, aupr_val: 72.16, auc_val: 86.78\n",
      "Validation: Epoch 18,  val_loss:0.5874, aupr_val: 72.16, auc_val: 86.78\n",
      "Validation: Epoch 19,  val_loss:0.5874, aupr_val: 72.16, auc_val: 86.78\n",
      "Validation: Epoch 20,  val_loss:0.5874, aupr_val: 72.16, auc_val: 86.78\n",
      "Validation: Epoch 21,  val_loss:0.5874, aupr_val: 72.16, auc_val: 86.78\n",
      "Validation: Epoch 22,  val_loss:0.5874, aupr_val: 72.16, auc_val: 86.78\n",
      "Validation: Epoch 23,  val_loss:0.5874, aupr_val: 72.16, auc_val: 86.78\n",
      "Validation: Epoch 24,  val_loss:0.5874, aupr_val: 72.16, auc_val: 86.78\n",
      "Total Time elapsed: 3.303 mins\n",
      "    course  percentile       acc       bac       auc     auprc\n",
      "0  dsp-001          40  0.775801  0.771823  0.848074  0.654967\n",
      "    course  percentile       acc       bac       auc     auprc\n",
      "0  dsp-001          40  0.786096  0.782539  0.871911  0.721987\n",
      "Dataset used:  P12\n",
      "Split id: 1\n",
      "4432 554 551 4432 554 551\n",
      "[1210] [151] [152]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4432, 1000, 12]) torch.Size([4432, 9]) torch.Size([4432, 1000, 1]) torch.Size([4432])\n",
      "- - Run 1 - -\n",
      "Stop epochs: 25, Batches/epoch: 50, Total batches: 1250\n",
      "Validation: Epoch 0,  val_loss:0.6293, aupr_val: 43.57, auc_val: 75.92\n",
      "**[S] Epoch 0, aupr_val: 43.5747, auc_val: 75.9207 **\n",
      "Validation: Epoch 1,  val_loss:0.5833, aupr_val: 58.60, auc_val: 85.71\n",
      "**[S] Epoch 1, aupr_val: 58.6047, auc_val: 85.7148 **\n",
      "Validation: Epoch 2,  val_loss:0.5653, aupr_val: 71.77, auc_val: 92.14\n",
      "**[S] Epoch 2, aupr_val: 71.7731, auc_val: 92.1384 **\n",
      "Validation: Epoch 3,  val_loss:0.5180, aupr_val: 80.59, auc_val: 93.18\n",
      "**[S] Epoch 3, aupr_val: 80.5908, auc_val: 93.1770 **\n",
      "Validation: Epoch 4,  val_loss:0.5149, aupr_val: 83.57, auc_val: 94.05\n",
      "**[S] Epoch 4, aupr_val: 83.5713, auc_val: 94.0496 **\n",
      "Validation: Epoch 5,  val_loss:0.4917, aupr_val: 86.40, auc_val: 94.86\n",
      "**[S] Epoch 5, aupr_val: 86.3960, auc_val: 94.8647 **\n",
      "Validation: Epoch 6,  val_loss:0.4906, aupr_val: 86.78, auc_val: 95.21\n",
      "**[S] Epoch 6, aupr_val: 86.7770, auc_val: 95.2065 **\n",
      "Validation: Epoch 7,  val_loss:0.5051, aupr_val: 85.85, auc_val: 94.84\n",
      "Validation: Epoch 8,  val_loss:0.4848, aupr_val: 86.06, auc_val: 94.79\n",
      "Epoch 00009: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Validation: Epoch 9,  val_loss:0.4773, aupr_val: 87.23, auc_val: 95.23\n",
      "**[S] Epoch 9, aupr_val: 87.2302, auc_val: 95.2311 **\n",
      "Validation: Epoch 10,  val_loss:0.4800, aupr_val: 87.06, auc_val: 95.13\n",
      "Validation: Epoch 11,  val_loss:0.4751, aupr_val: 87.70, auc_val: 95.37\n",
      "**[S] Epoch 11, aupr_val: 87.6996, auc_val: 95.3741 **\n",
      "Validation: Epoch 12,  val_loss:0.4790, aupr_val: 87.41, auc_val: 95.24\n",
      "Validation: Epoch 13,  val_loss:0.4827, aupr_val: 87.82, auc_val: 95.36\n",
      "**[S] Epoch 13, aupr_val: 87.8218, auc_val: 95.3642 **\n",
      "Validation: Epoch 14,  val_loss:0.4741, aupr_val: 87.65, auc_val: 95.34\n",
      "Validation: Epoch 15,  val_loss:0.4759, aupr_val: 87.67, auc_val: 95.33\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Validation: Epoch 16,  val_loss:0.4770, aupr_val: 87.67, auc_val: 95.32\n",
      "Validation: Epoch 17,  val_loss:0.4768, aupr_val: 87.68, auc_val: 95.32\n",
      "Epoch 00018: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Validation: Epoch 18,  val_loss:0.4767, aupr_val: 87.67, auc_val: 95.32\n",
      "Validation: Epoch 19,  val_loss:0.4766, aupr_val: 87.67, auc_val: 95.32\n",
      "Epoch 00020: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Validation: Epoch 20,  val_loss:0.4766, aupr_val: 87.67, auc_val: 95.32\n",
      "Validation: Epoch 21,  val_loss:0.4766, aupr_val: 87.67, auc_val: 95.32\n",
      "Epoch 00022: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Validation: Epoch 22,  val_loss:0.4766, aupr_val: 87.67, auc_val: 95.32\n",
      "Validation: Epoch 23,  val_loss:0.4766, aupr_val: 87.68, auc_val: 95.32\n",
      "Validation: Epoch 24,  val_loss:0.4766, aupr_val: 87.68, auc_val: 95.32\n",
      "Total Time elapsed: 3.532 mins\n",
      "    course  percentile       acc       bac       auc     auprc\n",
      "0  dsp-001          60  0.871886  0.883216  0.954445  0.877891\n",
      "    course  percentile      acc       bac       auc     auprc\n",
      "0  dsp-001          60  0.88057  0.895283  0.953788  0.884088\n",
      "Dataset used:  P12\n",
      "Split id: 1\n",
      "6172 771 772 6172 771 772\n",
      "[5097] [638] [638]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6172, 1000, 24]) torch.Size([6172, 9]) torch.Size([6172, 1000, 1]) torch.Size([6172])\n",
      "- - Run 1 - -\n",
      "Stop epochs: 25, Batches/epoch: 16, Total batches: 400\n",
      "Validation: Epoch 0,  val_loss:0.6921, aupr_val: 82.60, auc_val: 47.50\n",
      "**[S] Epoch 0, aupr_val: 82.5962, auc_val: 47.5004 **\n",
      "Validation: Epoch 1,  val_loss:0.6925, aupr_val: 82.65, auc_val: 48.50\n",
      "**[S] Epoch 1, aupr_val: 82.6504, auc_val: 48.4986 **\n",
      "Validation: Epoch 2,  val_loss:0.6923, aupr_val: 84.58, auc_val: 51.52\n",
      "**[S] Epoch 2, aupr_val: 84.5821, auc_val: 51.5203 **\n",
      "Validation: Epoch 3,  val_loss:0.6865, aupr_val: 86.66, auc_val: 56.36\n",
      "**[S] Epoch 3, aupr_val: 86.6599, auc_val: 56.3580 **\n",
      "Validation: Epoch 4,  val_loss:0.6927, aupr_val: 86.88, auc_val: 57.37\n",
      "**[S] Epoch 4, aupr_val: 86.8767, auc_val: 57.3697 **\n",
      "Validation: Epoch 5,  val_loss:0.6840, aupr_val: 87.01, auc_val: 57.75\n",
      "**[S] Epoch 5, aupr_val: 87.0092, auc_val: 57.7521 **\n",
      "Validation: Epoch 6,  val_loss:0.6881, aupr_val: 86.95, auc_val: 57.71\n",
      "Validation: Epoch 7,  val_loss:0.6777, aupr_val: 86.94, auc_val: 57.92\n",
      "Epoch 00008: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Validation: Epoch 8,  val_loss:0.6887, aupr_val: 86.99, auc_val: 58.02\n",
      "Validation: Epoch 9,  val_loss:0.6842, aupr_val: 86.96, auc_val: 58.00\n",
      "Epoch 00010: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Validation: Epoch 10,  val_loss:0.6849, aupr_val: 86.97, auc_val: 58.02\n",
      "Validation: Epoch 11,  val_loss:0.6856, aupr_val: 86.97, auc_val: 58.02\n",
      "Epoch 00012: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Validation: Epoch 12,  val_loss:0.6856, aupr_val: 86.97, auc_val: 58.02\n",
      "Validation: Epoch 13,  val_loss:0.6856, aupr_val: 86.97, auc_val: 58.02\n",
      "Epoch 00014: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Validation: Epoch 14,  val_loss:0.6856, aupr_val: 86.97, auc_val: 58.02\n",
      "Validation: Epoch 15,  val_loss:0.6856, aupr_val: 86.97, auc_val: 58.02\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Validation: Epoch 16,  val_loss:0.6856, aupr_val: 86.97, auc_val: 58.02\n",
      "Validation: Epoch 17,  val_loss:0.6856, aupr_val: 86.97, auc_val: 58.02\n",
      "Validation: Epoch 18,  val_loss:0.6856, aupr_val: 86.97, auc_val: 58.02\n",
      "Validation: Epoch 19,  val_loss:0.6856, aupr_val: 86.97, auc_val: 58.02\n",
      "Validation: Epoch 20,  val_loss:0.6856, aupr_val: 86.97, auc_val: 58.02\n",
      "Validation: Epoch 21,  val_loss:0.6856, aupr_val: 86.97, auc_val: 58.02\n",
      "Validation: Epoch 22,  val_loss:0.6856, aupr_val: 86.97, auc_val: 58.02\n",
      "Validation: Epoch 23,  val_loss:0.6856, aupr_val: 86.97, auc_val: 58.02\n",
      "Validation: Epoch 24,  val_loss:0.6856, aupr_val: 86.97, auc_val: 58.02\n",
      "Total Time elapsed: 1.674 mins\n",
      "        course  percentile      acc       bac       auc     auprc\n",
      "0  progfun-002          40  0.57398  0.560182  0.620878  0.866814\n",
      "        course  percentile       acc      bac       auc     auprc\n",
      "0  progfun-002          40  0.598214  0.56957  0.604502  0.869843\n",
      "Dataset used:  P12\n",
      "Split id: 1\n",
      "6227 777 781 6227 777 781\n",
      "[5123] [641] [641]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6227, 1000, 24]) torch.Size([6227, 9]) torch.Size([6227, 1000, 1]) torch.Size([6227])\n",
      "- - Run 1 - -\n",
      "Stop epochs: 25, Batches/epoch: 17, Total batches: 425\n",
      "Validation: Epoch 0,  val_loss:0.6910, aupr_val: 81.26, auc_val: 47.31\n",
      "**[S] Epoch 0, aupr_val: 81.2617, auc_val: 47.3089 **\n",
      "Validation: Epoch 1,  val_loss:0.6951, aupr_val: 81.79, auc_val: 47.58\n",
      "**[S] Epoch 1, aupr_val: 81.7909, auc_val: 47.5785 **\n",
      "Validation: Epoch 2,  val_loss:0.6858, aupr_val: 82.79, auc_val: 49.22\n",
      "**[S] Epoch 2, aupr_val: 82.7894, auc_val: 49.2165 **\n",
      "Validation: Epoch 3,  val_loss:0.6966, aupr_val: 85.55, auc_val: 54.05\n",
      "**[S] Epoch 3, aupr_val: 85.5464, auc_val: 54.0504 **\n",
      "Validation: Epoch 4,  val_loss:0.6732, aupr_val: 88.40, auc_val: 60.97\n",
      "**[S] Epoch 4, aupr_val: 88.3958, auc_val: 60.9698 **\n",
      "Validation: Epoch 5,  val_loss:0.6757, aupr_val: 89.00, auc_val: 62.63\n",
      "**[S] Epoch 5, aupr_val: 88.9961, auc_val: 62.6285 **\n",
      "Validation: Epoch 6,  val_loss:0.6922, aupr_val: 88.83, auc_val: 62.91\n",
      "Validation: Epoch 7,  val_loss:0.6948, aupr_val: 88.60, auc_val: 61.97\n",
      "Epoch 00008: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Validation: Epoch 8,  val_loss:0.6787, aupr_val: 88.64, auc_val: 62.06\n",
      "Validation: Epoch 9,  val_loss:0.6832, aupr_val: 88.63, auc_val: 62.05\n",
      "Epoch 00010: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Validation: Epoch 10,  val_loss:0.6829, aupr_val: 88.63, auc_val: 62.05\n",
      "Validation: Epoch 11,  val_loss:0.6820, aupr_val: 88.63, auc_val: 62.05\n",
      "Epoch 00012: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Validation: Epoch 12,  val_loss:0.6820, aupr_val: 88.63, auc_val: 62.05\n",
      "Validation: Epoch 13,  val_loss:0.6819, aupr_val: 88.63, auc_val: 62.05\n",
      "Epoch 00014: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Validation: Epoch 14,  val_loss:0.6819, aupr_val: 88.63, auc_val: 62.05\n",
      "Validation: Epoch 15,  val_loss:0.6819, aupr_val: 88.63, auc_val: 62.05\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Validation: Epoch 16,  val_loss:0.6819, aupr_val: 88.63, auc_val: 62.05\n",
      "Validation: Epoch 17,  val_loss:0.6819, aupr_val: 88.63, auc_val: 62.05\n",
      "Validation: Epoch 18,  val_loss:0.6819, aupr_val: 88.63, auc_val: 62.05\n",
      "Validation: Epoch 19,  val_loss:0.6819, aupr_val: 88.63, auc_val: 62.05\n",
      "Validation: Epoch 20,  val_loss:0.6819, aupr_val: 88.63, auc_val: 62.05\n",
      "Validation: Epoch 21,  val_loss:0.6819, aupr_val: 88.63, auc_val: 62.05\n",
      "Validation: Epoch 22,  val_loss:0.6819, aupr_val: 88.63, auc_val: 62.05\n",
      "Validation: Epoch 23,  val_loss:0.6819, aupr_val: 88.63, auc_val: 62.05\n",
      "Validation: Epoch 24,  val_loss:0.6819, aupr_val: 88.63, auc_val: 62.05\n",
      "Total Time elapsed: 1.753 mins\n",
      "        course  percentile       acc       bac       auc     auprc\n",
      "0  progfun-002          60  0.631378  0.619732  0.652761  0.890368\n",
      "        course  percentile       acc       bac       auc    auprc\n",
      "0  progfun-002          60  0.604592  0.595202  0.644426  0.88993\n",
      "Dataset used:  P12\n",
      "Split id: 1\n",
      "445 56 56 445 56 56\n",
      "[222] [28] [27]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([445, 1000, 26]) torch.Size([445, 9]) torch.Size([445, 1000, 1]) torch.Size([445])\n",
      "- - Run 1 - -\n",
      "Stop epochs: 25, Batches/epoch: 3, Total batches: 75\n",
      "Validation: Epoch 0,  val_loss:0.6933, aupr_val: 46.80, auc_val: 45.15\n",
      "**[S] Epoch 0, aupr_val: 46.7958, auc_val: 45.1531 **\n",
      "Validation: Epoch 1,  val_loss:0.6931, aupr_val: 52.08, auc_val: 51.91\n",
      "**[S] Epoch 1, aupr_val: 52.0804, auc_val: 51.9133 **\n",
      "Validation: Epoch 2,  val_loss:0.6931, aupr_val: 54.42, auc_val: 54.08\n",
      "**[S] Epoch 2, aupr_val: 54.4203, auc_val: 54.0816 **\n",
      "Validation: Epoch 3,  val_loss:0.6930, aupr_val: 54.89, auc_val: 53.95\n",
      "**[S] Epoch 3, aupr_val: 54.8911, auc_val: 53.9541 **\n",
      "Validation: Epoch 4,  val_loss:0.6929, aupr_val: 54.34, auc_val: 53.70\n",
      "Validation: Epoch 5,  val_loss:0.6928, aupr_val: 53.60, auc_val: 53.83\n",
      "Epoch 00006: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Validation: Epoch 6,  val_loss:0.6928, aupr_val: 53.74, auc_val: 53.95\n",
      "Validation: Epoch 7,  val_loss:0.6929, aupr_val: 52.64, auc_val: 53.32\n",
      "Epoch 00008: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Validation: Epoch 8,  val_loss:0.6929, aupr_val: 52.64, auc_val: 53.32\n",
      "Validation: Epoch 9,  val_loss:0.6929, aupr_val: 52.64, auc_val: 53.32\n",
      "Epoch 00010: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Validation: Epoch 10,  val_loss:0.6929, aupr_val: 52.64, auc_val: 53.32\n",
      "Validation: Epoch 11,  val_loss:0.6929, aupr_val: 52.64, auc_val: 53.32\n",
      "Epoch 00012: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Validation: Epoch 12,  val_loss:0.6929, aupr_val: 52.64, auc_val: 53.32\n",
      "Validation: Epoch 13,  val_loss:0.6929, aupr_val: 52.64, auc_val: 53.32\n",
      "Epoch 00014: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Validation: Epoch 14,  val_loss:0.6929, aupr_val: 52.64, auc_val: 53.32\n",
      "Validation: Epoch 15,  val_loss:0.6929, aupr_val: 52.64, auc_val: 53.32\n",
      "Validation: Epoch 16,  val_loss:0.6929, aupr_val: 52.64, auc_val: 53.32\n",
      "Validation: Epoch 17,  val_loss:0.6929, aupr_val: 52.64, auc_val: 53.32\n",
      "Validation: Epoch 18,  val_loss:0.6929, aupr_val: 52.64, auc_val: 53.32\n",
      "Validation: Epoch 19,  val_loss:0.6929, aupr_val: 52.64, auc_val: 53.32\n",
      "Validation: Epoch 20,  val_loss:0.6929, aupr_val: 52.64, auc_val: 53.32\n",
      "Validation: Epoch 21,  val_loss:0.6929, aupr_val: 52.64, auc_val: 53.32\n",
      "Validation: Epoch 22,  val_loss:0.6929, aupr_val: 52.64, auc_val: 53.32\n",
      "Validation: Epoch 23,  val_loss:0.6929, aupr_val: 52.64, auc_val: 53.32\n",
      "Validation: Epoch 24,  val_loss:0.6929, aupr_val: 52.64, auc_val: 53.32\n",
      "Total Time elapsed: 0.326 mins\n",
      "                 course  percentile       acc       bac       auc     auprc\n",
      "0  microcontroleurs-003          40  0.561404  0.562808  0.549261  0.523975\n",
      "                 course  percentile       acc       bac       auc     auprc\n",
      "0  microcontroleurs-003          40  0.578947  0.580665  0.552956  0.544674\n",
      "Dataset used:  P12\n",
      "Split id: 1\n",
      "452 57 57 452 57 57\n",
      "[224] [28] [28]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([452, 1000, 26]) torch.Size([452, 9]) torch.Size([452, 1000, 1]) torch.Size([452])\n",
      "- - Run 1 - -\n",
      "Stop epochs: 25, Batches/epoch: 3, Total batches: 75\n",
      "Validation: Epoch 0,  val_loss:0.6932, aupr_val: 45.18, auc_val: 44.83\n",
      "**[S] Epoch 0, aupr_val: 45.1767, auc_val: 44.8276 **\n",
      "Validation: Epoch 1,  val_loss:0.6934, aupr_val: 47.38, auc_val: 49.26\n",
      "**[S] Epoch 1, aupr_val: 47.3825, auc_val: 49.2611 **\n",
      "Validation: Epoch 2,  val_loss:0.6933, aupr_val: 47.95, auc_val: 48.77\n",
      "**[S] Epoch 2, aupr_val: 47.9486, auc_val: 48.7685 **\n",
      "Validation: Epoch 3,  val_loss:0.6935, aupr_val: 49.27, auc_val: 46.92\n",
      "**[S] Epoch 3, aupr_val: 49.2712, auc_val: 46.9212 **\n",
      "Validation: Epoch 4,  val_loss:0.6938, aupr_val: 49.52, auc_val: 46.80\n",
      "**[S] Epoch 4, aupr_val: 49.5193, auc_val: 46.7980 **\n",
      "Validation: Epoch 5,  val_loss:0.6940, aupr_val: 48.27, auc_val: 45.44\n",
      "Validation: Epoch 6,  val_loss:0.6945, aupr_val: 48.18, auc_val: 45.32\n",
      "Epoch 00007: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Validation: Epoch 7,  val_loss:0.6946, aupr_val: 48.18, auc_val: 45.32\n",
      "Validation: Epoch 8,  val_loss:0.6946, aupr_val: 48.17, auc_val: 45.07\n",
      "Epoch 00009: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Validation: Epoch 9,  val_loss:0.6946, aupr_val: 48.17, auc_val: 45.07\n",
      "Validation: Epoch 10,  val_loss:0.6947, aupr_val: 48.17, auc_val: 45.07\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Validation: Epoch 11,  val_loss:0.6947, aupr_val: 48.17, auc_val: 45.07\n",
      "Validation: Epoch 12,  val_loss:0.6947, aupr_val: 48.17, auc_val: 45.07\n",
      "Epoch 00013: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Validation: Epoch 13,  val_loss:0.6947, aupr_val: 48.17, auc_val: 45.07\n",
      "Validation: Epoch 14,  val_loss:0.6947, aupr_val: 48.17, auc_val: 45.07\n",
      "Epoch 00015: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Validation: Epoch 15,  val_loss:0.6947, aupr_val: 48.17, auc_val: 45.07\n",
      "Validation: Epoch 16,  val_loss:0.6947, aupr_val: 48.17, auc_val: 45.07\n",
      "Validation: Epoch 17,  val_loss:0.6947, aupr_val: 48.17, auc_val: 45.07\n",
      "Validation: Epoch 18,  val_loss:0.6947, aupr_val: 48.17, auc_val: 45.07\n",
      "Validation: Epoch 19,  val_loss:0.6947, aupr_val: 48.17, auc_val: 45.07\n",
      "Validation: Epoch 20,  val_loss:0.6947, aupr_val: 48.17, auc_val: 45.07\n",
      "Validation: Epoch 21,  val_loss:0.6947, aupr_val: 48.17, auc_val: 45.07\n",
      "Validation: Epoch 22,  val_loss:0.6947, aupr_val: 48.17, auc_val: 45.07\n",
      "Validation: Epoch 23,  val_loss:0.6947, aupr_val: 48.17, auc_val: 45.07\n",
      "Validation: Epoch 24,  val_loss:0.6947, aupr_val: 48.17, auc_val: 45.07\n",
      "Total Time elapsed: 0.267 mins\n",
      "                 course  percentile       acc       bac       auc     auprc\n",
      "0  microcontroleurs-003          60  0.614035  0.616995  0.616995  0.530262\n",
      "                 course  percentile       acc       bac       auc     auprc\n",
      "0  microcontroleurs-003          60  0.438596  0.442734  0.469212  0.495267\n",
      "Dataset used:  P12\n",
      "Split id: 1\n",
      "356 45 43 356 45 43\n",
      "[163] [20] [21]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([356, 1000, 24]) torch.Size([356, 9]) torch.Size([356, 1000, 1]) torch.Size([356])\n",
      "- - Run 1 - -\n",
      "Stop epochs: 25, Batches/epoch: 3, Total batches: 75\n",
      "Validation: Epoch 0,  val_loss:0.6932, aupr_val: 42.23, auc_val: 47.60\n",
      "**[S] Epoch 0, aupr_val: 42.2344, auc_val: 47.6000 **\n",
      "Validation: Epoch 1,  val_loss:0.6935, aupr_val: 43.54, auc_val: 50.40\n",
      "**[S] Epoch 1, aupr_val: 43.5397, auc_val: 50.4000 **\n",
      "Validation: Epoch 2,  val_loss:0.6931, aupr_val: 43.32, auc_val: 49.60\n",
      "Validation: Epoch 3,  val_loss:0.6938, aupr_val: 43.34, auc_val: 49.80\n",
      "Epoch 00004: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Validation: Epoch 4,  val_loss:0.6938, aupr_val: 43.34, auc_val: 49.80\n",
      "Validation: Epoch 5,  val_loss:0.6937, aupr_val: 43.34, auc_val: 49.80\n",
      "Epoch 00006: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Validation: Epoch 6,  val_loss:0.6937, aupr_val: 43.34, auc_val: 49.80\n",
      "Validation: Epoch 7,  val_loss:0.6937, aupr_val: 43.34, auc_val: 49.80\n",
      "Epoch 00008: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Validation: Epoch 8,  val_loss:0.6937, aupr_val: 43.34, auc_val: 49.80\n",
      "Validation: Epoch 9,  val_loss:0.6937, aupr_val: 43.34, auc_val: 49.80\n",
      "Epoch 00010: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Validation: Epoch 10,  val_loss:0.6937, aupr_val: 43.34, auc_val: 49.80\n",
      "Validation: Epoch 11,  val_loss:0.6937, aupr_val: 43.34, auc_val: 49.80\n",
      "Epoch 00012: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Validation: Epoch 12,  val_loss:0.6937, aupr_val: 43.34, auc_val: 49.80\n",
      "Validation: Epoch 13,  val_loss:0.6937, aupr_val: 43.34, auc_val: 49.80\n",
      "Validation: Epoch 14,  val_loss:0.6937, aupr_val: 43.34, auc_val: 49.80\n",
      "Validation: Epoch 15,  val_loss:0.6937, aupr_val: 43.34, auc_val: 49.80\n",
      "Validation: Epoch 16,  val_loss:0.6937, aupr_val: 43.34, auc_val: 49.80\n",
      "Validation: Epoch 17,  val_loss:0.6937, aupr_val: 43.34, auc_val: 49.80\n",
      "Validation: Epoch 18,  val_loss:0.6937, aupr_val: 43.34, auc_val: 49.80\n",
      "Validation: Epoch 19,  val_loss:0.6937, aupr_val: 43.34, auc_val: 49.80\n",
      "Validation: Epoch 20,  val_loss:0.6937, aupr_val: 43.34, auc_val: 49.80\n",
      "Validation: Epoch 21,  val_loss:0.6937, aupr_val: 43.34, auc_val: 49.80\n",
      "Validation: Epoch 22,  val_loss:0.6937, aupr_val: 43.34, auc_val: 49.80\n",
      "Validation: Epoch 23,  val_loss:0.6937, aupr_val: 43.34, auc_val: 49.80\n",
      "Validation: Epoch 24,  val_loss:0.6937, aupr_val: 43.34, auc_val: 49.80\n",
      "Total Time elapsed: 0.259 mins\n",
      "           course  percentile  acc       bac       auc    auprc\n",
      "0  geomatique-003          40  0.5  0.520952  0.497143  0.46717\n",
      "           course  percentile       acc    bac    auc     auprc\n",
      "0  geomatique-003          40  0.422222  0.455  0.496  0.432268\n",
      "Dataset used:  P12\n",
      "Split id: 1\n",
      "357 45 44 357 45 44\n",
      "[163] [20] [21]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([357, 1000, 24]) torch.Size([357, 9]) torch.Size([357, 1000, 1]) torch.Size([357])\n",
      "- - Run 1 - -\n",
      "Stop epochs: 25, Batches/epoch: 3, Total batches: 75\n",
      "Validation: Epoch 0,  val_loss:0.6925, aupr_val: 54.63, auc_val: 58.60\n",
      "**[S] Epoch 0, aupr_val: 54.6305, auc_val: 58.6000 **\n",
      "Validation: Epoch 1,  val_loss:0.6931, aupr_val: 53.04, auc_val: 59.00\n",
      "Validation: Epoch 2,  val_loss:0.6926, aupr_val: 52.83, auc_val: 59.40\n",
      "Epoch 00003: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Validation: Epoch 3,  val_loss:0.6925, aupr_val: 52.95, auc_val: 59.60\n",
      "Validation: Epoch 4,  val_loss:0.6925, aupr_val: 53.13, auc_val: 60.00\n",
      "Epoch 00005: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Validation: Epoch 5,  val_loss:0.6925, aupr_val: 53.13, auc_val: 60.00\n",
      "Validation: Epoch 6,  val_loss:0.6925, aupr_val: 53.13, auc_val: 60.00\n",
      "Epoch 00007: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Validation: Epoch 7,  val_loss:0.6925, aupr_val: 53.13, auc_val: 60.00\n",
      "Validation: Epoch 8,  val_loss:0.6925, aupr_val: 53.13, auc_val: 60.00\n",
      "Epoch 00009: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Validation: Epoch 9,  val_loss:0.6925, aupr_val: 53.13, auc_val: 60.00\n",
      "Validation: Epoch 10,  val_loss:0.6925, aupr_val: 53.13, auc_val: 60.00\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Validation: Epoch 11,  val_loss:0.6925, aupr_val: 53.13, auc_val: 60.00\n",
      "Validation: Epoch 12,  val_loss:0.6925, aupr_val: 53.13, auc_val: 60.00\n",
      "Validation: Epoch 13,  val_loss:0.6925, aupr_val: 53.13, auc_val: 60.00\n",
      "Validation: Epoch 14,  val_loss:0.6925, aupr_val: 53.13, auc_val: 60.00\n",
      "Validation: Epoch 15,  val_loss:0.6925, aupr_val: 53.13, auc_val: 60.00\n",
      "Validation: Epoch 16,  val_loss:0.6925, aupr_val: 53.13, auc_val: 60.00\n",
      "Validation: Epoch 17,  val_loss:0.6925, aupr_val: 53.13, auc_val: 60.00\n",
      "Validation: Epoch 18,  val_loss:0.6925, aupr_val: 53.13, auc_val: 60.00\n",
      "Validation: Epoch 19,  val_loss:0.6925, aupr_val: 53.13, auc_val: 60.00\n",
      "Validation: Epoch 20,  val_loss:0.6925, aupr_val: 53.13, auc_val: 60.00\n",
      "Validation: Epoch 21,  val_loss:0.6925, aupr_val: 53.13, auc_val: 60.00\n",
      "Validation: Epoch 22,  val_loss:0.6925, aupr_val: 53.13, auc_val: 60.00\n",
      "Validation: Epoch 23,  val_loss:0.6925, aupr_val: 53.13, auc_val: 60.00\n",
      "Validation: Epoch 24,  val_loss:0.6925, aupr_val: 53.13, auc_val: 60.00\n",
      "Total Time elapsed: 0.276 mins\n",
      "           course  percentile       acc  bac       auc     auprc\n",
      "0  geomatique-003          60  0.543478  0.5  0.565714  0.523658\n",
      "           course  percentile       acc  bac    auc    auprc\n",
      "0  geomatique-003          60  0.555556  0.5  0.578  0.54072\n",
      "Dataset used:  P12\n",
      "Split id: 1\n",
      "3602 441 450 3602 441 450\n",
      "[445] [56] [56]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3602, 1000, 24]) torch.Size([3602, 9]) torch.Size([3602, 1000, 1]) torch.Size([3602])\n",
      "- - Run 1 - -\n",
      "Stop epochs: 25, Batches/epoch: 20, Total batches: 500\n",
      "Validation: Epoch 0,  val_loss:0.6928, aupr_val: 14.05, auc_val: 57.80\n",
      "**[S] Epoch 0, aupr_val: 14.0518, auc_val: 57.7968 **\n",
      "Validation: Epoch 1,  val_loss:0.6680, aupr_val: 19.30, auc_val: 64.50\n",
      "**[S] Epoch 1, aupr_val: 19.3043, auc_val: 64.5037 **\n",
      "Validation: Epoch 2,  val_loss:0.6088, aupr_val: 33.33, auc_val: 83.65\n",
      "**[S] Epoch 2, aupr_val: 33.3262, auc_val: 83.6456 **\n",
      "Validation: Epoch 3,  val_loss:0.5627, aupr_val: 37.43, auc_val: 86.52\n",
      "**[S] Epoch 3, aupr_val: 37.4298, auc_val: 86.5167 **\n",
      "Validation: Epoch 4,  val_loss:0.5471, aupr_val: 37.77, auc_val: 86.28\n",
      "**[S] Epoch 4, aupr_val: 37.7678, auc_val: 86.2755 **\n",
      "Validation: Epoch 5,  val_loss:0.5506, aupr_val: 37.26, auc_val: 86.56\n",
      "Validation: Epoch 6,  val_loss:0.5695, aupr_val: 38.95, auc_val: 87.33\n",
      "**[S] Epoch 6, aupr_val: 38.9491, auc_val: 87.3284 **\n",
      "Validation: Epoch 7,  val_loss:0.5569, aupr_val: 36.43, auc_val: 85.94\n",
      "Validation: Epoch 8,  val_loss:0.5807, aupr_val: 45.33, auc_val: 88.29\n",
      "**[S] Epoch 8, aupr_val: 45.3281, auc_val: 88.2931 **\n",
      "Validation: Epoch 9,  val_loss:0.5405, aupr_val: 42.79, auc_val: 87.68\n",
      "Validation: Epoch 10,  val_loss:0.5519, aupr_val: 39.56, auc_val: 86.98\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Validation: Epoch 11,  val_loss:0.5495, aupr_val: 40.97, auc_val: 87.59\n",
      "Validation: Epoch 12,  val_loss:0.5484, aupr_val: 41.68, auc_val: 87.97\n",
      "Epoch 00013: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Validation: Epoch 13,  val_loss:0.5481, aupr_val: 41.54, auc_val: 87.93\n",
      "Validation: Epoch 14,  val_loss:0.5487, aupr_val: 41.50, auc_val: 87.94\n",
      "Epoch 00015: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Validation: Epoch 15,  val_loss:0.5486, aupr_val: 41.49, auc_val: 87.94\n",
      "Validation: Epoch 16,  val_loss:0.5485, aupr_val: 41.49, auc_val: 87.94\n",
      "Epoch 00017: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Validation: Epoch 17,  val_loss:0.5485, aupr_val: 41.49, auc_val: 87.94\n",
      "Validation: Epoch 18,  val_loss:0.5485, aupr_val: 41.49, auc_val: 87.94\n",
      "Epoch 00019: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Validation: Epoch 19,  val_loss:0.5485, aupr_val: 41.49, auc_val: 87.94\n",
      "Validation: Epoch 20,  val_loss:0.5485, aupr_val: 41.49, auc_val: 87.94\n",
      "Validation: Epoch 21,  val_loss:0.5485, aupr_val: 41.49, auc_val: 87.94\n",
      "Validation: Epoch 22,  val_loss:0.5485, aupr_val: 41.49, auc_val: 87.94\n",
      "Validation: Epoch 23,  val_loss:0.5485, aupr_val: 41.49, auc_val: 87.94\n",
      "Validation: Epoch 24,  val_loss:0.5485, aupr_val: 41.49, auc_val: 87.94\n",
      "Total Time elapsed: 1.871 mins\n",
      "                 course  percentile       acc       bac       auc     auprc\n",
      "0  villesafricaines-001          40  0.678788  0.772169  0.859095  0.380063\n",
      "                 course  percentile       acc       bac      auc    auprc\n",
      "0  villesafricaines-001          40  0.742915  0.831662  0.89938  0.46646\n",
      "Dataset used:  P12\n",
      "Split id: 1\n",
      "3718 457 469 3718 457 469\n",
      "[449] [56] [56]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3718, 1000, 24]) torch.Size([3718, 9]) torch.Size([3718, 1000, 1]) torch.Size([3718])\n",
      "- - Run 1 - -\n",
      "Stop epochs: 25, Batches/epoch: 21, Total batches: 525\n",
      "Validation: Epoch 0,  val_loss:0.6925, aupr_val: 12.88, auc_val: 56.15\n",
      "**[S] Epoch 0, aupr_val: 12.8803, auc_val: 56.1543 **\n",
      "Validation: Epoch 1,  val_loss:0.6721, aupr_val: 14.39, auc_val: 61.54\n",
      "**[S] Epoch 1, aupr_val: 14.3899, auc_val: 61.5426 **\n",
      "Validation: Epoch 2,  val_loss:0.5935, aupr_val: 25.76, auc_val: 76.39\n",
      "**[S] Epoch 2, aupr_val: 25.7629, auc_val: 76.3894 **\n",
      "Validation: Epoch 3,  val_loss:0.5790, aupr_val: 42.70, auc_val: 88.06\n",
      "**[S] Epoch 3, aupr_val: 42.6991, auc_val: 88.0566 **\n",
      "Validation: Epoch 4,  val_loss:0.5645, aupr_val: 41.18, auc_val: 88.45\n",
      "Validation: Epoch 5,  val_loss:0.5097, aupr_val: 41.53, auc_val: 88.20\n",
      "Epoch 00006: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Validation: Epoch 6,  val_loss:0.5523, aupr_val: 44.76, auc_val: 89.01\n",
      "**[S] Epoch 6, aupr_val: 44.7627, auc_val: 89.0096 **\n",
      "Validation: Epoch 7,  val_loss:0.5462, aupr_val: 44.78, auc_val: 89.01\n",
      "**[S] Epoch 7, aupr_val: 44.7798, auc_val: 89.0096 **\n",
      "Validation: Epoch 8,  val_loss:0.5422, aupr_val: 45.19, auc_val: 89.00\n",
      "**[S] Epoch 8, aupr_val: 45.1943, auc_val: 88.9963 **\n",
      "Validation: Epoch 9,  val_loss:0.5497, aupr_val: 46.54, auc_val: 89.12\n",
      "**[S] Epoch 9, aupr_val: 46.5378, auc_val: 89.1165 **\n",
      "Validation: Epoch 10,  val_loss:0.5386, aupr_val: 46.76, auc_val: 89.10\n",
      "**[S] Epoch 10, aupr_val: 46.7630, auc_val: 89.0987 **\n",
      "Validation: Epoch 11,  val_loss:0.5378, aupr_val: 47.11, auc_val: 89.12\n",
      "**[S] Epoch 11, aupr_val: 47.1097, auc_val: 89.1165 **\n",
      "Validation: Epoch 12,  val_loss:0.5471, aupr_val: 48.09, auc_val: 89.28\n",
      "**[S] Epoch 12, aupr_val: 48.0937, auc_val: 89.2813 **\n",
      "Validation: Epoch 13,  val_loss:0.5299, aupr_val: 48.17, auc_val: 89.15\n",
      "**[S] Epoch 13, aupr_val: 48.1687, auc_val: 89.1521 **\n",
      "Validation: Epoch 14,  val_loss:0.5434, aupr_val: 48.45, auc_val: 89.39\n",
      "**[S] Epoch 14, aupr_val: 48.4524, auc_val: 89.3926 **\n",
      "Validation: Epoch 15,  val_loss:0.5363, aupr_val: 48.00, auc_val: 89.18\n",
      "Validation: Epoch 16,  val_loss:0.5400, aupr_val: 48.16, auc_val: 89.32\n",
      "Epoch 00017: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Validation: Epoch 17,  val_loss:0.5408, aupr_val: 48.12, auc_val: 89.32\n",
      "Validation: Epoch 18,  val_loss:0.5439, aupr_val: 48.55, auc_val: 89.40\n",
      "**[S] Epoch 18, aupr_val: 48.5480, auc_val: 89.3970 **\n",
      "Validation: Epoch 19,  val_loss:0.5422, aupr_val: 48.60, auc_val: 89.41\n",
      "**[S] Epoch 19, aupr_val: 48.6019, auc_val: 89.4104 **\n",
      "Validation: Epoch 20,  val_loss:0.5419, aupr_val: 48.57, auc_val: 89.40\n",
      "Validation: Epoch 21,  val_loss:0.5411, aupr_val: 48.57, auc_val: 89.40\n",
      "Epoch 00022: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Validation: Epoch 22,  val_loss:0.5410, aupr_val: 48.57, auc_val: 89.40\n",
      "Validation: Epoch 23,  val_loss:0.5410, aupr_val: 48.56, auc_val: 89.39\n",
      "Epoch 00024: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Validation: Epoch 24,  val_loss:0.5410, aupr_val: 48.57, auc_val: 89.40\n",
      "Total Time elapsed: 2.024 mins\n",
      "                 course  percentile       acc       bac       auc     auprc\n",
      "0  villesafricaines-001          60  0.773737  0.802331  0.879393  0.444127\n",
      "                 course  percentile       acc       bac       auc     auprc\n",
      "0  villesafricaines-001          60  0.807692  0.829256  0.901174  0.472801\n",
      "Dataset used:  P12\n",
      "Split id: 1\n",
      "8386 1046 1051 8386 1046 1051\n",
      "[4508] [562] [563]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8386, 1000, 24]) torch.Size([8386, 9]) torch.Size([8386, 1000, 1]) torch.Size([8386])\n",
      "- - Run 1 - -\n",
      "Stop epochs: 25, Batches/epoch: 60, Total batches: 1500\n",
      "Validation: Epoch 0,  val_loss:0.6858, aupr_val: 66.75, auc_val: 65.21\n",
      "**[S] Epoch 0, aupr_val: 66.7502, auc_val: 65.2128 **\n",
      "Validation: Epoch 1,  val_loss:0.6703, aupr_val: 68.61, auc_val: 69.19\n",
      "**[S] Epoch 1, aupr_val: 68.6053, auc_val: 69.1902 **\n",
      "Validation: Epoch 2,  val_loss:0.6623, aupr_val: 69.74, auc_val: 70.26\n",
      "**[S] Epoch 2, aupr_val: 69.7388, auc_val: 70.2619 **\n",
      "Validation: Epoch 3,  val_loss:0.6604, aupr_val: 69.73, auc_val: 70.48\n",
      "Validation: Epoch 4,  val_loss:0.6637, aupr_val: 68.64, auc_val: 69.98\n",
      "Epoch 00005: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Validation: Epoch 5,  val_loss:0.6617, aupr_val: 69.61, auc_val: 70.37\n",
      "Validation: Epoch 6,  val_loss:0.6624, aupr_val: 69.38, auc_val: 70.25\n",
      "Epoch 00007: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Validation: Epoch 7,  val_loss:0.6624, aupr_val: 69.38, auc_val: 70.26\n",
      "Validation: Epoch 8,  val_loss:0.6622, aupr_val: 69.43, auc_val: 70.29\n",
      "Epoch 00009: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Validation: Epoch 9,  val_loss:0.6622, aupr_val: 69.42, auc_val: 70.28\n",
      "Validation: Epoch 10,  val_loss:0.6622, aupr_val: 69.43, auc_val: 70.28\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Validation: Epoch 11,  val_loss:0.6622, aupr_val: 69.43, auc_val: 70.28\n",
      "Validation: Epoch 12,  val_loss:0.6622, aupr_val: 69.43, auc_val: 70.28\n",
      "Epoch 00013: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Validation: Epoch 13,  val_loss:0.6622, aupr_val: 69.43, auc_val: 70.28\n",
      "Validation: Epoch 14,  val_loss:0.6622, aupr_val: 69.43, auc_val: 70.28\n",
      "Validation: Epoch 15,  val_loss:0.6622, aupr_val: 69.43, auc_val: 70.28\n",
      "Validation: Epoch 16,  val_loss:0.6622, aupr_val: 69.43, auc_val: 70.28\n",
      "Validation: Epoch 17,  val_loss:0.6622, aupr_val: 69.43, auc_val: 70.28\n",
      "Validation: Epoch 18,  val_loss:0.6622, aupr_val: 69.43, auc_val: 70.28\n",
      "Validation: Epoch 19,  val_loss:0.6622, aupr_val: 69.43, auc_val: 70.28\n",
      "Validation: Epoch 20,  val_loss:0.6622, aupr_val: 69.43, auc_val: 70.28\n",
      "Validation: Epoch 21,  val_loss:0.6622, aupr_val: 69.43, auc_val: 70.28\n",
      "Validation: Epoch 22,  val_loss:0.6622, aupr_val: 69.43, auc_val: 70.28\n",
      "Validation: Epoch 23,  val_loss:0.6622, aupr_val: 69.43, auc_val: 70.28\n",
      "Validation: Epoch 24,  val_loss:0.6622, aupr_val: 69.43, auc_val: 70.28\n",
      "Total Time elapsed: 5.873 mins\n",
      "        course  percentile       acc       bac       auc     auprc\n",
      "0  progfun-003          40  0.669733  0.666381  0.719636  0.720862\n",
      "        course  percentile       acc      bac       auc     auprc\n",
      "0  progfun-003          40  0.658379  0.65671  0.718344  0.696375\n",
      "Dataset used:  P12\n",
      "Split id: 1\n",
      "8500 1063 1066 8500 1063 1066\n",
      "[4521] [565] [566]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8500, 1000, 24]) torch.Size([8500, 9]) torch.Size([8500, 1000, 1]) torch.Size([8500])\n",
      "- - Run 1 - -\n",
      "Stop epochs: 25, Batches/epoch: 62, Total batches: 1550\n",
      "Validation: Epoch 0,  val_loss:0.6668, aupr_val: 77.72, auc_val: 77.99\n",
      "**[S] Epoch 0, aupr_val: 77.7233, auc_val: 77.9884 **\n",
      "Validation: Epoch 1,  val_loss:0.6261, aupr_val: 78.13, auc_val: 79.59\n",
      "**[S] Epoch 1, aupr_val: 78.1262, auc_val: 79.5892 **\n",
      "Validation: Epoch 2,  val_loss:0.6283, aupr_val: 77.96, auc_val: 79.21\n",
      "Validation: Epoch 3,  val_loss:0.6278, aupr_val: 78.28, auc_val: 79.47\n",
      "**[S] Epoch 3, aupr_val: 78.2795, auc_val: 79.4733 **\n",
      "Validation: Epoch 4,  val_loss:0.6186, aupr_val: 78.26, auc_val: 79.47\n",
      "Validation: Epoch 5,  val_loss:0.6202, aupr_val: 78.77, auc_val: 79.45\n",
      "**[S] Epoch 5, aupr_val: 78.7655, auc_val: 79.4456 **\n",
      "Validation: Epoch 6,  val_loss:0.6274, aupr_val: 79.67, auc_val: 79.50\n",
      "**[S] Epoch 6, aupr_val: 79.6661, auc_val: 79.5035 **\n",
      "Validation: Epoch 7,  val_loss:0.6234, aupr_val: 80.02, auc_val: 79.68\n",
      "**[S] Epoch 7, aupr_val: 80.0218, auc_val: 79.6773 **\n",
      "Validation: Epoch 8,  val_loss:0.6314, aupr_val: 79.73, auc_val: 79.33\n",
      "Validation: Epoch 9,  val_loss:0.6243, aupr_val: 80.32, auc_val: 79.95\n",
      "**[S] Epoch 9, aupr_val: 80.3199, auc_val: 79.9453 **\n",
      "Validation: Epoch 10,  val_loss:0.6258, aupr_val: 80.58, auc_val: 80.04\n",
      "**[S] Epoch 10, aupr_val: 80.5786, auc_val: 80.0352 **\n",
      "Validation: Epoch 11,  val_loss:0.6240, aupr_val: 80.63, auc_val: 80.20\n",
      "**[S] Epoch 11, aupr_val: 80.6316, auc_val: 80.1994 **\n",
      "Validation: Epoch 12,  val_loss:0.6218, aupr_val: 80.72, auc_val: 80.17\n",
      "**[S] Epoch 12, aupr_val: 80.7227, auc_val: 80.1681 **\n",
      "Validation: Epoch 13,  val_loss:0.6209, aupr_val: 80.82, auc_val: 80.08\n",
      "**[S] Epoch 13, aupr_val: 80.8220, auc_val: 80.0832 **\n",
      "Validation: Epoch 14,  val_loss:0.6221, aupr_val: 80.48, auc_val: 80.19\n",
      "Validation: Epoch 15,  val_loss:0.6278, aupr_val: 80.45, auc_val: 80.10\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Validation: Epoch 16,  val_loss:0.6222, aupr_val: 80.51, auc_val: 80.12\n",
      "Validation: Epoch 17,  val_loss:0.6223, aupr_val: 80.75, auc_val: 80.18\n",
      "Epoch 00018: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Validation: Epoch 18,  val_loss:0.6219, aupr_val: 80.73, auc_val: 80.17\n",
      "Validation: Epoch 19,  val_loss:0.6215, aupr_val: 80.72, auc_val: 80.16\n",
      "Epoch 00020: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Validation: Epoch 20,  val_loss:0.6215, aupr_val: 80.72, auc_val: 80.16\n",
      "Validation: Epoch 21,  val_loss:0.6215, aupr_val: 80.71, auc_val: 80.16\n",
      "Epoch 00022: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Validation: Epoch 22,  val_loss:0.6215, aupr_val: 80.71, auc_val: 80.16\n",
      "Validation: Epoch 23,  val_loss:0.6215, aupr_val: 80.71, auc_val: 80.16\n",
      "Epoch 00024: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Validation: Epoch 24,  val_loss:0.6215, aupr_val: 80.71, auc_val: 80.16\n",
      "Total Time elapsed: 6.759 mins\n",
      "        course  percentile       acc       bac      auc     auprc\n",
      "0  progfun-003          60  0.736891  0.735142  0.81833  0.821702\n",
      "        course  percentile       acc      bac      auc    auprc\n",
      "0  progfun-003          60  0.742173  0.74085  0.80791  0.80409\n",
      "Dataset used:  P12\n",
      "Split id: 1\n",
      "3047 381 385 3047 381 385\n",
      "[740] [93] [91]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3047, 1000, 24]) torch.Size([3047, 9]) torch.Size([3047, 1000, 1]) torch.Size([3047])\n",
      "- - Run 1 - -\n",
      "Stop epochs: 25, Batches/epoch: 34, Total batches: 850\n",
      "Validation: Epoch 0,  val_loss:0.6883, aupr_val: 27.80, auc_val: 59.07\n",
      "**[S] Epoch 0, aupr_val: 27.7965, auc_val: 59.0688 **\n",
      "Validation: Epoch 1,  val_loss:0.6547, aupr_val: 54.96, auc_val: 78.99\n",
      "**[S] Epoch 1, aupr_val: 54.9578, auc_val: 78.9912 **\n",
      "Validation: Epoch 2,  val_loss:0.6205, aupr_val: 60.14, auc_val: 81.59\n",
      "**[S] Epoch 2, aupr_val: 60.1442, auc_val: 81.5860 **\n",
      "Validation: Epoch 3,  val_loss:0.6134, aupr_val: 61.31, auc_val: 81.75\n",
      "**[S] Epoch 3, aupr_val: 61.3130, auc_val: 81.7466 **\n",
      "Validation: Epoch 4,  val_loss:0.6168, aupr_val: 62.48, auc_val: 82.43\n",
      "**[S] Epoch 4, aupr_val: 62.4788, auc_val: 82.4335 **\n",
      "Validation: Epoch 5,  val_loss:0.6167, aupr_val: 59.98, auc_val: 81.20\n",
      "Validation: Epoch 6,  val_loss:0.6156, aupr_val: 62.77, auc_val: 82.65\n",
      "**[S] Epoch 6, aupr_val: 62.7682, auc_val: 82.6464 **\n",
      "Validation: Epoch 7,  val_loss:0.6120, aupr_val: 63.58, auc_val: 82.49\n",
      "**[S] Epoch 7, aupr_val: 63.5753, auc_val: 82.4895 **\n",
      "Validation: Epoch 8,  val_loss:0.6037, aupr_val: 62.36, auc_val: 82.29\n",
      "Validation: Epoch 9,  val_loss:0.6206, aupr_val: 63.30, auc_val: 82.58\n",
      "Epoch 00010: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Validation: Epoch 10,  val_loss:0.6094, aupr_val: 63.42, auc_val: 82.56\n",
      "Validation: Epoch 11,  val_loss:0.6074, aupr_val: 63.36, auc_val: 82.60\n",
      "Epoch 00012: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Validation: Epoch 12,  val_loss:0.6073, aupr_val: 63.45, auc_val: 82.64\n",
      "Validation: Epoch 13,  val_loss:0.6073, aupr_val: 63.48, auc_val: 82.65\n",
      "Epoch 00014: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Validation: Epoch 14,  val_loss:0.6073, aupr_val: 63.47, auc_val: 82.65\n",
      "Validation: Epoch 15,  val_loss:0.6072, aupr_val: 63.47, auc_val: 82.65\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Validation: Epoch 16,  val_loss:0.6072, aupr_val: 63.47, auc_val: 82.65\n",
      "Validation: Epoch 17,  val_loss:0.6072, aupr_val: 63.47, auc_val: 82.65\n",
      "Epoch 00018: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Validation: Epoch 18,  val_loss:0.6072, aupr_val: 63.47, auc_val: 82.65\n",
      "Validation: Epoch 19,  val_loss:0.6072, aupr_val: 63.47, auc_val: 82.65\n",
      "Validation: Epoch 20,  val_loss:0.6072, aupr_val: 63.47, auc_val: 82.65\n",
      "Validation: Epoch 21,  val_loss:0.6072, aupr_val: 63.47, auc_val: 82.65\n",
      "Validation: Epoch 22,  val_loss:0.6072, aupr_val: 63.47, auc_val: 82.65\n",
      "Validation: Epoch 23,  val_loss:0.6072, aupr_val: 63.47, auc_val: 82.65\n",
      "Validation: Epoch 24,  val_loss:0.6072, aupr_val: 63.47, auc_val: 82.65\n",
      "Total Time elapsed: 3.608 mins\n",
      "    course  percentile       acc       bac       auc     auprc\n",
      "0  dsp-002          40  0.713568  0.689794  0.764781  0.492905\n",
      "    course  percentile       acc       bac       auc     auprc\n",
      "0  dsp-002          40  0.790932  0.773928  0.834218  0.640675\n",
      "Dataset used:  P12\n",
      "Split id: 1\n",
      "3102 390 392 3102 390 392\n",
      "[742] [93] [93]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3102, 1000, 24]) torch.Size([3102, 9]) torch.Size([3102, 1000, 1]) torch.Size([3102])\n",
      "- - Run 1 - -\n",
      "Stop epochs: 25, Batches/epoch: 34, Total batches: 850\n",
      "Validation: Epoch 0,  val_loss:0.6810, aupr_val: 28.61, auc_val: 62.91\n",
      "**[S] Epoch 0, aupr_val: 28.6143, auc_val: 62.9050 **\n",
      "Validation: Epoch 1,  val_loss:0.6144, aupr_val: 78.49, auc_val: 88.99\n",
      "**[S] Epoch 1, aupr_val: 78.4917, auc_val: 88.9866 **\n",
      "Validation: Epoch 2,  val_loss:0.5342, aupr_val: 78.81, auc_val: 91.37\n",
      "**[S] Epoch 2, aupr_val: 78.8080, auc_val: 91.3725 **\n",
      "Validation: Epoch 3,  val_loss:0.5367, aupr_val: 78.31, auc_val: 91.62\n",
      "Validation: Epoch 4,  val_loss:0.5275, aupr_val: 81.21, auc_val: 91.97\n",
      "**[S] Epoch 4, aupr_val: 81.2101, auc_val: 91.9663 **\n",
      "Validation: Epoch 5,  val_loss:0.5564, aupr_val: 79.46, auc_val: 91.70\n",
      "Validation: Epoch 6,  val_loss:0.5456, aupr_val: 80.96, auc_val: 91.97\n",
      "Epoch 00007: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Validation: Epoch 7,  val_loss:0.5350, aupr_val: 81.05, auc_val: 91.95\n",
      "Validation: Epoch 8,  val_loss:0.5315, aupr_val: 81.15, auc_val: 91.97\n",
      "Epoch 00009: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Validation: Epoch 9,  val_loss:0.5312, aupr_val: 81.17, auc_val: 91.98\n",
      "Validation: Epoch 10,  val_loss:0.5310, aupr_val: 81.22, auc_val: 91.99\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-06.\n",
      "**[S] Epoch 10, aupr_val: 81.2235, auc_val: 91.9880 **\n",
      "Validation: Epoch 11,  val_loss:0.5311, aupr_val: 81.22, auc_val: 91.99\n",
      "Validation: Epoch 12,  val_loss:0.5312, aupr_val: 81.22, auc_val: 91.99\n",
      "Epoch 00013: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Validation: Epoch 13,  val_loss:0.5312, aupr_val: 81.22, auc_val: 91.99\n",
      "Validation: Epoch 14,  val_loss:0.5312, aupr_val: 81.22, auc_val: 91.99\n",
      "Epoch 00015: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Validation: Epoch 15,  val_loss:0.5312, aupr_val: 81.22, auc_val: 91.99\n",
      "Validation: Epoch 16,  val_loss:0.5312, aupr_val: 81.22, auc_val: 91.99\n",
      "Validation: Epoch 17,  val_loss:0.5312, aupr_val: 81.22, auc_val: 91.99\n",
      "Validation: Epoch 18,  val_loss:0.5312, aupr_val: 81.22, auc_val: 91.99\n",
      "Validation: Epoch 19,  val_loss:0.5312, aupr_val: 81.22, auc_val: 91.99\n",
      "Validation: Epoch 20,  val_loss:0.5312, aupr_val: 81.22, auc_val: 91.99\n",
      "Validation: Epoch 21,  val_loss:0.5312, aupr_val: 81.22, auc_val: 91.99\n",
      "Validation: Epoch 22,  val_loss:0.5312, aupr_val: 81.22, auc_val: 91.99\n",
      "Validation: Epoch 23,  val_loss:0.5312, aupr_val: 81.22, auc_val: 91.99\n",
      "Validation: Epoch 24,  val_loss:0.5312, aupr_val: 81.22, auc_val: 91.99\n",
      "Total Time elapsed: 3.725 mins\n",
      "    course  percentile       acc       bac       auc     auprc\n",
      "0  dsp-002          60  0.819095  0.837123  0.883342  0.675479\n",
      "    course  percentile       acc      bac       auc     auprc\n",
      "0  dsp-002          60  0.861461  0.86476  0.922114  0.813304\n",
      "Dataset used:  P12\n",
      "Split id: 1\n",
      "77 10 10 77 10 10\n",
      "[65] [8] [9]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([77, 1000, 26]) torch.Size([77, 9]) torch.Size([77, 1000, 1]) torch.Size([77])\n",
      "- - Run 1 - -\n",
      "Stop epochs: 25, Batches/epoch: 0, Total batches: 0\n",
      "Validation: Epoch 0,  val_loss:0.6932, aupr_val: 80.00, auc_val: 50.00\n",
      "**[S] Epoch 0, aupr_val: 80.0000, auc_val: 50.0000 **\n",
      "Validation: Epoch 1,  val_loss:0.6932, aupr_val: 80.00, auc_val: 50.00\n",
      "Validation: Epoch 2,  val_loss:0.6932, aupr_val: 80.00, auc_val: 50.00\n",
      "Epoch 00003: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Validation: Epoch 3,  val_loss:0.6932, aupr_val: 80.00, auc_val: 50.00\n",
      "Validation: Epoch 4,  val_loss:0.6932, aupr_val: 80.00, auc_val: 50.00\n",
      "Epoch 00005: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Validation: Epoch 5,  val_loss:0.6932, aupr_val: 80.00, auc_val: 50.00\n",
      "Validation: Epoch 6,  val_loss:0.6932, aupr_val: 80.00, auc_val: 50.00\n",
      "Epoch 00007: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Validation: Epoch 7,  val_loss:0.6932, aupr_val: 80.00, auc_val: 50.00\n",
      "Validation: Epoch 8,  val_loss:0.6932, aupr_val: 80.00, auc_val: 50.00\n",
      "Epoch 00009: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Validation: Epoch 9,  val_loss:0.6932, aupr_val: 80.00, auc_val: 50.00\n",
      "Validation: Epoch 10,  val_loss:0.6932, aupr_val: 80.00, auc_val: 50.00\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Validation: Epoch 11,  val_loss:0.6932, aupr_val: 80.00, auc_val: 50.00\n",
      "Validation: Epoch 12,  val_loss:0.6932, aupr_val: 80.00, auc_val: 50.00\n",
      "Validation: Epoch 13,  val_loss:0.6932, aupr_val: 80.00, auc_val: 50.00\n",
      "Validation: Epoch 14,  val_loss:0.6932, aupr_val: 80.00, auc_val: 50.00\n",
      "Validation: Epoch 15,  val_loss:0.6932, aupr_val: 80.00, auc_val: 50.00\n",
      "Validation: Epoch 16,  val_loss:0.6932, aupr_val: 80.00, auc_val: 50.00\n",
      "Validation: Epoch 17,  val_loss:0.6932, aupr_val: 80.00, auc_val: 50.00\n",
      "Validation: Epoch 18,  val_loss:0.6932, aupr_val: 80.00, auc_val: 50.00\n",
      "Validation: Epoch 19,  val_loss:0.6932, aupr_val: 80.00, auc_val: 50.00\n",
      "Validation: Epoch 20,  val_loss:0.6932, aupr_val: 80.00, auc_val: 50.00\n",
      "Validation: Epoch 21,  val_loss:0.6932, aupr_val: 80.00, auc_val: 50.00\n",
      "Validation: Epoch 22,  val_loss:0.6932, aupr_val: 80.00, auc_val: 50.00\n",
      "Validation: Epoch 23,  val_loss:0.6932, aupr_val: 80.00, auc_val: 50.00\n",
      "Validation: Epoch 24,  val_loss:0.6932, aupr_val: 80.00, auc_val: 50.00\n",
      "Total Time elapsed: 0.006 mins\n",
      "           course  percentile  acc  bac  auc  auprc\n",
      "0  structures-002          40  0.1  0.5  0.5    0.9\n",
      "           course  percentile  acc  bac  auc  auprc\n",
      "0  structures-002          40  0.2  0.5  0.5    0.8\n",
      "Dataset used:  P12\n",
      "Split id: 1\n",
      "77 10 10 77 10 10\n",
      "[65] [8] [9]\n",
      "torch.Size([77, 1000, 26]) torch.Size([77, 9]) torch.Size([77, 1000, 1]) torch.Size([77])\n",
      "- - Run 1 - -\n",
      "Stop epochs: 25, Batches/epoch: 0, Total batches: 0\n",
      "Validation: Epoch 0,  val_loss:0.6982, aupr_val: 80.00, auc_val: 50.00\n",
      "**[S] Epoch 0, aupr_val: 80.0000, auc_val: 50.0000 **\n",
      "Validation: Epoch 1,  val_loss:0.6982, aupr_val: 80.00, auc_val: 50.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch 2,  val_loss:0.6982, aupr_val: 80.00, auc_val: 50.00\n",
      "Epoch 00003: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Validation: Epoch 3,  val_loss:0.6982, aupr_val: 80.00, auc_val: 50.00\n",
      "Validation: Epoch 4,  val_loss:0.6982, aupr_val: 80.00, auc_val: 50.00\n",
      "Epoch 00005: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Validation: Epoch 5,  val_loss:0.6982, aupr_val: 80.00, auc_val: 50.00\n",
      "Validation: Epoch 6,  val_loss:0.6982, aupr_val: 80.00, auc_val: 50.00\n",
      "Epoch 00007: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Validation: Epoch 7,  val_loss:0.6982, aupr_val: 80.00, auc_val: 50.00\n",
      "Validation: Epoch 8,  val_loss:0.6982, aupr_val: 80.00, auc_val: 50.00\n",
      "Epoch 00009: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Validation: Epoch 9,  val_loss:0.6982, aupr_val: 80.00, auc_val: 50.00\n",
      "Validation: Epoch 10,  val_loss:0.6982, aupr_val: 80.00, auc_val: 50.00\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Validation: Epoch 11,  val_loss:0.6982, aupr_val: 80.00, auc_val: 50.00\n",
      "Validation: Epoch 12,  val_loss:0.6982, aupr_val: 80.00, auc_val: 50.00\n",
      "Validation: Epoch 13,  val_loss:0.6982, aupr_val: 80.00, auc_val: 50.00\n",
      "Validation: Epoch 14,  val_loss:0.6982, aupr_val: 80.00, auc_val: 50.00\n",
      "Validation: Epoch 15,  val_loss:0.6982, aupr_val: 80.00, auc_val: 50.00\n",
      "Validation: Epoch 16,  val_loss:0.6982, aupr_val: 80.00, auc_val: 50.00\n",
      "Validation: Epoch 17,  val_loss:0.6982, aupr_val: 80.00, auc_val: 50.00\n",
      "Validation: Epoch 18,  val_loss:0.6982, aupr_val: 80.00, auc_val: 50.00\n",
      "Validation: Epoch 19,  val_loss:0.6982, aupr_val: 80.00, auc_val: 50.00\n",
      "Validation: Epoch 20,  val_loss:0.6982, aupr_val: 80.00, auc_val: 50.00\n",
      "Validation: Epoch 21,  val_loss:0.6982, aupr_val: 80.00, auc_val: 50.00\n",
      "Validation: Epoch 22,  val_loss:0.6982, aupr_val: 80.00, auc_val: 50.00\n",
      "Validation: Epoch 23,  val_loss:0.6982, aupr_val: 80.00, auc_val: 50.00\n",
      "Validation: Epoch 24,  val_loss:0.6982, aupr_val: 80.00, auc_val: 50.00\n",
      "Total Time elapsed: 0.007 mins\n",
      "           course  percentile  acc  bac  auc  auprc\n",
      "0  structures-002          60  0.1  0.5  0.5    0.9\n",
      "           course  percentile  acc  bac  auc  auprc\n",
      "0  structures-002          60  0.2  0.5  0.5    0.8\n",
      "Dataset used:  P12\n",
      "Split id: 1\n",
      "542 67 72 542 67 72\n",
      "[353] [45] [47]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([542, 1000, 26]) torch.Size([542, 9]) torch.Size([542, 1000, 1]) torch.Size([542])\n",
      "- - Run 1 - -\n",
      "Stop epochs: 25, Batches/epoch: 2, Total batches: 50\n",
      "Validation: Epoch 0,  val_loss:0.6914, aupr_val: 72.79, auc_val: 51.52\n",
      "**[S] Epoch 0, aupr_val: 72.7898, auc_val: 51.5152 **\n",
      "Validation: Epoch 1,  val_loss:0.6941, aupr_val: 75.16, auc_val: 54.65\n",
      "**[S] Epoch 1, aupr_val: 75.1618, auc_val: 54.6465 **\n",
      "Validation: Epoch 2,  val_loss:0.6932, aupr_val: 75.47, auc_val: 56.06\n",
      "**[S] Epoch 2, aupr_val: 75.4666, auc_val: 56.0606 **\n",
      "Validation: Epoch 3,  val_loss:0.6914, aupr_val: 75.40, auc_val: 56.97\n",
      "Validation: Epoch 4,  val_loss:0.6916, aupr_val: 75.74, auc_val: 59.49\n",
      "**[S] Epoch 4, aupr_val: 75.7378, auc_val: 59.4949 **\n",
      "Validation: Epoch 5,  val_loss:0.6920, aupr_val: 76.91, auc_val: 61.31\n",
      "**[S] Epoch 5, aupr_val: 76.9143, auc_val: 61.3131 **\n",
      "Validation: Epoch 6,  val_loss:0.6901, aupr_val: 77.45, auc_val: 62.73\n",
      "**[S] Epoch 6, aupr_val: 77.4500, auc_val: 62.7273 **\n",
      "Validation: Epoch 7,  val_loss:0.6918, aupr_val: 77.27, auc_val: 63.33\n",
      "Validation: Epoch 8,  val_loss:0.6880, aupr_val: 77.74, auc_val: 63.74\n",
      "**[S] Epoch 8, aupr_val: 77.7379, auc_val: 63.7374 **\n",
      "Validation: Epoch 9,  val_loss:0.6898, aupr_val: 78.38, auc_val: 64.34\n",
      "**[S] Epoch 9, aupr_val: 78.3752, auc_val: 64.3434 **\n",
      "Validation: Epoch 10,  val_loss:0.6874, aupr_val: 77.97, auc_val: 63.43\n",
      "Validation: Epoch 11,  val_loss:0.6902, aupr_val: 77.34, auc_val: 63.33\n",
      "Epoch 00012: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Validation: Epoch 12,  val_loss:0.6898, aupr_val: 77.07, auc_val: 62.83\n",
      "Validation: Epoch 13,  val_loss:0.6887, aupr_val: 76.80, auc_val: 62.53\n",
      "Epoch 00014: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Validation: Epoch 14,  val_loss:0.6886, aupr_val: 76.80, auc_val: 62.53\n",
      "Validation: Epoch 15,  val_loss:0.6885, aupr_val: 76.80, auc_val: 62.53\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Validation: Epoch 16,  val_loss:0.6885, aupr_val: 76.80, auc_val: 62.53\n",
      "Validation: Epoch 17,  val_loss:0.6884, aupr_val: 76.80, auc_val: 62.53\n",
      "Epoch 00018: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Validation: Epoch 18,  val_loss:0.6884, aupr_val: 76.80, auc_val: 62.53\n",
      "Validation: Epoch 19,  val_loss:0.6884, aupr_val: 76.80, auc_val: 62.53\n",
      "Epoch 00020: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Validation: Epoch 20,  val_loss:0.6884, aupr_val: 76.80, auc_val: 62.53\n",
      "Validation: Epoch 21,  val_loss:0.6884, aupr_val: 76.80, auc_val: 62.53\n",
      "Validation: Epoch 22,  val_loss:0.6884, aupr_val: 76.80, auc_val: 62.53\n",
      "Validation: Epoch 23,  val_loss:0.6884, aupr_val: 76.80, auc_val: 62.53\n",
      "Validation: Epoch 24,  val_loss:0.6884, aupr_val: 76.80, auc_val: 62.53\n",
      "Total Time elapsed: 0.216 mins\n",
      "            course  percentile       acc       bac      auc     auprc\n",
      "0  initprogcpp-001          40  0.410959  0.422259  0.43126  0.593362\n",
      "            course  percentile      acc       bac      auc    auprc\n",
      "0  initprogcpp-001          40  0.60274  0.638889  0.69847  0.78161\n",
      "Dataset used:  P12\n",
      "Split id: 1\n",
      "576 70 73 576 70 73\n",
      "[367] [46] [47]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([576, 1000, 26]) torch.Size([576, 9]) torch.Size([576, 1000, 1]) torch.Size([576])\n",
      "- - Run 1 - -\n",
      "Stop epochs: 25, Batches/epoch: 3, Total batches: 75\n",
      "Validation: Epoch 0,  val_loss:0.6934, aupr_val: 64.85, auc_val: 48.91\n",
      "**[S] Epoch 0, aupr_val: 64.8493, auc_val: 48.9130 **\n",
      "Validation: Epoch 1,  val_loss:0.6926, aupr_val: 67.31, auc_val: 52.26\n",
      "**[S] Epoch 1, aupr_val: 67.3135, auc_val: 52.2645 **\n",
      "Validation: Epoch 2,  val_loss:0.6939, aupr_val: 67.80, auc_val: 53.99\n",
      "**[S] Epoch 2, aupr_val: 67.8033, auc_val: 53.9855 **\n",
      "Validation: Epoch 3,  val_loss:0.6923, aupr_val: 67.94, auc_val: 55.07\n",
      "**[S] Epoch 3, aupr_val: 67.9414, auc_val: 55.0725 **\n",
      "Validation: Epoch 4,  val_loss:0.6934, aupr_val: 68.16, auc_val: 55.34\n",
      "**[S] Epoch 4, aupr_val: 68.1580, auc_val: 55.3442 **\n",
      "Validation: Epoch 5,  val_loss:0.6913, aupr_val: 68.42, auc_val: 55.71\n",
      "**[S] Epoch 5, aupr_val: 68.4236, auc_val: 55.7065 **\n",
      "Validation: Epoch 6,  val_loss:0.6917, aupr_val: 69.75, auc_val: 56.70\n",
      "**[S] Epoch 6, aupr_val: 69.7518, auc_val: 56.7029 **\n",
      "Validation: Epoch 7,  val_loss:0.6919, aupr_val: 70.54, auc_val: 57.16\n",
      "**[S] Epoch 7, aupr_val: 70.5375, auc_val: 57.1558 **\n",
      "Validation: Epoch 8,  val_loss:0.6945, aupr_val: 72.86, auc_val: 58.06\n",
      "**[S] Epoch 8, aupr_val: 72.8627, auc_val: 58.0616 **\n",
      "Validation: Epoch 9,  val_loss:0.6925, aupr_val: 74.02, auc_val: 58.42\n",
      "**[S] Epoch 9, aupr_val: 74.0237, auc_val: 58.4239 **\n",
      "Validation: Epoch 10,  val_loss:0.6890, aupr_val: 74.76, auc_val: 59.24\n",
      "**[S] Epoch 10, aupr_val: 74.7630, auc_val: 59.2391 **\n",
      "Validation: Epoch 11,  val_loss:0.6882, aupr_val: 76.42, auc_val: 60.78\n",
      "**[S] Epoch 11, aupr_val: 76.4188, auc_val: 60.7790 **\n",
      "Validation: Epoch 12,  val_loss:0.6840, aupr_val: 77.16, auc_val: 61.96\n",
      "**[S] Epoch 12, aupr_val: 77.1584, auc_val: 61.9565 **\n",
      "Validation: Epoch 13,  val_loss:0.6876, aupr_val: 77.54, auc_val: 62.68\n",
      "**[S] Epoch 13, aupr_val: 77.5421, auc_val: 62.6812 **\n",
      "Validation: Epoch 14,  val_loss:0.6857, aupr_val: 77.25, auc_val: 61.87\n",
      "Validation: Epoch 15,  val_loss:0.6899, aupr_val: 78.54, auc_val: 62.05\n",
      "**[S] Epoch 15, aupr_val: 78.5421, auc_val: 62.0471 **\n",
      "Validation: Epoch 16,  val_loss:0.6756, aupr_val: 79.21, auc_val: 62.59\n",
      "**[S] Epoch 16, aupr_val: 79.2098, auc_val: 62.5906 **\n",
      "Validation: Epoch 17,  val_loss:0.6925, aupr_val: 79.56, auc_val: 63.22\n",
      "**[S] Epoch 17, aupr_val: 79.5594, auc_val: 63.2246 **\n",
      "Validation: Epoch 18,  val_loss:0.6781, aupr_val: 79.70, auc_val: 63.50\n",
      "**[S] Epoch 18, aupr_val: 79.6984, auc_val: 63.4964 **\n",
      "Validation: Epoch 19,  val_loss:0.6896, aupr_val: 79.92, auc_val: 64.58\n",
      "**[S] Epoch 19, aupr_val: 79.9209, auc_val: 64.5833 **\n",
      "Validation: Epoch 20,  val_loss:0.6816, aupr_val: 80.16, auc_val: 65.04\n",
      "**[S] Epoch 20, aupr_val: 80.1615, auc_val: 65.0362 **\n",
      "Validation: Epoch 21,  val_loss:0.6841, aupr_val: 80.14, auc_val: 65.22\n",
      "Validation: Epoch 22,  val_loss:0.6801, aupr_val: 80.08, auc_val: 65.22\n",
      "Epoch 00023: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Validation: Epoch 23,  val_loss:0.6816, aupr_val: 80.06, auc_val: 65.31\n",
      "Validation: Epoch 24,  val_loss:0.6830, aupr_val: 80.09, auc_val: 65.22\n",
      "Epoch 00025: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Total Time elapsed: 0.358 mins\n",
      "            course  percentile       acc       bac       auc     auprc\n",
      "0  initprogcpp-001          60  0.493151  0.486088  0.601473  0.770536\n",
      "            course  percentile       acc       bac      auc     auprc\n",
      "0  initprogcpp-001          60  0.630137  0.637681  0.68599  0.799785\n",
      "Dataset used:  P12\n",
      "Split id: 1\n",
      "360 45 46 360 45 46\n",
      "[273] [34] [35]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([360, 1000, 12]) torch.Size([360, 9]) torch.Size([360, 1000, 1]) torch.Size([360])\n",
      "- - Run 1 - -\n",
      "Stop epochs: 25, Batches/epoch: 1, Total batches: 25\n",
      "Validation: Epoch 0,  val_loss:0.6980, aupr_val: 69.42, auc_val: 34.76\n",
      "**[S] Epoch 0, aupr_val: 69.4158, auc_val: 34.7594 **\n",
      "Validation: Epoch 1,  val_loss:0.6926, aupr_val: 69.99, auc_val: 45.99\n",
      "**[S] Epoch 1, aupr_val: 69.9864, auc_val: 45.9893 **\n",
      "Validation: Epoch 2,  val_loss:0.6890, aupr_val: 71.89, auc_val: 52.14\n",
      "**[S] Epoch 2, aupr_val: 71.8879, auc_val: 52.1390 **\n",
      "Validation: Epoch 3,  val_loss:0.6884, aupr_val: 74.48, auc_val: 57.49\n",
      "**[S] Epoch 3, aupr_val: 74.4760, auc_val: 57.4866 **\n",
      "Validation: Epoch 4,  val_loss:0.6897, aupr_val: 75.77, auc_val: 59.89\n",
      "**[S] Epoch 4, aupr_val: 75.7701, auc_val: 59.8930 **\n",
      "Validation: Epoch 5,  val_loss:0.6919, aupr_val: 76.03, auc_val: 60.70\n",
      "**[S] Epoch 5, aupr_val: 76.0283, auc_val: 60.6952 **\n",
      "Validation: Epoch 6,  val_loss:0.6938, aupr_val: 76.56, auc_val: 62.83\n",
      "**[S] Epoch 6, aupr_val: 76.5562, auc_val: 62.8342 **\n",
      "Validation: Epoch 7,  val_loss:0.6947, aupr_val: 77.12, auc_val: 64.97\n",
      "**[S] Epoch 7, aupr_val: 77.1181, auc_val: 64.9733 **\n",
      "Validation: Epoch 8,  val_loss:0.6943, aupr_val: 77.57, auc_val: 66.84\n",
      "**[S] Epoch 8, aupr_val: 77.5659, auc_val: 66.8449 **\n",
      "Validation: Epoch 9,  val_loss:0.6930, aupr_val: 77.43, auc_val: 66.31\n",
      "Validation: Epoch 10,  val_loss:0.6913, aupr_val: 77.48, auc_val: 66.58\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Validation: Epoch 11,  val_loss:0.6911, aupr_val: 77.73, auc_val: 66.84\n",
      "**[S] Epoch 11, aupr_val: 77.7256, auc_val: 66.8449 **\n",
      "Validation: Epoch 12,  val_loss:0.6910, aupr_val: 77.73, auc_val: 66.84\n",
      "Validation: Epoch 13,  val_loss:0.6909, aupr_val: 77.64, auc_val: 66.58\n",
      "Epoch 00014: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Validation: Epoch 14,  val_loss:0.6909, aupr_val: 77.64, auc_val: 66.58\n",
      "Validation: Epoch 15,  val_loss:0.6909, aupr_val: 77.64, auc_val: 66.58\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Validation: Epoch 16,  val_loss:0.6909, aupr_val: 77.64, auc_val: 66.58\n",
      "Validation: Epoch 17,  val_loss:0.6909, aupr_val: 77.64, auc_val: 66.58\n",
      "Epoch 00018: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Validation: Epoch 18,  val_loss:0.6909, aupr_val: 77.64, auc_val: 66.58\n",
      "Validation: Epoch 19,  val_loss:0.6909, aupr_val: 77.64, auc_val: 66.58\n",
      "Epoch 00020: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Validation: Epoch 20,  val_loss:0.6909, aupr_val: 77.64, auc_val: 66.58\n",
      "Validation: Epoch 21,  val_loss:0.6909, aupr_val: 77.64, auc_val: 66.58\n",
      "Validation: Epoch 22,  val_loss:0.6909, aupr_val: 77.64, auc_val: 66.58\n",
      "Validation: Epoch 23,  val_loss:0.6909, aupr_val: 77.64, auc_val: 66.58\n",
      "Validation: Epoch 24,  val_loss:0.6909, aupr_val: 77.64, auc_val: 66.58\n",
      "Total Time elapsed: 0.071 mins\n",
      "                 course  percentile       acc       bac       auc     auprc\n",
      "0  analysenumerique-003          40  0.586957  0.448052  0.425974  0.710409\n",
      "                 course  percentile      acc       bac       auc     auprc\n",
      "0  analysenumerique-003          40  0.76087  0.703431  0.698529  0.778017\n",
      "Dataset used:  P12\n",
      "Split id: 1\n",
      "362 45 46 362 45 46\n",
      "[274] [34] [35]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([362, 1000, 12]) torch.Size([362, 9]) torch.Size([362, 1000, 1]) torch.Size([362])\n",
      "- - Run 1 - -\n",
      "Stop epochs: 25, Batches/epoch: 1, Total batches: 25\n",
      "Validation: Epoch 0,  val_loss:0.6897, aupr_val: 70.84, auc_val: 47.59\n",
      "**[S] Epoch 0, aupr_val: 70.8440, auc_val: 47.5936 **\n",
      "Validation: Epoch 1,  val_loss:0.6913, aupr_val: 69.75, auc_val: 43.05\n",
      "Validation: Epoch 2,  val_loss:0.6940, aupr_val: 70.48, auc_val: 46.52\n",
      "Epoch 00003: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Validation: Epoch 3,  val_loss:0.6941, aupr_val: 70.44, auc_val: 46.52\n",
      "Validation: Epoch 4,  val_loss:0.6940, aupr_val: 70.46, auc_val: 46.52\n",
      "Epoch 00005: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Validation: Epoch 5,  val_loss:0.6940, aupr_val: 70.63, auc_val: 46.79\n",
      "Validation: Epoch 6,  val_loss:0.6940, aupr_val: 70.89, auc_val: 47.33\n",
      "Epoch 00007: reducing learning rate of group 0 to 1.0000e-06.\n",
      "**[S] Epoch 6, aupr_val: 70.8863, auc_val: 47.3262 **\n",
      "Validation: Epoch 7,  val_loss:0.6940, aupr_val: 70.89, auc_val: 47.33\n",
      "Validation: Epoch 8,  val_loss:0.6940, aupr_val: 70.89, auc_val: 47.33\n",
      "Epoch 00009: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Validation: Epoch 9,  val_loss:0.6940, aupr_val: 70.89, auc_val: 47.33\n",
      "Validation: Epoch 10,  val_loss:0.6940, aupr_val: 70.89, auc_val: 47.33\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Validation: Epoch 11,  val_loss:0.6940, aupr_val: 70.89, auc_val: 47.33\n",
      "Validation: Epoch 12,  val_loss:0.6940, aupr_val: 70.89, auc_val: 47.33\n",
      "Validation: Epoch 13,  val_loss:0.6940, aupr_val: 70.89, auc_val: 47.33\n",
      "Validation: Epoch 14,  val_loss:0.6940, aupr_val: 70.89, auc_val: 47.33\n",
      "Validation: Epoch 15,  val_loss:0.6940, aupr_val: 70.89, auc_val: 47.33\n",
      "Validation: Epoch 16,  val_loss:0.6940, aupr_val: 70.89, auc_val: 47.33\n",
      "Validation: Epoch 17,  val_loss:0.6940, aupr_val: 70.89, auc_val: 47.33\n",
      "Validation: Epoch 18,  val_loss:0.6940, aupr_val: 70.89, auc_val: 47.33\n",
      "Validation: Epoch 19,  val_loss:0.6940, aupr_val: 70.89, auc_val: 47.33\n",
      "Validation: Epoch 20,  val_loss:0.6940, aupr_val: 70.89, auc_val: 47.33\n",
      "Validation: Epoch 21,  val_loss:0.6940, aupr_val: 70.89, auc_val: 47.33\n",
      "Validation: Epoch 22,  val_loss:0.6940, aupr_val: 70.89, auc_val: 47.33\n",
      "Validation: Epoch 23,  val_loss:0.6940, aupr_val: 70.89, auc_val: 47.33\n",
      "Validation: Epoch 24,  val_loss:0.6940, aupr_val: 70.89, auc_val: 47.33\n",
      "Total Time elapsed: 0.075 mins\n",
      "                 course  percentile       acc       bac       auc     auprc\n",
      "0  analysenumerique-003          60  0.217391  0.454545  0.444156  0.687939\n",
      "                 course  percentile      acc       bac       auc     auprc\n",
      "0  analysenumerique-003          60  0.23913  0.458333  0.519608  0.709559\n",
      "Dataset used:  P12\n",
      "Split id: 1\n",
      "840 109 102 840 109 102\n",
      "[122] [16] [14]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([840, 1000, 26]) torch.Size([840, 9]) torch.Size([840, 1000, 1]) torch.Size([840])\n",
      "- - Run 1 - -\n",
      "Stop epochs: 25, Batches/epoch: 5, Total batches: 125\n",
      "Validation: Epoch 0,  val_loss:0.6926, aupr_val: 24.64, auc_val: 63.98\n",
      "**[S] Epoch 0, aupr_val: 24.6403, auc_val: 63.9785 **\n",
      "Validation: Epoch 1,  val_loss:0.6891, aupr_val: 28.45, auc_val: 64.65\n",
      "**[S] Epoch 1, aupr_val: 28.4497, auc_val: 64.6505 **\n",
      "Validation: Epoch 2,  val_loss:0.6783, aupr_val: 30.00, auc_val: 65.86\n",
      "**[S] Epoch 2, aupr_val: 30.0000, auc_val: 65.8602 **\n",
      "Validation: Epoch 3,  val_loss:0.6627, aupr_val: 30.09, auc_val: 67.27\n",
      "**[S] Epoch 3, aupr_val: 30.0865, auc_val: 67.2715 **\n",
      "Validation: Epoch 4,  val_loss:0.6698, aupr_val: 27.46, auc_val: 67.54\n",
      "Validation: Epoch 5,  val_loss:0.6539, aupr_val: 31.70, auc_val: 68.21\n",
      "**[S] Epoch 5, aupr_val: 31.6976, auc_val: 68.2124 **\n",
      "Validation: Epoch 6,  val_loss:0.6587, aupr_val: 28.76, auc_val: 69.42\n",
      "Validation: Epoch 7,  val_loss:0.6645, aupr_val: 29.22, auc_val: 69.76\n",
      "Epoch 00008: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Validation: Epoch 8,  val_loss:0.6638, aupr_val: 29.36, auc_val: 69.83\n",
      "Validation: Epoch 9,  val_loss:0.6650, aupr_val: 29.28, auc_val: 69.83\n",
      "Epoch 00010: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Validation: Epoch 10,  val_loss:0.6649, aupr_val: 29.23, auc_val: 69.76\n",
      "Validation: Epoch 11,  val_loss:0.6646, aupr_val: 29.23, auc_val: 69.76\n",
      "Epoch 00012: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Validation: Epoch 12,  val_loss:0.6646, aupr_val: 29.23, auc_val: 69.76\n",
      "Validation: Epoch 13,  val_loss:0.6646, aupr_val: 29.23, auc_val: 69.76\n",
      "Epoch 00014: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Validation: Epoch 14,  val_loss:0.6646, aupr_val: 29.23, auc_val: 69.76\n",
      "Validation: Epoch 15,  val_loss:0.6646, aupr_val: 29.23, auc_val: 69.76\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Validation: Epoch 16,  val_loss:0.6646, aupr_val: 29.23, auc_val: 69.76\n",
      "Validation: Epoch 17,  val_loss:0.6646, aupr_val: 29.23, auc_val: 69.76\n",
      "Validation: Epoch 18,  val_loss:0.6646, aupr_val: 29.23, auc_val: 69.76\n",
      "Validation: Epoch 19,  val_loss:0.6646, aupr_val: 29.23, auc_val: 69.76\n",
      "Validation: Epoch 20,  val_loss:0.6646, aupr_val: 29.23, auc_val: 69.76\n",
      "Validation: Epoch 21,  val_loss:0.6646, aupr_val: 29.23, auc_val: 69.76\n",
      "Validation: Epoch 22,  val_loss:0.6646, aupr_val: 29.23, auc_val: 69.76\n",
      "Validation: Epoch 23,  val_loss:0.6646, aupr_val: 29.23, auc_val: 69.76\n",
      "Validation: Epoch 24,  val_loss:0.6646, aupr_val: 29.23, auc_val: 69.76\n",
      "Total Time elapsed: 0.400 mins\n",
      "                 course  percentile       acc       bac       auc    auprc\n",
      "0  microcontroleurs-006          40  0.782313  0.658397  0.727099  0.28489\n",
      "                 course  percentile       acc       bac       auc    auprc\n",
      "0  microcontroleurs-006          40  0.761905  0.646947  0.773855  0.31486\n",
      "Dataset used:  P12\n",
      "Split id: 1\n",
      "971 123 127 971 123 127\n",
      "[127] [16] [16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([971, 1000, 26]) torch.Size([971, 9]) torch.Size([971, 1000, 1]) torch.Size([971])\n",
      "- - Run 1 - -\n",
      "Stop epochs: 25, Batches/epoch: 5, Total batches: 125\n",
      "Validation: Epoch 0,  val_loss:0.6917, aupr_val: 22.16, auc_val: 61.27\n",
      "**[S] Epoch 0, aupr_val: 22.1644, auc_val: 61.2734 **\n",
      "Validation: Epoch 1,  val_loss:0.6901, aupr_val: 25.19, auc_val: 64.14\n",
      "**[S] Epoch 1, aupr_val: 25.1880, auc_val: 64.1355 **\n",
      "Validation: Epoch 2,  val_loss:0.6828, aupr_val: 27.35, auc_val: 66.12\n",
      "**[S] Epoch 2, aupr_val: 27.3519, auc_val: 66.1215 **\n",
      "Validation: Epoch 3,  val_loss:0.6722, aupr_val: 29.45, auc_val: 67.29\n",
      "**[S] Epoch 3, aupr_val: 29.4497, auc_val: 67.2897 **\n",
      "Validation: Epoch 4,  val_loss:0.6760, aupr_val: 30.85, auc_val: 68.69\n",
      "**[S] Epoch 4, aupr_val: 30.8532, auc_val: 68.6916 **\n",
      "Validation: Epoch 5,  val_loss:0.6530, aupr_val: 30.01, auc_val: 69.86\n",
      "Validation: Epoch 6,  val_loss:0.6584, aupr_val: 28.89, auc_val: 71.20\n",
      "Epoch 00007: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Validation: Epoch 7,  val_loss:0.6547, aupr_val: 28.32, auc_val: 71.38\n",
      "Validation: Epoch 8,  val_loss:0.6486, aupr_val: 28.61, auc_val: 71.73\n",
      "Epoch 00009: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Validation: Epoch 9,  val_loss:0.6488, aupr_val: 28.61, auc_val: 71.73\n",
      "Validation: Epoch 10,  val_loss:0.6503, aupr_val: 28.58, auc_val: 71.67\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Validation: Epoch 11,  val_loss:0.6505, aupr_val: 28.58, auc_val: 71.67\n",
      "Validation: Epoch 12,  val_loss:0.6507, aupr_val: 28.58, auc_val: 71.67\n",
      "Epoch 00013: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Validation: Epoch 13,  val_loss:0.6507, aupr_val: 28.58, auc_val: 71.67\n",
      "Validation: Epoch 14,  val_loss:0.6507, aupr_val: 28.58, auc_val: 71.67\n",
      "Epoch 00015: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Validation: Epoch 15,  val_loss:0.6507, aupr_val: 28.58, auc_val: 71.67\n",
      "Validation: Epoch 16,  val_loss:0.6507, aupr_val: 28.58, auc_val: 71.67\n",
      "Validation: Epoch 17,  val_loss:0.6507, aupr_val: 28.58, auc_val: 71.67\n",
      "Validation: Epoch 18,  val_loss:0.6507, aupr_val: 28.58, auc_val: 71.67\n",
      "Validation: Epoch 19,  val_loss:0.6507, aupr_val: 28.58, auc_val: 71.67\n",
      "Validation: Epoch 20,  val_loss:0.6507, aupr_val: 28.58, auc_val: 71.67\n",
      "Validation: Epoch 21,  val_loss:0.6507, aupr_val: 28.58, auc_val: 71.67\n",
      "Validation: Epoch 22,  val_loss:0.6507, aupr_val: 28.58, auc_val: 71.67\n",
      "Validation: Epoch 23,  val_loss:0.6507, aupr_val: 28.58, auc_val: 71.67\n",
      "Validation: Epoch 24,  val_loss:0.6507, aupr_val: 28.58, auc_val: 71.67\n",
      "Total Time elapsed: 0.428 mins\n",
      "                 course  percentile       acc       bac       auc     auprc\n",
      "0  microcontroleurs-006          60  0.693878  0.636212  0.739027  0.297051\n",
      "                 course  percentile      acc       bac       auc     auprc\n",
      "0  microcontroleurs-006          60  0.70068  0.640029  0.744752  0.308636\n",
      "Dataset used:  P12\n",
      "Split id: 1\n",
      "2001 253 255 2001 253 255\n",
      "[358] [44] [45]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2001, 1000, 24]) torch.Size([2001, 9]) torch.Size([2001, 1000, 1]) torch.Size([2001])\n",
      "- - Run 1 - -\n",
      "Stop epochs: 25, Batches/epoch: 16, Total batches: 400\n",
      "Validation: Epoch 0,  val_loss:0.6919, aupr_val: 23.82, auc_val: 65.09\n",
      "**[S] Epoch 0, aupr_val: 23.8198, auc_val: 65.0935 **\n",
      "Validation: Epoch 1,  val_loss:0.6783, aupr_val: 26.24, auc_val: 67.36\n",
      "**[S] Epoch 1, aupr_val: 26.2373, auc_val: 67.3554 **\n",
      "Validation: Epoch 2,  val_loss:0.6684, aupr_val: 30.89, auc_val: 70.27\n",
      "**[S] Epoch 2, aupr_val: 30.8877, auc_val: 70.2697 **\n",
      "Validation: Epoch 3,  val_loss:0.6957, aupr_val: 36.84, auc_val: 73.46\n",
      "**[S] Epoch 3, aupr_val: 36.8361, auc_val: 73.4559 **\n",
      "Validation: Epoch 4,  val_loss:0.6650, aupr_val: 42.04, auc_val: 75.62\n",
      "**[S] Epoch 4, aupr_val: 42.0445, auc_val: 75.6198 **\n",
      "Validation: Epoch 5,  val_loss:0.6431, aupr_val: 38.19, auc_val: 73.68\n",
      "Validation: Epoch 6,  val_loss:0.6392, aupr_val: 34.80, auc_val: 73.33\n",
      "Epoch 00007: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Validation: Epoch 7,  val_loss:0.6420, aupr_val: 34.25, auc_val: 73.34\n",
      "Validation: Epoch 8,  val_loss:0.6408, aupr_val: 33.50, auc_val: 73.20\n",
      "Epoch 00009: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Validation: Epoch 9,  val_loss:0.6414, aupr_val: 33.37, auc_val: 73.18\n",
      "Validation: Epoch 10,  val_loss:0.6423, aupr_val: 33.21, auc_val: 73.13\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Validation: Epoch 11,  val_loss:0.6420, aupr_val: 33.13, auc_val: 73.11\n",
      "Validation: Epoch 12,  val_loss:0.6419, aupr_val: 33.13, auc_val: 73.11\n",
      "Epoch 00013: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Validation: Epoch 13,  val_loss:0.6419, aupr_val: 33.13, auc_val: 73.11\n",
      "Validation: Epoch 14,  val_loss:0.6419, aupr_val: 33.13, auc_val: 73.11\n",
      "Epoch 00015: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Validation: Epoch 15,  val_loss:0.6419, aupr_val: 33.13, auc_val: 73.11\n",
      "Validation: Epoch 16,  val_loss:0.6419, aupr_val: 33.13, auc_val: 73.11\n",
      "Validation: Epoch 17,  val_loss:0.6419, aupr_val: 33.13, auc_val: 73.11\n",
      "Validation: Epoch 18,  val_loss:0.6419, aupr_val: 33.13, auc_val: 73.11\n",
      "Validation: Epoch 19,  val_loss:0.6419, aupr_val: 33.13, auc_val: 73.11\n",
      "Validation: Epoch 20,  val_loss:0.6419, aupr_val: 33.13, auc_val: 73.11\n",
      "Validation: Epoch 21,  val_loss:0.6419, aupr_val: 33.13, auc_val: 73.11\n",
      "Validation: Epoch 22,  val_loss:0.6419, aupr_val: 33.13, auc_val: 73.11\n",
      "Validation: Epoch 23,  val_loss:0.6419, aupr_val: 33.13, auc_val: 73.11\n",
      "Validation: Epoch 24,  val_loss:0.6419, aupr_val: 33.13, auc_val: 73.11\n",
      "Total Time elapsed: 1.558 mins\n",
      "    course  percentile       acc       bac       auc     auprc\n",
      "0  dsp-005          40  0.536398  0.587963  0.665432  0.272243\n",
      "    course  percentile  acc       bac       auc     auprc\n",
      "0  dsp-005          40  0.6  0.686869  0.764415  0.421239\n",
      "Dataset used:  P12\n",
      "Split id: 1\n",
      "2042 258 259 2042 258 259\n",
      "[358] [44] [45]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2042, 1000, 24]) torch.Size([2042, 9]) torch.Size([2042, 1000, 1]) torch.Size([2042])\n",
      "- - Run 1 - -\n",
      "Stop epochs: 25, Batches/epoch: 16, Total batches: 400\n",
      "Validation: Epoch 0,  val_loss:0.6922, aupr_val: 24.23, auc_val: 65.16\n",
      "**[S] Epoch 0, aupr_val: 24.2283, auc_val: 65.1551 **\n",
      "Validation: Epoch 1,  val_loss:0.6954, aupr_val: 23.55, auc_val: 65.12\n",
      "Validation: Epoch 2,  val_loss:0.6716, aupr_val: 24.66, auc_val: 67.03\n",
      "**[S] Epoch 2, aupr_val: 24.6576, auc_val: 67.0348 **\n",
      "Validation: Epoch 3,  val_loss:0.6243, aupr_val: 29.27, auc_val: 71.21\n",
      "**[S] Epoch 3, aupr_val: 29.2687, auc_val: 71.2086 **\n",
      "Validation: Epoch 4,  val_loss:0.6591, aupr_val: 37.20, auc_val: 76.53\n",
      "**[S] Epoch 4, aupr_val: 37.2013, auc_val: 76.5293 **\n",
      "Validation: Epoch 5,  val_loss:0.5939, aupr_val: 46.13, auc_val: 80.48\n",
      "**[S] Epoch 5, aupr_val: 46.1339, auc_val: 80.4800 **\n",
      "Validation: Epoch 6,  val_loss:0.6419, aupr_val: 48.70, auc_val: 83.29\n",
      "**[S] Epoch 6, aupr_val: 48.6982, auc_val: 83.2944 **\n",
      "Validation: Epoch 7,  val_loss:0.6156, aupr_val: 43.96, auc_val: 82.37\n",
      "Validation: Epoch 8,  val_loss:0.5928, aupr_val: 46.40, auc_val: 83.64\n",
      "Epoch 00009: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Validation: Epoch 9,  val_loss:0.5973, aupr_val: 48.17, auc_val: 83.83\n",
      "Validation: Epoch 10,  val_loss:0.5990, aupr_val: 46.11, auc_val: 83.58\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Validation: Epoch 11,  val_loss:0.5981, aupr_val: 46.15, auc_val: 83.58\n",
      "Validation: Epoch 12,  val_loss:0.5950, aupr_val: 46.27, auc_val: 83.59\n",
      "Epoch 00013: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Validation: Epoch 13,  val_loss:0.5952, aupr_val: 46.34, auc_val: 83.60\n",
      "Validation: Epoch 14,  val_loss:0.5953, aupr_val: 46.32, auc_val: 83.59\n",
      "Epoch 00015: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Validation: Epoch 15,  val_loss:0.5953, aupr_val: 46.32, auc_val: 83.59\n",
      "Validation: Epoch 16,  val_loss:0.5953, aupr_val: 46.33, auc_val: 83.60\n",
      "Epoch 00017: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Validation: Epoch 17,  val_loss:0.5953, aupr_val: 46.33, auc_val: 83.60\n",
      "Validation: Epoch 18,  val_loss:0.5953, aupr_val: 46.33, auc_val: 83.60\n",
      "Validation: Epoch 19,  val_loss:0.5953, aupr_val: 46.33, auc_val: 83.60\n",
      "Validation: Epoch 20,  val_loss:0.5953, aupr_val: 46.33, auc_val: 83.60\n",
      "Validation: Epoch 21,  val_loss:0.5953, aupr_val: 46.33, auc_val: 83.60\n",
      "Validation: Epoch 22,  val_loss:0.5953, aupr_val: 46.33, auc_val: 83.60\n",
      "Validation: Epoch 23,  val_loss:0.5953, aupr_val: 46.33, auc_val: 83.60\n",
      "Validation: Epoch 24,  val_loss:0.5953, aupr_val: 46.33, auc_val: 83.60\n",
      "Total Time elapsed: 1.600 mins\n",
      "    course  percentile       acc       bac       auc     auprc\n",
      "0  dsp-005          60  0.624521  0.746759  0.825514  0.385187\n",
      "    course  percentile       acc       bac       auc     auprc\n",
      "0  dsp-005          60  0.619231  0.743687  0.832702  0.480462\n",
      "Dataset used:  P12\n",
      "Split id: 1\n",
      "749 91 96 749 91 96\n",
      "[386] [47] [51]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([749, 1000, 24]) torch.Size([749, 9]) torch.Size([749, 1000, 1]) torch.Size([749])\n",
      "- - Run 1 - -\n",
      "Stop epochs: 25, Batches/epoch: 5, Total batches: 125\n",
      "Validation: Epoch 0,  val_loss:0.6930, aupr_val: 48.36, auc_val: 49.81\n",
      "**[S] Epoch 0, aupr_val: 48.3606, auc_val: 49.8066 **\n",
      "Validation: Epoch 1,  val_loss:0.6932, aupr_val: 46.61, auc_val: 46.71\n",
      "Validation: Epoch 2,  val_loss:0.6933, aupr_val: 47.14, auc_val: 47.73\n",
      "Epoch 00003: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Validation: Epoch 3,  val_loss:0.6932, aupr_val: 47.25, auc_val: 47.92\n",
      "Validation: Epoch 4,  val_loss:0.6931, aupr_val: 47.39, auc_val: 48.16\n",
      "Epoch 00005: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Validation: Epoch 5,  val_loss:0.6931, aupr_val: 47.47, auc_val: 48.26\n",
      "Validation: Epoch 6,  val_loss:0.6931, aupr_val: 47.51, auc_val: 48.31\n",
      "Epoch 00007: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Validation: Epoch 7,  val_loss:0.6931, aupr_val: 47.51, auc_val: 48.31\n",
      "Validation: Epoch 8,  val_loss:0.6931, aupr_val: 47.51, auc_val: 48.31\n",
      "Epoch 00009: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Validation: Epoch 9,  val_loss:0.6931, aupr_val: 47.51, auc_val: 48.31\n",
      "Validation: Epoch 10,  val_loss:0.6931, aupr_val: 47.51, auc_val: 48.31\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Validation: Epoch 11,  val_loss:0.6931, aupr_val: 47.51, auc_val: 48.31\n",
      "Validation: Epoch 12,  val_loss:0.6931, aupr_val: 47.51, auc_val: 48.31\n",
      "Validation: Epoch 13,  val_loss:0.6931, aupr_val: 47.51, auc_val: 48.31\n",
      "Validation: Epoch 14,  val_loss:0.6931, aupr_val: 47.51, auc_val: 48.31\n",
      "Validation: Epoch 15,  val_loss:0.6931, aupr_val: 47.51, auc_val: 48.31\n",
      "Validation: Epoch 16,  val_loss:0.6931, aupr_val: 47.51, auc_val: 48.31\n",
      "Validation: Epoch 17,  val_loss:0.6931, aupr_val: 47.51, auc_val: 48.31\n",
      "Validation: Epoch 18,  val_loss:0.6931, aupr_val: 47.51, auc_val: 48.31\n",
      "Validation: Epoch 19,  val_loss:0.6931, aupr_val: 47.51, auc_val: 48.31\n",
      "Validation: Epoch 20,  val_loss:0.6931, aupr_val: 47.51, auc_val: 48.31\n",
      "Validation: Epoch 21,  val_loss:0.6931, aupr_val: 47.51, auc_val: 48.31\n",
      "Validation: Epoch 22,  val_loss:0.6931, aupr_val: 47.51, auc_val: 48.31\n",
      "Validation: Epoch 23,  val_loss:0.6931, aupr_val: 47.51, auc_val: 48.31\n",
      "Validation: Epoch 24,  val_loss:0.6931, aupr_val: 47.51, auc_val: 48.31\n",
      "Total Time elapsed: 0.455 mins\n",
      "     course  percentile       acc       bac       auc     auprc\n",
      "0  hwts-002          40  0.572816  0.576923  0.575792  0.513264\n",
      "     course  percentile       acc       bac       auc    auprc\n",
      "0  hwts-002          40  0.539216  0.546923  0.541154  0.47526\n",
      "Dataset used:  P12\n",
      "Split id: 1\n",
      "790 97 100 790 97 100\n",
      "[401] [50] [51]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([790, 1000, 24]) torch.Size([790, 9]) torch.Size([790, 1000, 1]) torch.Size([790])\n",
      "- - Run 1 - -\n",
      "Stop epochs: 25, Batches/epoch: 6, Total batches: 150\n",
      "Validation: Epoch 0,  val_loss:0.6929, aupr_val: 48.83, auc_val: 51.36\n",
      "**[S] Epoch 0, aupr_val: 48.8272, auc_val: 51.3617 **\n",
      "Validation: Epoch 1,  val_loss:0.6922, aupr_val: 48.90, auc_val: 51.91\n",
      "**[S] Epoch 1, aupr_val: 48.8999, auc_val: 51.9149 **\n",
      "Validation: Epoch 2,  val_loss:0.6912, aupr_val: 48.96, auc_val: 51.91\n",
      "**[S] Epoch 2, aupr_val: 48.9598, auc_val: 51.9149 **\n",
      "Validation: Epoch 3,  val_loss:0.6895, aupr_val: 49.55, auc_val: 53.79\n",
      "**[S] Epoch 3, aupr_val: 49.5489, auc_val: 53.7872 **\n",
      "Validation: Epoch 4,  val_loss:0.6882, aupr_val: 49.90, auc_val: 54.43\n",
      "**[S] Epoch 4, aupr_val: 49.9005, auc_val: 54.4255 **\n",
      "Validation: Epoch 5,  val_loss:0.6865, aupr_val: 50.40, auc_val: 54.89\n",
      "**[S] Epoch 5, aupr_val: 50.3974, auc_val: 54.8936 **\n",
      "Validation: Epoch 6,  val_loss:0.6849, aupr_val: 51.20, auc_val: 56.17\n",
      "**[S] Epoch 6, aupr_val: 51.1971, auc_val: 56.1702 **\n",
      "Validation: Epoch 7,  val_loss:0.6832, aupr_val: 53.68, auc_val: 58.94\n",
      "**[S] Epoch 7, aupr_val: 53.6837, auc_val: 58.9362 **\n",
      "Validation: Epoch 8,  val_loss:0.6818, aupr_val: 55.64, auc_val: 59.83\n",
      "**[S] Epoch 8, aupr_val: 55.6416, auc_val: 59.8298 **\n",
      "Validation: Epoch 9,  val_loss:0.6755, aupr_val: 58.60, auc_val: 63.19\n",
      "**[S] Epoch 9, aupr_val: 58.5964, auc_val: 63.1915 **\n",
      "Validation: Epoch 10,  val_loss:0.6703, aupr_val: 60.19, auc_val: 64.94\n",
      "**[S] Epoch 10, aupr_val: 60.1861, auc_val: 64.9362 **\n",
      "Validation: Epoch 11,  val_loss:0.6648, aupr_val: 61.31, auc_val: 67.32\n",
      "**[S] Epoch 11, aupr_val: 61.3143, auc_val: 67.3191 **\n",
      "Validation: Epoch 12,  val_loss:0.6646, aupr_val: 64.02, auc_val: 70.38\n",
      "**[S] Epoch 12, aupr_val: 64.0228, auc_val: 70.3830 **\n",
      "Validation: Epoch 13,  val_loss:0.6622, aupr_val: 64.70, auc_val: 71.06\n",
      "**[S] Epoch 13, aupr_val: 64.7008, auc_val: 71.0638 **\n",
      "Validation: Epoch 14,  val_loss:0.6631, aupr_val: 64.62, auc_val: 70.81\n",
      "Validation: Epoch 15,  val_loss:0.6630, aupr_val: 65.13, auc_val: 70.81\n",
      "**[S] Epoch 15, aupr_val: 65.1313, auc_val: 70.8085 **\n",
      "Validation: Epoch 16,  val_loss:0.6616, aupr_val: 64.40, auc_val: 70.43\n",
      "Validation: Epoch 17,  val_loss:0.6604, aupr_val: 64.03, auc_val: 70.43\n",
      "Epoch 00018: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Validation: Epoch 18,  val_loss:0.6587, aupr_val: 64.17, auc_val: 70.68\n",
      "Validation: Epoch 19,  val_loss:0.6584, aupr_val: 64.14, auc_val: 70.60\n",
      "Epoch 00020: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Validation: Epoch 20,  val_loss:0.6584, aupr_val: 64.22, auc_val: 70.68\n",
      "Validation: Epoch 21,  val_loss:0.6585, aupr_val: 64.22, auc_val: 70.68\n",
      "Epoch 00022: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Validation: Epoch 22,  val_loss:0.6585, aupr_val: 64.22, auc_val: 70.68\n",
      "Validation: Epoch 23,  val_loss:0.6585, aupr_val: 64.22, auc_val: 70.68\n",
      "Epoch 00024: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Validation: Epoch 24,  val_loss:0.6585, aupr_val: 64.22, auc_val: 70.68\n",
      "Total Time elapsed: 0.596 mins\n",
      "     course  percentile       acc       bac       auc     auprc\n",
      "0  hwts-002          60  0.640777  0.641403  0.696456  0.647516\n",
      "     course  percentile       acc       bac       auc     auprc\n",
      "0  hwts-002          60  0.696078  0.698462  0.736923  0.651764\n",
      "Dataset used:  P12\n",
      "Split id: 1\n",
      "1096 134 137 1096 134 137\n",
      "[282] [35] [36]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1096, 1000, 24]) torch.Size([1096, 9]) torch.Size([1096, 1000, 1]) torch.Size([1096])\n",
      "- - Run 1 - -\n",
      "Stop epochs: 25, Batches/epoch: 12, Total batches: 300\n",
      "Validation: Epoch 0,  val_loss:0.6928, aupr_val: 35.16, auc_val: 58.73\n",
      "**[S] Epoch 0, aupr_val: 35.1563, auc_val: 58.7302 **\n",
      "Validation: Epoch 1,  val_loss:0.6909, aupr_val: 34.95, auc_val: 59.48\n",
      "Validation: Epoch 2,  val_loss:0.6859, aupr_val: 34.86, auc_val: 59.34\n",
      "Epoch 00003: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Validation: Epoch 3,  val_loss:0.6913, aupr_val: 34.97, auc_val: 59.45\n",
      "Validation: Epoch 4,  val_loss:0.6870, aupr_val: 34.56, auc_val: 59.37\n",
      "Epoch 00005: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Validation: Epoch 5,  val_loss:0.6885, aupr_val: 34.56, auc_val: 59.34\n",
      "Validation: Epoch 6,  val_loss:0.6889, aupr_val: 34.56, auc_val: 59.34\n",
      "Epoch 00007: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Validation: Epoch 7,  val_loss:0.6889, aupr_val: 34.56, auc_val: 59.34\n",
      "Validation: Epoch 8,  val_loss:0.6888, aupr_val: 34.56, auc_val: 59.34\n",
      "Epoch 00009: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Validation: Epoch 9,  val_loss:0.6888, aupr_val: 34.56, auc_val: 59.34\n",
      "Validation: Epoch 10,  val_loss:0.6888, aupr_val: 34.56, auc_val: 59.34\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Validation: Epoch 11,  val_loss:0.6888, aupr_val: 34.56, auc_val: 59.34\n",
      "Validation: Epoch 12,  val_loss:0.6888, aupr_val: 34.56, auc_val: 59.34\n",
      "Validation: Epoch 13,  val_loss:0.6888, aupr_val: 34.56, auc_val: 59.34\n",
      "Validation: Epoch 14,  val_loss:0.6888, aupr_val: 34.56, auc_val: 59.34\n",
      "Validation: Epoch 15,  val_loss:0.6888, aupr_val: 34.56, auc_val: 59.34\n",
      "Validation: Epoch 16,  val_loss:0.6888, aupr_val: 34.56, auc_val: 59.34\n",
      "Validation: Epoch 17,  val_loss:0.6888, aupr_val: 34.56, auc_val: 59.34\n",
      "Validation: Epoch 18,  val_loss:0.6888, aupr_val: 34.56, auc_val: 59.34\n",
      "Validation: Epoch 19,  val_loss:0.6888, aupr_val: 34.56, auc_val: 59.34\n",
      "Validation: Epoch 20,  val_loss:0.6888, aupr_val: 34.56, auc_val: 59.34\n",
      "Validation: Epoch 21,  val_loss:0.6888, aupr_val: 34.56, auc_val: 59.34\n",
      "Validation: Epoch 22,  val_loss:0.6888, aupr_val: 34.56, auc_val: 59.34\n",
      "Validation: Epoch 23,  val_loss:0.6888, aupr_val: 34.56, auc_val: 59.34\n",
      "Validation: Epoch 24,  val_loss:0.6888, aupr_val: 34.56, auc_val: 59.34\n",
      "Total Time elapsed: 1.361 mins\n",
      "    course  percentile      acc       bac       auc     auprc\n",
      "0  dsp-006          40  0.55102  0.599474  0.630631  0.320513\n",
      "    course  percentile       acc       bac      auc     auprc\n",
      "0  dsp-006          40  0.537415  0.588393  0.63699  0.347246\n",
      "Dataset used:  P12\n",
      "Split id: 1\n",
      "1131 142 142 1131 142 142\n",
      "[282] [35] [36]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1131, 1000, 26]) torch.Size([1131, 9]) torch.Size([1131, 1000, 1]) torch.Size([1131])\n",
      "- - Run 1 - -\n",
      "Stop epochs: 25, Batches/epoch: 13, Total batches: 325\n",
      "Validation: Epoch 0,  val_loss:0.6919, aupr_val: 32.57, auc_val: 59.81\n",
      "**[S] Epoch 0, aupr_val: 32.5716, auc_val: 59.8131 **\n",
      "Validation: Epoch 1,  val_loss:0.6956, aupr_val: 32.78, auc_val: 58.69\n",
      "**[S] Epoch 1, aupr_val: 32.7750, auc_val: 58.6916 **\n",
      "Validation: Epoch 2,  val_loss:0.6880, aupr_val: 34.48, auc_val: 61.01\n",
      "**[S] Epoch 2, aupr_val: 34.4759, auc_val: 61.0147 **\n",
      "Validation: Epoch 3,  val_loss:0.6558, aupr_val: 37.69, auc_val: 67.34\n",
      "**[S] Epoch 3, aupr_val: 37.6902, auc_val: 67.3431 **\n",
      "Validation: Epoch 4,  val_loss:0.6467, aupr_val: 41.57, auc_val: 74.58\n",
      "**[S] Epoch 4, aupr_val: 41.5683, auc_val: 74.5794 **\n",
      "Validation: Epoch 5,  val_loss:0.6189, aupr_val: 45.00, auc_val: 78.16\n",
      "**[S] Epoch 5, aupr_val: 45.0038, auc_val: 78.1575 **\n",
      "Validation: Epoch 6,  val_loss:0.6252, aupr_val: 42.94, auc_val: 77.12\n",
      "Validation: Epoch 7,  val_loss:0.6436, aupr_val: 42.37, auc_val: 76.69\n",
      "Epoch 00008: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Validation: Epoch 8,  val_loss:0.6193, aupr_val: 42.34, auc_val: 76.74\n",
      "Validation: Epoch 9,  val_loss:0.6186, aupr_val: 42.30, auc_val: 76.69\n",
      "Epoch 00010: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Validation: Epoch 10,  val_loss:0.6207, aupr_val: 42.27, auc_val: 76.69\n",
      "Validation: Epoch 11,  val_loss:0.6230, aupr_val: 42.27, auc_val: 76.69\n",
      "Epoch 00012: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Validation: Epoch 12,  val_loss:0.6231, aupr_val: 42.27, auc_val: 76.69\n",
      "Validation: Epoch 13,  val_loss:0.6232, aupr_val: 42.27, auc_val: 76.69\n",
      "Epoch 00014: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Validation: Epoch 14,  val_loss:0.6232, aupr_val: 42.27, auc_val: 76.69\n",
      "Validation: Epoch 15,  val_loss:0.6232, aupr_val: 42.27, auc_val: 76.69\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Validation: Epoch 16,  val_loss:0.6232, aupr_val: 42.27, auc_val: 76.69\n",
      "Validation: Epoch 17,  val_loss:0.6232, aupr_val: 42.27, auc_val: 76.69\n",
      "Validation: Epoch 18,  val_loss:0.6232, aupr_val: 42.27, auc_val: 76.69\n",
      "Validation: Epoch 19,  val_loss:0.6232, aupr_val: 42.27, auc_val: 76.69\n",
      "Validation: Epoch 20,  val_loss:0.6232, aupr_val: 42.27, auc_val: 76.69\n",
      "Validation: Epoch 21,  val_loss:0.6232, aupr_val: 42.27, auc_val: 76.69\n",
      "Validation: Epoch 22,  val_loss:0.6232, aupr_val: 42.27, auc_val: 76.69\n",
      "Validation: Epoch 23,  val_loss:0.6232, aupr_val: 42.27, auc_val: 76.69\n",
      "Validation: Epoch 24,  val_loss:0.6232, aupr_val: 42.27, auc_val: 76.69\n",
      "Total Time elapsed: 1.586 mins\n",
      "    course  percentile       acc       bac       auc     auprc\n",
      "0  dsp-006          60  0.761905  0.710961  0.821321  0.526433\n",
      "    course  percentile       acc       bac       auc     auprc\n",
      "0  dsp-006          60  0.741497  0.732143  0.791071  0.449982\n",
      "Dataset used:  P12\n",
      "Split id: 1\n",
      "394 49 50 394 49 50\n",
      "[289] [36] [36]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([394, 1000, 24]) torch.Size([394, 9]) torch.Size([394, 1000, 1]) torch.Size([394])\n",
      "- - Run 1 - -\n",
      "Stop epochs: 25, Batches/epoch: 1, Total batches: 25\n",
      "Validation: Epoch 0,  val_loss:0.6902, aupr_val: 79.02, auc_val: 57.69\n",
      "**[S] Epoch 0, aupr_val: 79.0217, auc_val: 57.6923 **\n",
      "Validation: Epoch 1,  val_loss:0.6917, aupr_val: 85.15, auc_val: 64.32\n",
      "**[S] Epoch 1, aupr_val: 85.1482, auc_val: 64.3162 **\n",
      "Validation: Epoch 2,  val_loss:0.6932, aupr_val: 85.76, auc_val: 66.03\n",
      "**[S] Epoch 2, aupr_val: 85.7600, auc_val: 66.0256 **\n",
      "Validation: Epoch 3,  val_loss:0.6938, aupr_val: 85.53, auc_val: 66.45\n",
      "Validation: Epoch 4,  val_loss:0.6931, aupr_val: 85.73, auc_val: 68.16\n",
      "Epoch 00005: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Validation: Epoch 5,  val_loss:0.6930, aupr_val: 85.73, auc_val: 68.16\n",
      "Validation: Epoch 6,  val_loss:0.6930, aupr_val: 86.02, auc_val: 69.02\n",
      "**[S] Epoch 6, aupr_val: 86.0160, auc_val: 69.0171 **\n",
      "Validation: Epoch 7,  val_loss:0.6929, aupr_val: 86.06, auc_val: 69.23\n",
      "**[S] Epoch 7, aupr_val: 86.0630, auc_val: 69.2308 **\n",
      "Validation: Epoch 8,  val_loss:0.6928, aupr_val: 86.06, auc_val: 69.23\n",
      "Epoch 00009: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Validation: Epoch 9,  val_loss:0.6927, aupr_val: 86.06, auc_val: 69.23\n",
      "Validation: Epoch 10,  val_loss:0.6927, aupr_val: 86.06, auc_val: 69.23\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Validation: Epoch 11,  val_loss:0.6927, aupr_val: 86.06, auc_val: 69.23\n",
      "Validation: Epoch 12,  val_loss:0.6927, aupr_val: 86.06, auc_val: 69.23\n",
      "Epoch 00013: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Validation: Epoch 13,  val_loss:0.6927, aupr_val: 86.06, auc_val: 69.23\n",
      "Validation: Epoch 14,  val_loss:0.6927, aupr_val: 86.06, auc_val: 69.23\n",
      "Epoch 00015: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Validation: Epoch 15,  val_loss:0.6927, aupr_val: 86.06, auc_val: 69.23\n",
      "Validation: Epoch 16,  val_loss:0.6927, aupr_val: 86.06, auc_val: 69.23\n",
      "Validation: Epoch 17,  val_loss:0.6927, aupr_val: 86.06, auc_val: 69.23\n",
      "Validation: Epoch 18,  val_loss:0.6927, aupr_val: 86.06, auc_val: 69.23\n",
      "Validation: Epoch 19,  val_loss:0.6927, aupr_val: 86.06, auc_val: 69.23\n",
      "Validation: Epoch 20,  val_loss:0.6927, aupr_val: 86.06, auc_val: 69.23\n",
      "Validation: Epoch 21,  val_loss:0.6927, aupr_val: 86.06, auc_val: 69.23\n",
      "Validation: Epoch 22,  val_loss:0.6927, aupr_val: 86.06, auc_val: 69.23\n",
      "Validation: Epoch 23,  val_loss:0.6927, aupr_val: 86.06, auc_val: 69.23\n",
      "Validation: Epoch 24,  val_loss:0.6927, aupr_val: 86.06, auc_val: 69.23\n",
      "Total Time elapsed: 0.084 mins\n",
      "                 course  percentile       acc       bac      auc     auprc\n",
      "0  analysenumerique-002          40  0.509804  0.555556  0.62037  0.808286\n",
      "                 course  percentile   acc       bac       auc     auprc\n",
      "0  analysenumerique-002          40  0.52  0.623016  0.710317  0.862958\n",
      "Dataset used:  P12\n",
      "Split id: 1\n",
      "396 49 50 396 49 50\n",
      "[289] [36] [36]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([396, 1000, 24]) torch.Size([396, 9]) torch.Size([396, 1000, 1]) torch.Size([396])\n",
      "- - Run 1 - -\n",
      "Stop epochs: 25, Batches/epoch: 1, Total batches: 25\n",
      "Validation: Epoch 0,  val_loss:0.6879, aupr_val: 79.26, auc_val: 61.97\n",
      "**[S] Epoch 0, aupr_val: 79.2642, auc_val: 61.9658 **\n",
      "Validation: Epoch 1,  val_loss:0.6923, aupr_val: 82.28, auc_val: 69.66\n",
      "**[S] Epoch 1, aupr_val: 82.2754, auc_val: 69.6581 **\n",
      "Validation: Epoch 2,  val_loss:0.6957, aupr_val: 82.26, auc_val: 70.73\n",
      "Validation: Epoch 3,  val_loss:0.6955, aupr_val: 82.38, auc_val: 70.73\n",
      "**[S] Epoch 3, aupr_val: 82.3825, auc_val: 70.7265 **\n",
      "Validation: Epoch 4,  val_loss:0.6938, aupr_val: 83.47, auc_val: 69.66\n",
      "**[S] Epoch 4, aupr_val: 83.4677, auc_val: 69.6581 **\n",
      "Validation: Epoch 5,  val_loss:0.6919, aupr_val: 83.53, auc_val: 69.87\n",
      "**[S] Epoch 5, aupr_val: 83.5302, auc_val: 69.8718 **\n",
      "Validation: Epoch 6,  val_loss:0.6904, aupr_val: 83.40, auc_val: 69.44\n",
      "Epoch 00007: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Validation: Epoch 7,  val_loss:0.6903, aupr_val: 83.35, auc_val: 69.23\n",
      "Validation: Epoch 8,  val_loss:0.6903, aupr_val: 83.41, auc_val: 69.44\n",
      "Epoch 00009: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Validation: Epoch 9,  val_loss:0.6903, aupr_val: 83.41, auc_val: 69.44\n",
      "Validation: Epoch 10,  val_loss:0.6904, aupr_val: 83.35, auc_val: 69.34\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Validation: Epoch 11,  val_loss:0.6904, aupr_val: 83.35, auc_val: 69.23\n",
      "Validation: Epoch 12,  val_loss:0.6904, aupr_val: 83.35, auc_val: 69.23\n",
      "Epoch 00013: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Validation: Epoch 13,  val_loss:0.6904, aupr_val: 83.35, auc_val: 69.23\n",
      "Validation: Epoch 14,  val_loss:0.6904, aupr_val: 83.35, auc_val: 69.23\n",
      "Epoch 00015: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Validation: Epoch 15,  val_loss:0.6904, aupr_val: 83.35, auc_val: 69.23\n",
      "Validation: Epoch 16,  val_loss:0.6904, aupr_val: 83.35, auc_val: 69.23\n",
      "Validation: Epoch 17,  val_loss:0.6904, aupr_val: 83.35, auc_val: 69.23\n",
      "Validation: Epoch 18,  val_loss:0.6904, aupr_val: 83.35, auc_val: 69.23\n",
      "Validation: Epoch 19,  val_loss:0.6904, aupr_val: 83.35, auc_val: 69.23\n",
      "Validation: Epoch 20,  val_loss:0.6904, aupr_val: 83.35, auc_val: 69.23\n",
      "Validation: Epoch 21,  val_loss:0.6904, aupr_val: 83.35, auc_val: 69.23\n",
      "Validation: Epoch 22,  val_loss:0.6904, aupr_val: 83.35, auc_val: 69.23\n",
      "Validation: Epoch 23,  val_loss:0.6904, aupr_val: 83.35, auc_val: 69.23\n",
      "Validation: Epoch 24,  val_loss:0.6904, aupr_val: 83.35, auc_val: 69.23\n",
      "Total Time elapsed: 0.089 mins\n",
      "                 course  percentile       acc       bac       auc     auprc\n",
      "0  analysenumerique-002          60  0.686275  0.622222  0.692593  0.852289\n",
      "                 course  percentile   acc       bac       auc     auprc\n",
      "0  analysenumerique-002          60  0.74  0.753968  0.714286  0.819981\n",
      "Dataset used:  P12\n",
      "Split id: 1\n",
      "138 17 18 138 17 18\n",
      "[43] [5] [6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([138, 1000, 26]) torch.Size([138, 9]) torch.Size([138, 1000, 1]) torch.Size([138])\n",
      "- - Run 1 - -\n",
      "Stop epochs: 25, Batches/epoch: 1, Total batches: 25\n",
      "Validation: Epoch 0,  val_loss:0.6925, aupr_val: 36.63, auc_val: 51.67\n",
      "**[S] Epoch 0, aupr_val: 36.6300, auc_val: 51.6667 **\n",
      "Validation: Epoch 1,  val_loss:0.6975, aupr_val: 48.06, auc_val: 55.00\n",
      "**[S] Epoch 1, aupr_val: 48.0586, auc_val: 55.0000 **\n",
      "Validation: Epoch 2,  val_loss:0.6973, aupr_val: 49.39, auc_val: 56.67\n",
      "**[S] Epoch 2, aupr_val: 49.3919, auc_val: 56.6667 **\n",
      "Validation: Epoch 3,  val_loss:0.6953, aupr_val: 50.83, auc_val: 58.33\n",
      "**[S] Epoch 3, aupr_val: 50.8333, auc_val: 58.3333 **\n",
      "Validation: Epoch 4,  val_loss:0.6930, aupr_val: 48.83, auc_val: 56.67\n",
      "Validation: Epoch 5,  val_loss:0.6916, aupr_val: 47.33, auc_val: 53.33\n",
      "Epoch 00006: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Validation: Epoch 6,  val_loss:0.6915, aupr_val: 47.33, auc_val: 53.33\n",
      "Validation: Epoch 7,  val_loss:0.6916, aupr_val: 47.33, auc_val: 53.33\n",
      "Epoch 00008: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Validation: Epoch 8,  val_loss:0.6916, aupr_val: 47.33, auc_val: 53.33\n",
      "Validation: Epoch 9,  val_loss:0.6916, aupr_val: 47.33, auc_val: 53.33\n",
      "Epoch 00010: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Validation: Epoch 10,  val_loss:0.6916, aupr_val: 47.33, auc_val: 53.33\n",
      "Validation: Epoch 11,  val_loss:0.6916, aupr_val: 47.33, auc_val: 53.33\n",
      "Epoch 00012: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Validation: Epoch 12,  val_loss:0.6916, aupr_val: 47.33, auc_val: 53.33\n",
      "Validation: Epoch 13,  val_loss:0.6916, aupr_val: 47.33, auc_val: 53.33\n",
      "Epoch 00014: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Validation: Epoch 14,  val_loss:0.6916, aupr_val: 47.33, auc_val: 53.33\n",
      "Validation: Epoch 15,  val_loss:0.6916, aupr_val: 47.33, auc_val: 53.33\n",
      "Validation: Epoch 16,  val_loss:0.6916, aupr_val: 47.33, auc_val: 53.33\n",
      "Validation: Epoch 17,  val_loss:0.6916, aupr_val: 47.33, auc_val: 53.33\n",
      "Validation: Epoch 18,  val_loss:0.6916, aupr_val: 47.33, auc_val: 53.33\n",
      "Validation: Epoch 19,  val_loss:0.6916, aupr_val: 47.33, auc_val: 53.33\n",
      "Validation: Epoch 20,  val_loss:0.6916, aupr_val: 47.33, auc_val: 53.33\n",
      "Validation: Epoch 21,  val_loss:0.6916, aupr_val: 47.33, auc_val: 53.33\n",
      "Validation: Epoch 22,  val_loss:0.6916, aupr_val: 47.33, auc_val: 53.33\n",
      "Validation: Epoch 23,  val_loss:0.6916, aupr_val: 47.33, auc_val: 53.33\n",
      "Validation: Epoch 24,  val_loss:0.6916, aupr_val: 47.33, auc_val: 53.33\n",
      "Total Time elapsed: 0.101 mins\n",
      "           course  percentile       acc  bac       auc     auprc\n",
      "0  structures-003          40  0.333333  0.5  0.611111  0.451479\n",
      "           course  percentile       acc  bac       auc     auprc\n",
      "0  structures-003          40  0.294118  0.5  0.583333  0.508333\n",
      "Dataset used:  P12\n",
      "Split id: 1\n",
      "138 17 18 138 17 18\n",
      "[43] [5] [6]\n",
      "torch.Size([138, 1000, 26]) torch.Size([138, 9]) torch.Size([138, 1000, 1]) torch.Size([138])\n",
      "- - Run 1 - -\n",
      "Stop epochs: 25, Batches/epoch: 1, Total batches: 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch 0,  val_loss:0.6921, aupr_val: 34.97, auc_val: 55.00\n",
      "**[S] Epoch 0, aupr_val: 34.9650, auc_val: 55.0000 **\n",
      "Validation: Epoch 1,  val_loss:0.6892, aupr_val: 35.46, auc_val: 53.33\n",
      "**[S] Epoch 1, aupr_val: 35.4634, auc_val: 53.3333 **\n",
      "Validation: Epoch 2,  val_loss:0.6902, aupr_val: 34.99, auc_val: 51.67\n",
      "Validation: Epoch 3,  val_loss:0.6919, aupr_val: 36.30, auc_val: 55.00\n",
      "**[S] Epoch 3, aupr_val: 36.2967, auc_val: 55.0000 **\n",
      "Validation: Epoch 4,  val_loss:0.6932, aupr_val: 37.36, auc_val: 58.33\n",
      "**[S] Epoch 4, aupr_val: 37.3590, auc_val: 58.3333 **\n",
      "Validation: Epoch 5,  val_loss:0.6936, aupr_val: 35.93, auc_val: 56.67\n",
      "Validation: Epoch 6,  val_loss:0.6931, aupr_val: 35.93, auc_val: 56.67\n",
      "Epoch 00007: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Validation: Epoch 7,  val_loss:0.6930, aupr_val: 35.93, auc_val: 56.67\n",
      "Validation: Epoch 8,  val_loss:0.6928, aupr_val: 35.93, auc_val: 56.67\n",
      "Epoch 00009: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Validation: Epoch 9,  val_loss:0.6928, aupr_val: 35.93, auc_val: 56.67\n",
      "Validation: Epoch 10,  val_loss:0.6928, aupr_val: 35.93, auc_val: 56.67\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Validation: Epoch 11,  val_loss:0.6928, aupr_val: 35.93, auc_val: 56.67\n",
      "Validation: Epoch 12,  val_loss:0.6928, aupr_val: 35.93, auc_val: 56.67\n",
      "Epoch 00013: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Validation: Epoch 13,  val_loss:0.6928, aupr_val: 35.93, auc_val: 56.67\n",
      "Validation: Epoch 14,  val_loss:0.6928, aupr_val: 35.93, auc_val: 56.67\n",
      "Epoch 00015: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Validation: Epoch 15,  val_loss:0.6928, aupr_val: 35.93, auc_val: 56.67\n",
      "Validation: Epoch 16,  val_loss:0.6928, aupr_val: 35.93, auc_val: 56.67\n",
      "Validation: Epoch 17,  val_loss:0.6928, aupr_val: 35.93, auc_val: 56.67\n",
      "Validation: Epoch 18,  val_loss:0.6928, aupr_val: 35.93, auc_val: 56.67\n",
      "Validation: Epoch 19,  val_loss:0.6928, aupr_val: 35.93, auc_val: 56.67\n",
      "Validation: Epoch 20,  val_loss:0.6928, aupr_val: 35.93, auc_val: 56.67\n",
      "Validation: Epoch 21,  val_loss:0.6928, aupr_val: 35.93, auc_val: 56.67\n",
      "Validation: Epoch 22,  val_loss:0.6928, aupr_val: 35.93, auc_val: 56.67\n",
      "Validation: Epoch 23,  val_loss:0.6928, aupr_val: 35.93, auc_val: 56.67\n",
      "Validation: Epoch 24,  val_loss:0.6928, aupr_val: 35.93, auc_val: 56.67\n",
      "Total Time elapsed: 0.106 mins\n",
      "           course  percentile       acc       bac       auc     auprc\n",
      "0  structures-003          60  0.611111  0.708333  0.555556  0.370736\n",
      "           course  percentile       acc       bac       auc    auprc\n",
      "0  structures-003          60  0.411765  0.466667  0.583333  0.37359\n",
      "Dataset used:  P12\n",
      "Split id: 1\n",
      "1548 197 202 1548 197 202\n",
      "[163] [21] [21]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1548, 1000, 26]) torch.Size([1548, 9]) torch.Size([1548, 1000, 1]) torch.Size([1548])\n",
      "- - Run 1 - -\n",
      "Stop epochs: 25, Batches/epoch: 7, Total batches: 175\n",
      "Validation: Epoch 0,  val_loss:0.6905, aupr_val: 15.61, auc_val: 62.72\n",
      "**[S] Epoch 0, aupr_val: 15.6051, auc_val: 62.7165 **\n",
      "Validation: Epoch 1,  val_loss:0.6914, aupr_val: 19.54, auc_val: 66.18\n",
      "**[S] Epoch 1, aupr_val: 19.5414, auc_val: 66.1797 **\n",
      "Validation: Epoch 2,  val_loss:0.6756, aupr_val: 20.75, auc_val: 68.43\n",
      "**[S] Epoch 2, aupr_val: 20.7458, auc_val: 68.4253 **\n",
      "Validation: Epoch 3,  val_loss:0.6744, aupr_val: 22.54, auc_val: 71.73\n",
      "**[S] Epoch 3, aupr_val: 22.5352, auc_val: 71.7262 **\n",
      "Validation: Epoch 4,  val_loss:0.6819, aupr_val: 31.10, auc_val: 77.68\n",
      "**[S] Epoch 4, aupr_val: 31.1036, auc_val: 77.6786 **\n",
      "Validation: Epoch 5,  val_loss:0.6564, aupr_val: 35.80, auc_val: 81.17\n",
      "**[S] Epoch 5, aupr_val: 35.7982, auc_val: 81.1688 **\n",
      "Validation: Epoch 6,  val_loss:0.6358, aupr_val: 36.16, auc_val: 82.03\n",
      "**[S] Epoch 6, aupr_val: 36.1645, auc_val: 82.0346 **\n",
      "Validation: Epoch 7,  val_loss:0.6213, aupr_val: 35.95, auc_val: 81.63\n",
      "Validation: Epoch 8,  val_loss:0.5802, aupr_val: 35.86, auc_val: 82.22\n",
      "Epoch 00009: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Validation: Epoch 9,  val_loss:0.6059, aupr_val: 35.51, auc_val: 81.90\n",
      "Validation: Epoch 10,  val_loss:0.6304, aupr_val: 33.91, auc_val: 82.25\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Validation: Epoch 11,  val_loss:0.6301, aupr_val: 34.24, auc_val: 82.25\n",
      "Validation: Epoch 12,  val_loss:0.6283, aupr_val: 34.46, auc_val: 82.12\n",
      "Epoch 00013: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Validation: Epoch 13,  val_loss:0.6280, aupr_val: 34.49, auc_val: 82.14\n",
      "Validation: Epoch 14,  val_loss:0.6277, aupr_val: 34.47, auc_val: 82.12\n",
      "Epoch 00015: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Validation: Epoch 15,  val_loss:0.6276, aupr_val: 34.49, auc_val: 82.14\n",
      "Validation: Epoch 16,  val_loss:0.6276, aupr_val: 34.49, auc_val: 82.14\n",
      "Epoch 00017: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Validation: Epoch 17,  val_loss:0.6276, aupr_val: 34.49, auc_val: 82.14\n",
      "Validation: Epoch 18,  val_loss:0.6276, aupr_val: 34.49, auc_val: 82.14\n",
      "Validation: Epoch 19,  val_loss:0.6276, aupr_val: 34.49, auc_val: 82.14\n",
      "Validation: Epoch 20,  val_loss:0.6276, aupr_val: 34.49, auc_val: 82.14\n",
      "Validation: Epoch 21,  val_loss:0.6276, aupr_val: 34.49, auc_val: 82.14\n",
      "Validation: Epoch 22,  val_loss:0.6276, aupr_val: 34.49, auc_val: 82.14\n",
      "Validation: Epoch 23,  val_loss:0.6276, aupr_val: 34.49, auc_val: 82.14\n",
      "Validation: Epoch 24,  val_loss:0.6276, aupr_val: 34.49, auc_val: 82.14\n",
      "Total Time elapsed: 0.636 mins\n",
      "                 course  percentile       acc      bac       auc     auprc\n",
      "0  microcontroleurs-005          40  0.651515  0.70194  0.761905  0.238901\n",
      "                 course  percentile       acc       bac       auc     auprc\n",
      "0  microcontroleurs-005          40  0.753788  0.800999  0.869097  0.357588\n",
      "Dataset used:  P12\n",
      "Split id: 1\n",
      "1762 221 226 1762 221 226\n",
      "[169] [21] [21]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1762, 1000, 26]) torch.Size([1762, 9]) torch.Size([1762, 1000, 1]) torch.Size([1762])\n",
      "- - Run 1 - -\n",
      "Stop epochs: 25, Batches/epoch: 7, Total batches: 175\n",
      "Validation: Epoch 0,  val_loss:0.6926, aupr_val: 15.14, auc_val: 70.83\n",
      "**[S] Epoch 0, aupr_val: 15.1354, auc_val: 70.8333 **\n",
      "Validation: Epoch 1,  val_loss:0.6908, aupr_val: 15.95, auc_val: 72.10\n",
      "**[S] Epoch 1, aupr_val: 15.9537, auc_val: 72.0952 **\n",
      "Validation: Epoch 2,  val_loss:0.6620, aupr_val: 16.66, auc_val: 72.43\n",
      "**[S] Epoch 2, aupr_val: 16.6590, auc_val: 72.4286 **\n",
      "Validation: Epoch 3,  val_loss:0.6672, aupr_val: 19.69, auc_val: 74.64\n",
      "**[S] Epoch 3, aupr_val: 19.6926, auc_val: 74.6429 **\n",
      "Validation: Epoch 4,  val_loss:0.6582, aupr_val: 26.61, auc_val: 77.05\n",
      "**[S] Epoch 4, aupr_val: 26.6122, auc_val: 77.0476 **\n",
      "Validation: Epoch 5,  val_loss:0.6266, aupr_val: 30.49, auc_val: 79.02\n",
      "**[S] Epoch 5, aupr_val: 30.4881, auc_val: 79.0238 **\n",
      "Validation: Epoch 6,  val_loss:0.6650, aupr_val: 32.85, auc_val: 80.64\n",
      "**[S] Epoch 6, aupr_val: 32.8489, auc_val: 80.6429 **\n",
      "Validation: Epoch 7,  val_loss:0.6264, aupr_val: 31.87, auc_val: 80.74\n",
      "Validation: Epoch 8,  val_loss:0.5813, aupr_val: 35.13, auc_val: 82.10\n",
      "**[S] Epoch 8, aupr_val: 35.1329, auc_val: 82.0952 **\n",
      "Validation: Epoch 9,  val_loss:0.5820, aupr_val: 30.74, auc_val: 81.90\n",
      "Validation: Epoch 10,  val_loss:0.5744, aupr_val: 31.44, auc_val: 82.33\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Validation: Epoch 11,  val_loss:0.6131, aupr_val: 33.55, auc_val: 82.69\n",
      "Validation: Epoch 12,  val_loss:0.6231, aupr_val: 33.77, auc_val: 82.86\n",
      "Epoch 00013: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Validation: Epoch 13,  val_loss:0.6188, aupr_val: 34.02, auc_val: 82.93\n",
      "Validation: Epoch 14,  val_loss:0.6137, aupr_val: 34.11, auc_val: 82.98\n",
      "Epoch 00015: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Validation: Epoch 15,  val_loss:0.6133, aupr_val: 34.24, auc_val: 83.00\n",
      "Validation: Epoch 16,  val_loss:0.6130, aupr_val: 34.45, auc_val: 83.00\n",
      "Epoch 00017: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Validation: Epoch 17,  val_loss:0.6130, aupr_val: 34.45, auc_val: 83.00\n",
      "Validation: Epoch 18,  val_loss:0.6130, aupr_val: 34.47, auc_val: 83.02\n",
      "Epoch 00019: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Validation: Epoch 19,  val_loss:0.6130, aupr_val: 34.47, auc_val: 83.02\n",
      "Validation: Epoch 20,  val_loss:0.6130, aupr_val: 34.47, auc_val: 83.02\n",
      "Validation: Epoch 21,  val_loss:0.6130, aupr_val: 34.47, auc_val: 83.02\n",
      "Validation: Epoch 22,  val_loss:0.6130, aupr_val: 34.47, auc_val: 83.02\n",
      "Validation: Epoch 23,  val_loss:0.6130, aupr_val: 34.47, auc_val: 83.02\n",
      "Validation: Epoch 24,  val_loss:0.6130, aupr_val: 34.47, auc_val: 83.02\n",
      "Total Time elapsed: 0.675 mins\n",
      "                 course  percentile       acc       bac       auc     auprc\n",
      "0  microcontroleurs-005          60  0.791667  0.734568  0.807956  0.238278\n",
      "                 course  percentile       acc       bac      auc     auprc\n",
      "0  microcontroleurs-005          60  0.810606  0.723104  0.85342  0.352954\n",
      "Dataset used:  P12\n",
      "Split id: 1\n",
      "1910 256 231 1910 256 231\n",
      "[75] [9] [10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1910, 1000, 24]) torch.Size([1910, 9]) torch.Size([1910, 1000, 1]) torch.Size([1910])\n",
      "- - Run 1 - -\n",
      "Stop epochs: 25, Batches/epoch: 3, Total batches: 75\n",
      "Validation: Epoch 0,  val_loss:0.6932, aupr_val: 3.73, auc_val: 49.39\n",
      "**[S] Epoch 0, aupr_val: 3.7319, auc_val: 49.3927 **\n",
      "Validation: Epoch 1,  val_loss:0.6910, aupr_val: 3.78, auc_val: 49.57\n",
      "**[S] Epoch 1, aupr_val: 3.7828, auc_val: 49.5726 **\n",
      "Validation: Epoch 2,  val_loss:0.6900, aupr_val: 3.85, auc_val: 50.56\n",
      "**[S] Epoch 2, aupr_val: 3.8456, auc_val: 50.5623 **\n",
      "Validation: Epoch 3,  val_loss:0.6890, aupr_val: 3.89, auc_val: 51.28\n",
      "**[S] Epoch 3, aupr_val: 3.8863, auc_val: 51.2821 **\n",
      "Validation: Epoch 4,  val_loss:0.6893, aupr_val: 3.96, auc_val: 51.64\n",
      "**[S] Epoch 4, aupr_val: 3.9581, auc_val: 51.6419 **\n",
      "Validation: Epoch 5,  val_loss:0.6708, aupr_val: 4.14, auc_val: 52.99\n",
      "**[S] Epoch 5, aupr_val: 4.1444, auc_val: 52.9915 **\n",
      "Validation: Epoch 6,  val_loss:0.6852, aupr_val: 4.25, auc_val: 54.70\n",
      "**[S] Epoch 6, aupr_val: 4.2483, auc_val: 54.7009 **\n",
      "Validation: Epoch 7,  val_loss:0.6601, aupr_val: 4.51, auc_val: 56.82\n",
      "**[S] Epoch 7, aupr_val: 4.5135, auc_val: 56.8151 **\n",
      "Validation: Epoch 8,  val_loss:0.6877, aupr_val: 4.79, auc_val: 59.33\n",
      "**[S] Epoch 8, aupr_val: 4.7935, auc_val: 59.3342 **\n",
      "Validation: Epoch 9,  val_loss:0.6548, aupr_val: 5.14, auc_val: 61.49\n",
      "**[S] Epoch 9, aupr_val: 5.1400, auc_val: 61.4935 **\n",
      "Validation: Epoch 10,  val_loss:0.6738, aupr_val: 5.43, auc_val: 62.80\n",
      "**[S] Epoch 10, aupr_val: 5.4260, auc_val: 62.7980 **\n",
      "Validation: Epoch 11,  val_loss:0.6481, aupr_val: 5.69, auc_val: 63.92\n",
      "**[S] Epoch 11, aupr_val: 5.6896, auc_val: 63.9226 **\n",
      "Validation: Epoch 12,  val_loss:0.6560, aupr_val: 5.81, auc_val: 64.46\n",
      "**[S] Epoch 12, aupr_val: 5.8051, auc_val: 64.4624 **\n",
      "Validation: Epoch 13,  val_loss:0.6354, aupr_val: 6.05, auc_val: 65.32\n",
      "**[S] Epoch 13, aupr_val: 6.0490, auc_val: 65.3171 **\n",
      "Validation: Epoch 14,  val_loss:0.6514, aupr_val: 6.41, auc_val: 66.94\n",
      "**[S] Epoch 14, aupr_val: 6.4111, auc_val: 66.9366 **\n",
      "Validation: Epoch 15,  val_loss:0.6405, aupr_val: 7.09, auc_val: 68.11\n",
      "**[S] Epoch 15, aupr_val: 7.0923, auc_val: 68.1062 **\n",
      "Validation: Epoch 16,  val_loss:0.6117, aupr_val: 7.74, auc_val: 68.74\n",
      "**[S] Epoch 16, aupr_val: 7.7364, auc_val: 68.7359 **\n",
      "Validation: Epoch 17,  val_loss:0.6492, aupr_val: 9.47, auc_val: 70.94\n",
      "**[S] Epoch 17, aupr_val: 9.4658, auc_val: 70.9402 **\n",
      "Validation: Epoch 18,  val_loss:0.6257, aupr_val: 10.29, auc_val: 72.11\n",
      "**[S] Epoch 18, aupr_val: 10.2906, auc_val: 72.1098 **\n",
      "Validation: Epoch 19,  val_loss:0.5975, aupr_val: 10.87, auc_val: 73.41\n",
      "**[S] Epoch 19, aupr_val: 10.8658, auc_val: 73.4143 **\n",
      "Validation: Epoch 20,  val_loss:0.6182, aupr_val: 12.00, auc_val: 74.81\n",
      "**[S] Epoch 20, aupr_val: 12.0030, auc_val: 74.8088 **\n",
      "Validation: Epoch 21,  val_loss:0.6157, aupr_val: 12.28, auc_val: 75.17\n",
      "**[S] Epoch 21, aupr_val: 12.2849, auc_val: 75.1687 **\n",
      "Validation: Epoch 22,  val_loss:0.6089, aupr_val: 12.53, auc_val: 75.48\n",
      "**[S] Epoch 22, aupr_val: 12.5278, auc_val: 75.4836 **\n",
      "Validation: Epoch 23,  val_loss:0.6108, aupr_val: 13.17, auc_val: 75.53\n",
      "**[S] Epoch 23, aupr_val: 13.1667, auc_val: 75.5286 **\n",
      "Validation: Epoch 24,  val_loss:0.5895, aupr_val: 12.68, auc_val: 75.35\n",
      "Total Time elapsed: 0.311 mins\n",
      "        course  percentile       acc       bac       auc    auprc\n",
      "0  venture-001          40  0.785047  0.792283  0.859164  0.13957\n",
      "        course  percentile       acc       bac       auc     auprc\n",
      "0  venture-001          40  0.732087  0.754274  0.804487  0.125986\n",
      "Dataset used:  P12\n",
      "Split id: 1\n",
      "2197 282 277 2197 282 277\n",
      "[77] [9] [10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2197, 1000, 24]) torch.Size([2197, 9]) torch.Size([2197, 1000, 1]) torch.Size([2197])\n",
      "- - Run 1 - -\n",
      "Stop epochs: 25, Batches/epoch: 3, Total batches: 75\n",
      "Validation: Epoch 0,  val_loss:0.6917, aupr_val: 4.24, auc_val: 60.72\n",
      "**[S] Epoch 0, aupr_val: 4.2445, auc_val: 60.7245 **\n",
      "Validation: Epoch 1,  val_loss:0.6930, aupr_val: 4.39, auc_val: 61.70\n",
      "**[S] Epoch 1, aupr_val: 4.3924, auc_val: 61.7013 **\n",
      "Validation: Epoch 2,  val_loss:0.6883, aupr_val: 4.42, auc_val: 61.82\n",
      "**[S] Epoch 2, aupr_val: 4.4185, auc_val: 61.8234 **\n",
      "Validation: Epoch 3,  val_loss:0.6920, aupr_val: 4.47, auc_val: 62.07\n",
      "**[S] Epoch 3, aupr_val: 4.4702, auc_val: 62.0676 **\n",
      "Validation: Epoch 4,  val_loss:0.6880, aupr_val: 4.47, auc_val: 62.23\n",
      "Validation: Epoch 5,  val_loss:0.6921, aupr_val: 4.50, auc_val: 62.47\n",
      "**[S] Epoch 5, aupr_val: 4.5042, auc_val: 62.4746 **\n",
      "Validation: Epoch 6,  val_loss:0.6882, aupr_val: 4.51, auc_val: 62.64\n",
      "**[S] Epoch 6, aupr_val: 4.5051, auc_val: 62.6374 **\n",
      "Validation: Epoch 7,  val_loss:0.6895, aupr_val: 4.51, auc_val: 62.68\n",
      "**[S] Epoch 7, aupr_val: 4.5114, auc_val: 62.6781 **\n",
      "Validation: Epoch 8,  val_loss:0.6683, aupr_val: 4.51, auc_val: 62.60\n",
      "Validation: Epoch 9,  val_loss:0.6763, aupr_val: 4.63, auc_val: 63.00\n",
      "**[S] Epoch 9, aupr_val: 4.6295, auc_val: 63.0037 **\n",
      "Validation: Epoch 10,  val_loss:0.6794, aupr_val: 5.02, auc_val: 64.27\n",
      "**[S] Epoch 10, aupr_val: 5.0158, auc_val: 64.2654 **\n",
      "Validation: Epoch 11,  val_loss:0.6364, aupr_val: 5.21, auc_val: 64.75\n",
      "**[S] Epoch 11, aupr_val: 5.2074, auc_val: 64.7538 **\n",
      "Validation: Epoch 12,  val_loss:0.6858, aupr_val: 5.79, auc_val: 66.99\n",
      "**[S] Epoch 12, aupr_val: 5.7902, auc_val: 66.9923 **\n",
      "Validation: Epoch 13,  val_loss:0.6480, aupr_val: 6.13, auc_val: 67.20\n",
      "**[S] Epoch 13, aupr_val: 6.1325, auc_val: 67.1958 **\n",
      "Validation: Epoch 14,  val_loss:0.6556, aupr_val: 6.70, auc_val: 68.17\n",
      "**[S] Epoch 14, aupr_val: 6.6963, auc_val: 68.1726 **\n",
      "Validation: Epoch 15,  val_loss:0.6405, aupr_val: 7.39, auc_val: 69.11\n",
      "**[S] Epoch 15, aupr_val: 7.3931, auc_val: 69.1087 **\n",
      "Validation: Epoch 16,  val_loss:0.6319, aupr_val: 7.87, auc_val: 69.88\n",
      "**[S] Epoch 16, aupr_val: 7.8704, auc_val: 69.8820 **\n",
      "Validation: Epoch 17,  val_loss:0.6212, aupr_val: 8.58, auc_val: 70.61\n",
      "**[S] Epoch 17, aupr_val: 8.5797, auc_val: 70.6146 **\n",
      "Validation: Epoch 18,  val_loss:0.6404, aupr_val: 10.49, auc_val: 72.65\n",
      "**[S] Epoch 18, aupr_val: 10.4927, auc_val: 72.6496 **\n",
      "Validation: Epoch 19,  val_loss:0.6036, aupr_val: 10.36, auc_val: 73.14\n",
      "Validation: Epoch 20,  val_loss:0.6158, aupr_val: 10.98, auc_val: 74.20\n",
      "**[S] Epoch 20, aupr_val: 10.9849, auc_val: 74.1962 **\n",
      "Validation: Epoch 21,  val_loss:0.5955, aupr_val: 10.83, auc_val: 74.56\n",
      "Validation: Epoch 22,  val_loss:0.5999, aupr_val: 11.81, auc_val: 75.50\n",
      "**[S] Epoch 22, aupr_val: 11.8080, auc_val: 75.4986 **\n",
      "Validation: Epoch 23,  val_loss:0.5681, aupr_val: 11.52, auc_val: 75.91\n",
      "Validation: Epoch 24,  val_loss:0.6116, aupr_val: 10.99, auc_val: 76.52\n",
      "Epoch 00025: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Total Time elapsed: 0.347 mins\n",
      "        course  percentile       acc       bac       auc     auprc\n",
      "0  venture-001          60  0.722741  0.760129  0.878778  0.264555\n",
      "        course  percentile       acc       bac     auc     auprc\n",
      "0  venture-001          60  0.691589  0.625534  0.7849  0.116844\n",
      "Dataset used:  P12\n",
      "Split id: 1\n",
      "395 50 51 395 50 51\n",
      "[34] [4] [5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([395, 1000, 12]) torch.Size([395, 9]) torch.Size([395, 1000, 1]) torch.Size([395])\n",
      "- - Run 1 - -\n",
      "Stop epochs: 25, Batches/epoch: 1, Total batches: 25\n",
      "Validation: Epoch 0,  val_loss:0.6998, aupr_val: 10.49, auc_val: 51.63\n",
      "**[S] Epoch 0, aupr_val: 10.4861, auc_val: 51.6304 **\n",
      "Validation: Epoch 1,  val_loss:0.6959, aupr_val: 10.28, auc_val: 47.83\n",
      "Validation: Epoch 2,  val_loss:0.6908, aupr_val: 12.54, auc_val: 53.26\n",
      "**[S] Epoch 2, aupr_val: 12.5383, auc_val: 53.2609 **\n",
      "Validation: Epoch 3,  val_loss:0.6897, aupr_val: 12.98, auc_val: 54.89\n",
      "**[S] Epoch 3, aupr_val: 12.9831, auc_val: 54.8913 **\n",
      "Validation: Epoch 4,  val_loss:0.6914, aupr_val: 13.26, auc_val: 55.98\n",
      "**[S] Epoch 4, aupr_val: 13.2629, auc_val: 55.9783 **\n",
      "Validation: Epoch 5,  val_loss:0.6941, aupr_val: 13.36, auc_val: 55.98\n",
      "**[S] Epoch 5, aupr_val: 13.3608, auc_val: 55.9783 **\n",
      "Validation: Epoch 6,  val_loss:0.6964, aupr_val: 13.57, auc_val: 57.07\n",
      "**[S] Epoch 6, aupr_val: 13.5725, auc_val: 57.0652 **\n",
      "Validation: Epoch 7,  val_loss:0.6959, aupr_val: 12.64, auc_val: 56.52\n",
      "Validation: Epoch 8,  val_loss:0.6934, aupr_val: 12.51, auc_val: 55.98\n",
      "Epoch 00009: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Validation: Epoch 9,  val_loss:0.6932, aupr_val: 12.51, auc_val: 55.98\n",
      "Validation: Epoch 10,  val_loss:0.6929, aupr_val: 12.20, auc_val: 54.89\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Validation: Epoch 11,  val_loss:0.6929, aupr_val: 12.20, auc_val: 54.89\n",
      "Validation: Epoch 12,  val_loss:0.6929, aupr_val: 12.06, auc_val: 54.35\n",
      "Epoch 00013: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Validation: Epoch 13,  val_loss:0.6929, aupr_val: 12.06, auc_val: 54.35\n",
      "Validation: Epoch 14,  val_loss:0.6929, aupr_val: 12.06, auc_val: 54.35\n",
      "Epoch 00015: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Validation: Epoch 15,  val_loss:0.6929, aupr_val: 12.06, auc_val: 54.35\n",
      "Validation: Epoch 16,  val_loss:0.6929, aupr_val: 12.06, auc_val: 54.35\n",
      "Epoch 00017: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Validation: Epoch 17,  val_loss:0.6929, aupr_val: 12.06, auc_val: 54.35\n",
      "Validation: Epoch 18,  val_loss:0.6929, aupr_val: 12.06, auc_val: 54.35\n",
      "Validation: Epoch 19,  val_loss:0.6929, aupr_val: 12.06, auc_val: 54.35\n",
      "Validation: Epoch 20,  val_loss:0.6929, aupr_val: 12.06, auc_val: 54.35\n",
      "Validation: Epoch 21,  val_loss:0.6929, aupr_val: 12.06, auc_val: 54.35\n",
      "Validation: Epoch 22,  val_loss:0.6929, aupr_val: 12.06, auc_val: 54.35\n",
      "Validation: Epoch 23,  val_loss:0.6929, aupr_val: 12.06, auc_val: 54.35\n",
      "Validation: Epoch 24,  val_loss:0.6929, aupr_val: 12.06, auc_val: 54.35\n",
      "Total Time elapsed: 0.074 mins\n",
      "                 course  percentile       acc      bac       auc     auprc\n",
      "0  analysenumerique-001          40  0.117647  0.51087  0.573913  0.158549\n",
      "                 course  percentile   acc       bac       auc     auprc\n",
      "0  analysenumerique-001          40  0.18  0.440217  0.576087  0.138651\n",
      "Dataset used:  P12\n",
      "Split id: 1\n",
      "398 50 51 398 50 51\n",
      "[34] [4] [5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([398, 1000, 12]) torch.Size([398, 9]) torch.Size([398, 1000, 1]) torch.Size([398])\n",
      "- - Run 1 - -\n",
      "Stop epochs: 25, Batches/epoch: 1, Total batches: 25\n",
      "Validation: Epoch 0,  val_loss:0.7047, aupr_val: 9.67, auc_val: 50.54\n",
      "**[S] Epoch 0, aupr_val: 9.6750, auc_val: 50.5435 **\n",
      "Validation: Epoch 1,  val_loss:0.6949, aupr_val: 11.64, auc_val: 59.78\n",
      "**[S] Epoch 1, aupr_val: 11.6388, auc_val: 59.7826 **\n",
      "Validation: Epoch 2,  val_loss:0.6868, aupr_val: 12.26, auc_val: 63.04\n",
      "**[S] Epoch 2, aupr_val: 12.2583, auc_val: 63.0435 **\n",
      "Validation: Epoch 3,  val_loss:0.6853, aupr_val: 13.14, auc_val: 65.76\n",
      "**[S] Epoch 3, aupr_val: 13.1358, auc_val: 65.7609 **\n",
      "Validation: Epoch 4,  val_loss:0.6869, aupr_val: 12.48, auc_val: 63.04\n",
      "Validation: Epoch 5,  val_loss:0.6901, aupr_val: 12.36, auc_val: 61.41\n",
      "Epoch 00006: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Validation: Epoch 6,  val_loss:0.6904, aupr_val: 12.57, auc_val: 62.50\n",
      "Validation: Epoch 7,  val_loss:0.6907, aupr_val: 12.89, auc_val: 63.04\n",
      "Epoch 00008: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Validation: Epoch 8,  val_loss:0.6907, aupr_val: 12.78, auc_val: 62.50\n",
      "Validation: Epoch 9,  val_loss:0.6908, aupr_val: 12.78, auc_val: 62.50\n",
      "Epoch 00010: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Validation: Epoch 10,  val_loss:0.6908, aupr_val: 12.78, auc_val: 62.50\n",
      "Validation: Epoch 11,  val_loss:0.6908, aupr_val: 12.78, auc_val: 62.50\n",
      "Epoch 00012: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Validation: Epoch 12,  val_loss:0.6908, aupr_val: 12.78, auc_val: 62.50\n",
      "Validation: Epoch 13,  val_loss:0.6908, aupr_val: 12.78, auc_val: 62.50\n",
      "Epoch 00014: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Validation: Epoch 14,  val_loss:0.6908, aupr_val: 12.78, auc_val: 62.50\n",
      "Validation: Epoch 15,  val_loss:0.6908, aupr_val: 12.78, auc_val: 62.50\n",
      "Validation: Epoch 16,  val_loss:0.6908, aupr_val: 12.78, auc_val: 62.50\n",
      "Validation: Epoch 17,  val_loss:0.6908, aupr_val: 12.78, auc_val: 62.50\n",
      "Validation: Epoch 18,  val_loss:0.6908, aupr_val: 12.78, auc_val: 62.50\n",
      "Validation: Epoch 19,  val_loss:0.6908, aupr_val: 12.78, auc_val: 62.50\n",
      "Validation: Epoch 20,  val_loss:0.6908, aupr_val: 12.78, auc_val: 62.50\n",
      "Validation: Epoch 21,  val_loss:0.6908, aupr_val: 12.78, auc_val: 62.50\n",
      "Validation: Epoch 22,  val_loss:0.6908, aupr_val: 12.78, auc_val: 62.50\n",
      "Validation: Epoch 23,  val_loss:0.6908, aupr_val: 12.78, auc_val: 62.50\n",
      "Validation: Epoch 24,  val_loss:0.6908, aupr_val: 12.78, auc_val: 62.50\n",
      "Total Time elapsed: 0.084 mins\n",
      "                 course  percentile       acc  bac       auc     auprc\n",
      "0  analysenumerique-001          60  0.901961  0.5  0.382609  0.123566\n",
      "                 course  percentile   acc  bac       auc     auprc\n",
      "0  analysenumerique-001          60  0.92  0.5  0.657609  0.131793\n",
      "Dataset used:  P12\n",
      "Split id: 1\n",
      "595 77 76 595 77 76\n",
      "[234] [29] [31]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([595, 1000, 26]) torch.Size([595, 9]) torch.Size([595, 1000, 1]) torch.Size([595])\n",
      "- - Run 1 - -\n",
      "Stop epochs: 25, Batches/epoch: 5, Total batches: 125\n",
      "Validation: Epoch 0,  val_loss:0.6920, aupr_val: 39.16, auc_val: 53.16\n",
      "**[S] Epoch 0, aupr_val: 39.1592, auc_val: 53.1609 **\n",
      "Validation: Epoch 1,  val_loss:0.6932, aupr_val: 39.59, auc_val: 54.09\n",
      "**[S] Epoch 1, aupr_val: 39.5928, auc_val: 54.0948 **\n",
      "Validation: Epoch 2,  val_loss:0.6940, aupr_val: 38.83, auc_val: 51.72\n",
      "Validation: Epoch 3,  val_loss:0.6935, aupr_val: 38.18, auc_val: 51.08\n",
      "Epoch 00004: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Validation: Epoch 4,  val_loss:0.6929, aupr_val: 38.24, auc_val: 51.15\n",
      "Validation: Epoch 5,  val_loss:0.6928, aupr_val: 38.15, auc_val: 50.93\n",
      "Epoch 00006: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Validation: Epoch 6,  val_loss:0.6929, aupr_val: 38.15, auc_val: 50.93\n",
      "Validation: Epoch 7,  val_loss:0.6929, aupr_val: 38.15, auc_val: 50.93\n",
      "Epoch 00008: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Validation: Epoch 8,  val_loss:0.6929, aupr_val: 38.15, auc_val: 50.93\n",
      "Validation: Epoch 9,  val_loss:0.6929, aupr_val: 38.15, auc_val: 50.93\n",
      "Epoch 00010: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Validation: Epoch 10,  val_loss:0.6929, aupr_val: 38.15, auc_val: 50.93\n",
      "Validation: Epoch 11,  val_loss:0.6929, aupr_val: 38.15, auc_val: 50.93\n",
      "Epoch 00012: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Validation: Epoch 12,  val_loss:0.6929, aupr_val: 38.15, auc_val: 50.93\n",
      "Validation: Epoch 13,  val_loss:0.6929, aupr_val: 38.15, auc_val: 50.93\n",
      "Validation: Epoch 14,  val_loss:0.6929, aupr_val: 38.15, auc_val: 50.93\n",
      "Validation: Epoch 15,  val_loss:0.6929, aupr_val: 38.15, auc_val: 50.93\n",
      "Validation: Epoch 16,  val_loss:0.6929, aupr_val: 38.15, auc_val: 50.93\n",
      "Validation: Epoch 17,  val_loss:0.6929, aupr_val: 38.15, auc_val: 50.93\n",
      "Validation: Epoch 18,  val_loss:0.6929, aupr_val: 38.15, auc_val: 50.93\n",
      "Validation: Epoch 19,  val_loss:0.6929, aupr_val: 38.15, auc_val: 50.93\n",
      "Validation: Epoch 20,  val_loss:0.6929, aupr_val: 38.15, auc_val: 50.93\n",
      "Validation: Epoch 21,  val_loss:0.6929, aupr_val: 38.15, auc_val: 50.93\n",
      "Validation: Epoch 22,  val_loss:0.6929, aupr_val: 38.15, auc_val: 50.93\n",
      "Validation: Epoch 23,  val_loss:0.6929, aupr_val: 38.15, auc_val: 50.93\n",
      "Validation: Epoch 24,  val_loss:0.6929, aupr_val: 38.15, auc_val: 50.93\n",
      "Total Time elapsed: 0.425 mins\n",
      "       course  percentile       acc       bac       auc     auprc\n",
      "0  cpp-fr-001          40  0.518987  0.535618  0.538978  0.398184\n",
      "       course  percentile       acc       bac      auc    auprc\n",
      "0  cpp-fr-001          40  0.531646  0.538435  0.52483  0.38993\n",
      "Dataset used:  P12\n",
      "Split id: 1\n",
      "626 78 78 626 78 78\n",
      "[241] [29] [31]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([626, 1000, 26]) torch.Size([626, 9]) torch.Size([626, 1000, 1]) torch.Size([626])\n",
      "- - Run 1 - -\n",
      "Stop epochs: 25, Batches/epoch: 6, Total batches: 150\n",
      "Validation: Epoch 0,  val_loss:0.6926, aupr_val: 41.40, auc_val: 53.06\n",
      "**[S] Epoch 0, aupr_val: 41.3998, auc_val: 53.0612 **\n",
      "Validation: Epoch 1,  val_loss:0.6921, aupr_val: 40.90, auc_val: 53.34\n",
      "Validation: Epoch 2,  val_loss:0.6917, aupr_val: 41.20, auc_val: 53.62\n",
      "Epoch 00003: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Validation: Epoch 3,  val_loss:0.6933, aupr_val: 41.24, auc_val: 53.69\n",
      "Validation: Epoch 4,  val_loss:0.6930, aupr_val: 41.24, auc_val: 53.55\n",
      "Epoch 00005: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Validation: Epoch 5,  val_loss:0.6929, aupr_val: 41.24, auc_val: 53.55\n",
      "Validation: Epoch 6,  val_loss:0.6928, aupr_val: 41.24, auc_val: 53.55\n",
      "Epoch 00007: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Validation: Epoch 7,  val_loss:0.6928, aupr_val: 41.24, auc_val: 53.55\n",
      "Validation: Epoch 8,  val_loss:0.6928, aupr_val: 41.24, auc_val: 53.55\n",
      "Epoch 00009: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Validation: Epoch 9,  val_loss:0.6928, aupr_val: 41.24, auc_val: 53.55\n",
      "Validation: Epoch 10,  val_loss:0.6928, aupr_val: 41.24, auc_val: 53.55\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Validation: Epoch 11,  val_loss:0.6928, aupr_val: 41.24, auc_val: 53.55\n",
      "Validation: Epoch 12,  val_loss:0.6928, aupr_val: 41.24, auc_val: 53.55\n",
      "Validation: Epoch 13,  val_loss:0.6928, aupr_val: 41.24, auc_val: 53.55\n",
      "Validation: Epoch 14,  val_loss:0.6928, aupr_val: 41.24, auc_val: 53.55\n",
      "Validation: Epoch 15,  val_loss:0.6928, aupr_val: 41.24, auc_val: 53.55\n",
      "Validation: Epoch 16,  val_loss:0.6928, aupr_val: 41.24, auc_val: 53.55\n",
      "Validation: Epoch 17,  val_loss:0.6928, aupr_val: 41.24, auc_val: 53.55\n",
      "Validation: Epoch 18,  val_loss:0.6928, aupr_val: 41.24, auc_val: 53.55\n",
      "Validation: Epoch 19,  val_loss:0.6928, aupr_val: 41.24, auc_val: 53.55\n",
      "Validation: Epoch 20,  val_loss:0.6928, aupr_val: 41.24, auc_val: 53.55\n",
      "Validation: Epoch 21,  val_loss:0.6928, aupr_val: 41.24, auc_val: 53.55\n",
      "Validation: Epoch 22,  val_loss:0.6928, aupr_val: 41.24, auc_val: 53.55\n",
      "Validation: Epoch 23,  val_loss:0.6928, aupr_val: 41.24, auc_val: 53.55\n",
      "Validation: Epoch 24,  val_loss:0.6928, aupr_val: 41.24, auc_val: 53.55\n",
      "Total Time elapsed: 0.560 mins\n",
      "       course  percentile      acc       bac       auc     auprc\n",
      "0  cpp-fr-001          60  0.56962  0.503024  0.587366  0.443525\n",
      "       course  percentile       acc       bac       auc     auprc\n",
      "0  cpp-fr-001          60  0.620253  0.558163  0.527211  0.426176\n",
      "Dataset used:  P12\n",
      "Split id: 1\n",
      "73 9 10 73 9 10\n",
      "[50] [6] [7]\n",
      "torch.Size([73, 1000, 24]) torch.Size([73, 9]) torch.Size([73, 1000, 1]) torch.Size([73])\n",
      "- - Run 1 - -\n",
      "Stop epochs: 25, Batches/epoch: 0, Total batches: 0\n",
      "Validation: Epoch 0,  val_loss:0.6914, aupr_val: 66.67, auc_val: 50.00\n",
      "**[S] Epoch 0, aupr_val: 66.6667, auc_val: 50.0000 **\n",
      "Validation: Epoch 1,  val_loss:0.6914, aupr_val: 66.67, auc_val: 50.00\n",
      "Validation: Epoch 2,  val_loss:0.6914, aupr_val: 66.67, auc_val: 50.00\n",
      "Epoch 00003: reducing learning rate of group 0 to 1.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch 3,  val_loss:0.6914, aupr_val: 66.67, auc_val: 50.00\n",
      "Validation: Epoch 4,  val_loss:0.6914, aupr_val: 66.67, auc_val: 50.00\n",
      "Epoch 00005: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Validation: Epoch 5,  val_loss:0.6914, aupr_val: 66.67, auc_val: 50.00\n",
      "Validation: Epoch 6,  val_loss:0.6914, aupr_val: 66.67, auc_val: 50.00\n",
      "Epoch 00007: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Validation: Epoch 7,  val_loss:0.6914, aupr_val: 66.67, auc_val: 50.00\n",
      "Validation: Epoch 8,  val_loss:0.6914, aupr_val: 66.67, auc_val: 50.00\n",
      "Epoch 00009: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Validation: Epoch 9,  val_loss:0.6914, aupr_val: 66.67, auc_val: 50.00\n",
      "Validation: Epoch 10,  val_loss:0.6914, aupr_val: 66.67, auc_val: 50.00\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Validation: Epoch 11,  val_loss:0.6914, aupr_val: 66.67, auc_val: 50.00\n",
      "Validation: Epoch 12,  val_loss:0.6914, aupr_val: 66.67, auc_val: 50.00\n",
      "Validation: Epoch 13,  val_loss:0.6914, aupr_val: 66.67, auc_val: 50.00\n",
      "Validation: Epoch 14,  val_loss:0.6914, aupr_val: 66.67, auc_val: 50.00\n",
      "Validation: Epoch 15,  val_loss:0.6914, aupr_val: 66.67, auc_val: 50.00\n",
      "Validation: Epoch 16,  val_loss:0.6914, aupr_val: 66.67, auc_val: 50.00\n",
      "Validation: Epoch 17,  val_loss:0.6914, aupr_val: 66.67, auc_val: 50.00\n",
      "Validation: Epoch 18,  val_loss:0.6914, aupr_val: 66.67, auc_val: 50.00\n",
      "Validation: Epoch 19,  val_loss:0.6914, aupr_val: 66.67, auc_val: 50.00\n",
      "Validation: Epoch 20,  val_loss:0.6914, aupr_val: 66.67, auc_val: 50.00\n",
      "Validation: Epoch 21,  val_loss:0.6914, aupr_val: 66.67, auc_val: 50.00\n",
      "Validation: Epoch 22,  val_loss:0.6914, aupr_val: 66.67, auc_val: 50.00\n",
      "Validation: Epoch 23,  val_loss:0.6914, aupr_val: 66.67, auc_val: 50.00\n",
      "Validation: Epoch 24,  val_loss:0.6914, aupr_val: 66.67, auc_val: 50.00\n",
      "Total Time elapsed: 0.006 mins\n",
      "           course  percentile  acc  bac  auc  auprc\n",
      "0  structures-001          40  0.7  0.5  0.5    0.7\n",
      "           course  percentile       acc  bac  auc     auprc\n",
      "0  structures-001          40  0.666667  0.5  0.5  0.666667\n",
      "Dataset used:  P12\n",
      "Split id: 1\n",
      "75 9 10 75 9 10\n",
      "[50] [6] [7]\n",
      "torch.Size([75, 1000, 24]) torch.Size([75, 9]) torch.Size([75, 1000, 1]) torch.Size([75])\n",
      "- - Run 1 - -\n",
      "Stop epochs: 25, Batches/epoch: 0, Total batches: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch 0,  val_loss:0.6984, aupr_val: 66.67, auc_val: 50.00\n",
      "**[S] Epoch 0, aupr_val: 66.6667, auc_val: 50.0000 **\n",
      "Validation: Epoch 1,  val_loss:0.6984, aupr_val: 66.67, auc_val: 50.00\n",
      "Validation: Epoch 2,  val_loss:0.6984, aupr_val: 66.67, auc_val: 50.00\n",
      "Epoch 00003: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Validation: Epoch 3,  val_loss:0.6984, aupr_val: 66.67, auc_val: 50.00\n",
      "Validation: Epoch 4,  val_loss:0.6984, aupr_val: 66.67, auc_val: 50.00\n",
      "Epoch 00005: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Validation: Epoch 5,  val_loss:0.6984, aupr_val: 66.67, auc_val: 50.00\n",
      "Validation: Epoch 6,  val_loss:0.6984, aupr_val: 66.67, auc_val: 50.00\n",
      "Epoch 00007: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Validation: Epoch 7,  val_loss:0.6984, aupr_val: 66.67, auc_val: 50.00\n",
      "Validation: Epoch 8,  val_loss:0.6984, aupr_val: 66.67, auc_val: 50.00\n",
      "Epoch 00009: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Validation: Epoch 9,  val_loss:0.6984, aupr_val: 66.67, auc_val: 50.00\n",
      "Validation: Epoch 10,  val_loss:0.6984, aupr_val: 66.67, auc_val: 50.00\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Validation: Epoch 11,  val_loss:0.6984, aupr_val: 66.67, auc_val: 50.00\n",
      "Validation: Epoch 12,  val_loss:0.6984, aupr_val: 66.67, auc_val: 50.00\n",
      "Validation: Epoch 13,  val_loss:0.6984, aupr_val: 66.67, auc_val: 50.00\n",
      "Validation: Epoch 14,  val_loss:0.6984, aupr_val: 66.67, auc_val: 50.00\n",
      "Validation: Epoch 15,  val_loss:0.6984, aupr_val: 66.67, auc_val: 50.00\n",
      "Validation: Epoch 16,  val_loss:0.6984, aupr_val: 66.67, auc_val: 50.00\n",
      "Validation: Epoch 17,  val_loss:0.6984, aupr_val: 66.67, auc_val: 50.00\n",
      "Validation: Epoch 18,  val_loss:0.6984, aupr_val: 66.67, auc_val: 50.00\n",
      "Validation: Epoch 19,  val_loss:0.6984, aupr_val: 66.67, auc_val: 50.00\n",
      "Validation: Epoch 20,  val_loss:0.6984, aupr_val: 66.67, auc_val: 50.00\n",
      "Validation: Epoch 21,  val_loss:0.6984, aupr_val: 66.67, auc_val: 50.00\n",
      "Validation: Epoch 22,  val_loss:0.6984, aupr_val: 66.67, auc_val: 50.00\n",
      "Validation: Epoch 23,  val_loss:0.6984, aupr_val: 66.67, auc_val: 50.00\n",
      "Validation: Epoch 24,  val_loss:0.6984, aupr_val: 66.67, auc_val: 50.00\n",
      "Total Time elapsed: 0.007 mins\n",
      "           course  percentile  acc  bac  auc  auprc\n",
      "0  structures-001          60  0.3  0.5  0.5    0.7\n",
      "           course  percentile       acc  bac  auc     auprc\n",
      "0  structures-001          60  0.333333  0.5  0.5  0.666667\n"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "#import argparse\n",
    "#parser = argparse.ArgumentParser()\n",
    "\n",
    "#parser.add_argument('--predictive_label', type=str, default='mortality', choices=['mortality', 'LoS'],\n",
    "#                    help='use this only with P12 dataset (mortality or length of stay)')\n",
    "\n",
    "#args, unknown = parser.parse_known_args()\n",
    "\n",
    "\n",
    "def one_hot(y_):\n",
    "    # Function to encode output labels from number indexes\n",
    "    # e.g.: [[5], [0], [3]] --> [[0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0]]\n",
    "    y_ = y_.reshape(len(y_))\n",
    "\n",
    "    y_ = [int(x) for x in y_]\n",
    "    n_values = np.max(y_) + 1\n",
    "    return np.eye(n_values)[np.array(y_, dtype=np.int32)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for MOOC_idx, MOOC in enumerate(MOOCs_list):\n",
    "    \n",
    "    for percentile in [40, 60]:\n",
    "        \n",
    "        \n",
    "        d_inp = dims[percentile][MOOC_idx]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        base_path = '/../data/prep_data'\n",
    "        data_path = '/../data'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        arch = 'standard'\n",
    "\n",
    "        model_path = '../../models/seft/'\n",
    "\n",
    "        dataset = 'P12'\n",
    "        print('Dataset used: ', dataset)\n",
    "        #print('args.dataset, args.splittype, args.reverse, args.withmissingratio, args.feature_removal_level',\n",
    "        #      args.dataset, args.splittype, args.reverse, args.withmissingratio, args.feature_removal_level)\n",
    "\n",
    "\n",
    "        baseline = True\n",
    "        split = 'random'  # possible values: 'random', 'age', 'gender' ('age' not possible for dataset 'eICU')\n",
    "        reverse = False  # False or True\n",
    "        feature_removal_level = 'no_removal'  # 'set', 'sample'\n",
    "\n",
    "\n",
    "\n",
    "        missing_ratio = 0\n",
    "\n",
    "        num_epochs = 25\n",
    "\n",
    "        learning_rate = 0.001\n",
    "\n",
    "\n",
    "\n",
    "        d_static = 9\n",
    "        #d_inp = 6\n",
    "        static_info = 0\n",
    "\n",
    "\n",
    "        d_model = d_inp\n",
    "        nhid = 2 * d_model\n",
    "\n",
    "        nlayers = 2\n",
    "        nhead = 2\n",
    "\n",
    "        dropout = 0.2\n",
    "\n",
    "\n",
    "        max_len = 1000\n",
    "        n_classes = 2\n",
    "\n",
    "\n",
    "        aggreg = 'mean'\n",
    "\n",
    "        MAX = 100\n",
    "\n",
    "        n_runs = 1\n",
    "        n_splits = 1\n",
    "\n",
    "        subset = False\n",
    "\n",
    "        acc_arr = np.zeros((n_splits, n_runs))\n",
    "        auprc_arr = np.zeros((n_splits, n_runs))\n",
    "        auroc_arr = np.zeros((n_splits, n_runs))\n",
    "        precision_arr = np.zeros((n_splits, n_runs))\n",
    "        recall_arr = np.zeros((n_splits, n_runs))\n",
    "        F1_arr = np.zeros((n_splits, n_runs))\n",
    "\n",
    "        for k in range(n_splits):\n",
    "            split_idx = k + 1\n",
    "            print('Split id: %d' % split_idx)\n",
    "\n",
    "            #if dataset == 'P12':\n",
    "            #    if subset == True:\n",
    "            #        split_path = '/splits/phy12_split_subset' + str(split_idx) + '.npy'\n",
    "            #    else:\n",
    "            #        split_path = '/splits/phy12_split' + str(split_idx) + '.npy'\n",
    "            #elif dataset == 'P19':\n",
    "            #    split_path = '/splits/phy19_split' + str(split_idx) + '_new.npy'\n",
    "            #elif dataset == 'eICU':\n",
    "            #    split_path = '/splits/eICU_split' + str(split_idx) + '.npy'\n",
    "            #elif dataset == 'PAM':\n",
    "            #    split_path = '/splits/PAM_split_' + str(split_idx) + '.npy'\n",
    "\n",
    "            #Ptrain, Pval, Ptest, ytrain, yval, ytest = get_data_split(base_path, split_path, split_type=split,\n",
    "            #                                                          reverse=reverse, baseline=baseline, dataset=dataset,\n",
    "            #                                                          predictive_label=args.predictive_label)\n",
    "\n",
    "            Pdict_list = np.load(os.path.join(base_path, f\"{MOOC}_{percentile}_data_hard_fail.npy\"), allow_pickle=True)\n",
    "            arr_outcomes = np.load(os.path.join(base_path, f\"{MOOC}_{percentile}_y_hard_fail.npy\"), allow_pickle=True)\n",
    "\n",
    "            #Ptrain, Ptest, ytrain, ytest = train_test_split(Pdict_list, arr_outcomes, test_size=0.1, random_state=1)\n",
    "            #Ptrain, Pval, ytrain, yval = train_test_split(Ptrain, ytrain, test_size=1/9, random_state=1)\n",
    "            args_train, args_val, args_test = np.load(os.path.join(data_path, \n",
    "                                                                           'split_args', f\"split_{MOOC.replace('-', '_')}.npy\"),\n",
    "                                                             allow_pickle=True)\n",
    "            Ptrain = Pdict_list[args_train]\n",
    "            Pval = Pdict_list[args_val]\n",
    "            Ptest = Pdict_list[args_test]\n",
    "            ytrain = arr_outcomes[args_train, :]\n",
    "            yval = arr_outcomes[args_val, :]\n",
    "            ytest = arr_outcomes[args_test, :]\n",
    "\n",
    "\n",
    "            zero_indices = [i for i, item in enumerate(Ptrain) if item['length'] == 0]\n",
    "            #zero_Ptrain = Ptrain[zero_indices]\n",
    "            Ptrain = np.delete(Ptrain, zero_indices, axis=0)\n",
    "            ytrain = np.delete(ytrain, zero_indices, axis=0)\n",
    "\n",
    "            zero_indices = [i for i, item in enumerate(Pval) if item['length'] == 0]\n",
    "            zero_yval = yval[zero_indices]\n",
    "            Pval = np.delete(Pval, zero_indices, axis=0)\n",
    "            yval = np.delete(yval, zero_indices, axis=0)\n",
    "\n",
    "            zero_indices = [i for i, item in enumerate(Ptest) if item['length'] == 0]\n",
    "            zero_ytest = ytest[zero_indices]\n",
    "            Ptest = np.delete(Ptest, zero_indices, axis=0)  \n",
    "            ytest = np.delete(ytest, zero_indices, axis=0)\n",
    "\n",
    "            print(len(Ptrain), len(Pval), len(Ptest), len(ytrain), len(yval), len(ytest))\n",
    "            print(sum(ytrain), sum(yval), sum(ytest))\n",
    "\n",
    "            if dataset == 'P12' or dataset == 'P19' or dataset == 'eICU':\n",
    "                T, F = Ptrain[0]['arr'].shape\n",
    "                D = len(Ptrain[0]['extended_static'])\n",
    "\n",
    "                Ptrain_tensor = np.zeros((len(Ptrain), T, F))\n",
    "                Ptrain_static_tensor = np.zeros((len(Ptrain), D))\n",
    "\n",
    "                for i in range(len(Ptrain)):\n",
    "                    Ptrain_tensor[i] = Ptrain[i]['arr']\n",
    "                    Ptrain_static_tensor[i] = Ptrain[i]['extended_static']\n",
    "\n",
    "                mf, stdf = getStats(Ptrain_tensor)\n",
    "                ms, ss = getStats_static(Ptrain_static_tensor, dataset=dataset)\n",
    "\n",
    "                Ptrain_tensor, Ptrain_static_tensor, Ptrain_time_tensor, ytrain_tensor = tensorize_normalize(Ptrain, ytrain, mf, stdf, ms, ss)\n",
    "                Pval_tensor, Pval_static_tensor, Pval_time_tensor, yval_tensor = tensorize_normalize(Pval, yval, mf, stdf, ms, ss)\n",
    "                Ptest_tensor, Ptest_static_tensor, Ptest_time_tensor, ytest_tensor = tensorize_normalize(Ptest, ytest, mf, stdf, ms, ss)\n",
    "                print(Ptrain_tensor.shape, Ptrain_static_tensor.shape, Ptrain_time_tensor.shape, ytrain_tensor.shape)\n",
    "            elif dataset == 'PAM':\n",
    "                T, F = Ptrain[0].shape\n",
    "                D = 1\n",
    "\n",
    "                Ptrain_tensor = Ptrain\n",
    "                Ptrain_static_tensor = np.zeros((len(Ptrain), D))\n",
    "\n",
    "                mf, stdf = getStats(Ptrain)\n",
    "                Ptrain_tensor, Ptrain_static_tensor, Ptrain_time_tensor, ytrain_tensor = tensorize_normalize_other(Ptrain, ytrain, mf, stdf)\n",
    "                Pval_tensor, Pval_static_tensor, Pval_time_tensor, yval_tensor = tensorize_normalize_other(Pval, yval, mf, stdf)\n",
    "                Ptest_tensor, Ptest_static_tensor, Ptest_time_tensor, ytest_tensor = tensorize_normalize_other(Ptest, ytest, mf, stdf)\n",
    "\n",
    "            # remove part of variables in validation and test set\n",
    "            if missing_ratio > 0:\n",
    "                num_all_features = int(Pval_tensor.shape[2] / 2)\n",
    "                num_missing_features = round(missing_ratio * num_all_features)\n",
    "                if feature_removal_level == 'sample':\n",
    "                    for i, patient in enumerate(Pval_tensor):\n",
    "                        idx = np.random.choice(num_all_features, num_missing_features, replace=False)\n",
    "                        patient[:, idx] = torch.zeros(Pval_tensor.shape[1], num_missing_features)  # values\n",
    "                        patient[:, idx + num_all_features] = torch.zeros(Pval_tensor.shape[1],\n",
    "                                                                         num_missing_features)  # masks\n",
    "                        Pval_tensor[i] = patient\n",
    "                    for i, patient in enumerate(Ptest_tensor):\n",
    "                        idx = np.random.choice(num_all_features, num_missing_features, replace=False)\n",
    "                        patient[:, idx] = torch.zeros(Ptest_tensor.shape[1], num_missing_features)  # values\n",
    "                        patient[:, idx + num_all_features] = torch.zeros(Ptest_tensor.shape[1],\n",
    "                                                                         num_missing_features)  # masks\n",
    "                        Ptest_tensor[i] = patient\n",
    "                elif feature_removal_level == 'set':\n",
    "                    density_score_indices = np.load('saved/IG_density_scores_' + dataset + '.npy', allow_pickle=True)[:, 0]\n",
    "                    idx = density_score_indices[:num_missing_features].astype(int)\n",
    "                    Pval_tensor[:, :, idx] = torch.zeros(Pval_tensor.shape[0], Pval_tensor.shape[1], num_missing_features)  # values\n",
    "                    Pval_tensor[:, :, idx + num_all_features] = torch.zeros(Pval_tensor.shape[0], Pval_tensor.shape[1], num_missing_features)  # masks\n",
    "                    Ptest_tensor[:, :, idx] = torch.zeros(Ptest_tensor.shape[0], Ptest_tensor.shape[1], num_missing_features)  # values\n",
    "                    Ptest_tensor[:, :, idx + num_all_features] = torch.zeros(Ptest_tensor.shape[0], Ptest_tensor.shape[1], num_missing_features)  # masks\n",
    "\n",
    "            Ptrain_tensor = Ptrain_tensor.permute(1, 0, 2)\n",
    "            Pval_tensor = Pval_tensor.permute(1, 0, 2)\n",
    "            Ptest_tensor = Ptest_tensor.permute(1, 0, 2)\n",
    "\n",
    "            Ptrain_time_tensor = Ptrain_time_tensor.squeeze(2).permute(1, 0)\n",
    "            Pval_time_tensor = Pval_time_tensor.squeeze(2).permute(1, 0)\n",
    "            Ptest_time_tensor = Ptest_time_tensor.squeeze(2).permute(1, 0)\n",
    "\n",
    "            for m in range(n_runs):\n",
    "                print('- - Run %d - -' % (m + 1))\n",
    "                if dataset == 'P12' or dataset == 'P19' or dataset == 'eICU':\n",
    "                    model = SEFT(d_inp, d_model, nhead, nhid, nlayers, dropout, max_len, d_static, MAX, 0.5, aggreg, n_classes, static=False)\n",
    "                elif dataset == 'PAM':\n",
    "                    model = SEFT(d_inp, d_model, nhead, nhid, nlayers, dropout, max_len, d_static, MAX, 0.5, aggreg, n_classes, static=False)\n",
    "\n",
    "                model = model.cuda()\n",
    "\n",
    "                criterion = torch.nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "                scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.1,\n",
    "                                                                           patience=1, threshold=0.001, threshold_mode='rel',\n",
    "                                                                           cooldown=0, min_lr=1e-8, eps=1e-08, verbose=True)\n",
    "\n",
    "                idx_0 = np.where(ytrain == 0)[0]\n",
    "                idx_1 = np.where(ytrain == 1)[0]\n",
    "\n",
    "                if dataset == 'P12' or dataset == 'P19' or dataset == 'eICU':\n",
    "                    strategy = 2\n",
    "                elif dataset == 'PAM':\n",
    "                    strategy = 3\n",
    "\n",
    "                n0, n1 = len(idx_0), len(idx_1)\n",
    "                expanded_idx_1 = np.concatenate([idx_1, idx_1, idx_1], axis=0)\n",
    "                expanded_n1 = len(expanded_idx_1)\n",
    "\n",
    "                batch_size = 128\n",
    "                if strategy == 1:\n",
    "                    n_batches = 10  # number of batches to process per epoch\n",
    "                elif strategy == 2:\n",
    "                    K0 = n0 // int(batch_size / 2)\n",
    "                    K1 = expanded_n1 // int(batch_size / 2)\n",
    "                    n_batches = np.min([K0, K1])\n",
    "                elif strategy == 3:\n",
    "                    n_batches = 30\n",
    "\n",
    "                best_aupr_val = best_auc_val = 0.0\n",
    "                print('Stop epochs: %d, Batches/epoch: %d, Total batches: %d' % (num_epochs, n_batches, num_epochs * n_batches))\n",
    "\n",
    "                start = time.time()\n",
    "                if wandb:\n",
    "                    wandb.watch(model)\n",
    "                for epoch in range(num_epochs):\n",
    "                    model.train()\n",
    "\n",
    "                    if strategy == 2:\n",
    "                        np.random.shuffle(expanded_idx_1)\n",
    "                        I1 = expanded_idx_1\n",
    "                        np.random.shuffle(idx_0)\n",
    "                        I0 = idx_0\n",
    "\n",
    "                    for n in range(n_batches):\n",
    "                        if strategy == 1:\n",
    "                            idx = random_sample(idx_0, idx_1, batch_size)\n",
    "                        elif strategy == 2:\n",
    "                            \"\"\"In each batch=128, 64 positive samples, 64 negative samples\"\"\"\n",
    "                            idx0_batch = I0[n * int(batch_size / 2):(n + 1) * int(batch_size / 2)]\n",
    "                            idx1_batch = I1[n * int(batch_size / 2):(n + 1) * int(batch_size / 2)]\n",
    "                            idx = np.concatenate([idx0_batch, idx1_batch], axis=0)\n",
    "                        elif strategy == 3:\n",
    "                            idx = np.random.choice(list(range(Ptrain_tensor.shape[1])), size=int(batch_size), replace=False)\n",
    "                            # idx = random_sample_8(ytrain, batch_size)   # to balance dataset\n",
    "\n",
    "                        if dataset == 'P12' or dataset == 'P19' or dataset == 'eICU':\n",
    "                            P, Ptime, Pstatic, y = Ptrain_tensor[:, idx, :].cuda(), Ptrain_time_tensor[:, idx].cuda(), \\\n",
    "                                                   Ptrain_static_tensor[idx].cuda(), ytrain_tensor[idx].cuda()\n",
    "                        elif dataset == 'PAM':\n",
    "                            P, Ptime, Pstatic, y = Ptrain_tensor[:, idx, :].cuda(), Ptrain_time_tensor[:, idx].cuda(), \\\n",
    "                                                   None, ytrain_tensor[idx].cuda()\n",
    "\n",
    "                        lengths = torch.sum(Ptime > 0, dim=0)\n",
    "\n",
    "                        outputs = evaluate_standard(model, P, Ptime, None, static=static_info)\n",
    "\n",
    "                        optimizer.zero_grad()\n",
    "                        loss = criterion(outputs, y)\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                    if dataset == 'P12' or dataset == 'P19' or dataset == 'eICU':\n",
    "                        train_probs = torch.squeeze(torch.sigmoid(outputs))\n",
    "                        train_probs = train_probs.cpu().detach().numpy()\n",
    "                        train_probs = np.nan_to_num(train_probs)\n",
    "\n",
    "                        train_y = y.cpu().detach().numpy()\n",
    "                        train_auroc = roc_auc_score(train_y, train_probs[:, 1])\n",
    "                        train_auprc = average_precision_score(train_y, train_probs[:, 1])\n",
    "                    elif dataset == 'PAM':\n",
    "                        train_probs = torch.squeeze(nn.functional.softmax(outputs, dim=1))\n",
    "                        train_probs = train_probs.cpu().detach().numpy()\n",
    "                        train_probs = np.nan_to_num(train_probs)\n",
    "                        train_y = y.cpu().detach().numpy()\n",
    "                        train_auroc = roc_auc_score(one_hot(train_y), train_probs)\n",
    "                        train_auprc = average_precision_score(one_hot(train_y), train_probs)\n",
    "\n",
    "                    if wandb:\n",
    "                        wandb.log({ \"train_loss\": loss.item(), \"train_auprc\": train_auprc, \"train_auroc\": train_auroc})\n",
    "\n",
    "                    \"\"\"Validation\"\"\"\n",
    "                    model.eval()\n",
    "                    if epoch == 0 or epoch % 1 == 0:\n",
    "                        with torch.no_grad():\n",
    "                            out_val = evaluate_standard(model, Pval_tensor, Pval_time_tensor, Pval_static_tensor, static=static_info)\n",
    "                            out_val = torch.squeeze(torch.sigmoid(out_val))\n",
    "                            out_val = out_val.detach().cpu().numpy()\n",
    "                            out_val = np.nan_to_num(out_val)\n",
    "\n",
    "                            val_loss = criterion(torch.from_numpy(out_val), torch.from_numpy(yval.squeeze(1)).long())\n",
    "\n",
    "                            if dataset == 'P12' or dataset == 'P19' or dataset == 'eICU':\n",
    "                                auc_val = roc_auc_score(yval, out_val[:, 1])\n",
    "                                aupr_val = average_precision_score(yval, out_val[:, 1])\n",
    "                            elif dataset == 'PAM':\n",
    "                                auc_val = roc_auc_score(one_hot(yval), out_val)\n",
    "                                aupr_val = average_precision_score(one_hot(yval), out_val)\n",
    "\n",
    "                            print(\"Validation: Epoch %d,  val_loss:%.4f, aupr_val: %.2f, auc_val: %.2f\" % (epoch,\n",
    "                              val_loss.item(), aupr_val * 100, auc_val * 100))\n",
    "\n",
    "                            if wandb:\n",
    "                                wandb.log({\"val_loss\": val_loss.item(), \"val_auprc\": aupr_val, \"val_auroc\": auc_val})\n",
    "\n",
    "                            scheduler.step(aupr_val)\n",
    "                            if aupr_val > best_aupr_val:\n",
    "                                best_aupr_val = aupr_val\n",
    "                                print(\"**[S] Epoch %d, aupr_val: %.4f, auc_val: %.4f **\" % (epoch, aupr_val * 100, auc_val * 100))\n",
    "                                torch.save(model.state_dict(), model_path + arch + '_' + str(split_idx) + '.pt')\n",
    "\n",
    "                end = time.time()\n",
    "                time_elapsed = end - start\n",
    "                print('Total Time elapsed: %.3f mins' % (time_elapsed / 60.0))\n",
    "\n",
    "                \"\"\"Testing\"\"\"\n",
    "                model.load_state_dict(torch.load(model_path + arch + '_' + str(split_idx) + '.pt'))\n",
    "                model.eval()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    out_test = evaluate_standard(model, Ptest_tensor, Ptest_time_tensor, Ptest_static_tensor, n_classes=n_classes, static=static_info).detach().cpu().numpy()\n",
    "                    out_test = np.nan_to_num(out_test)\n",
    "                    ypred = np.argmax(out_test, axis=1)\n",
    "                    \n",
    "                    \n",
    "                    # Adding zero interaction students\n",
    "                    ytest = np.append(ytest, zero_ytest, axis=0)\n",
    "                    ypred = np.append(ypred, np.zeros([1, len(zero_ytest)]))\n",
    "                    \n",
    "\n",
    "                    denoms = np.sum(np.exp(out_test), axis=1).reshape((-1, 1))\n",
    "                    probs = np.exp(out_test) / denoms\n",
    "                    \n",
    "                    # Adding zero interaction students\n",
    "                    probs = np.append(probs, np.zeros([len(zero_ytest), 2]), axis=0)\n",
    "                    probs = np.nan_to_num(probs)\n",
    "\n",
    "                    acc = np.sum(ytest.ravel() == ypred.ravel()) / ytest.shape[0]\n",
    "                    bac = balanced_accuracy_score(ytest.ravel(), ypred.ravel())\n",
    "\n",
    "\n",
    "                    auc = roc_auc_score(ytest, probs[:, 1])\n",
    "                    aupr = average_precision_score(ytest, probs[:, 1])\n",
    "\n",
    "                    \n",
    "                    results = pd.DataFrame(columns=['course', 'percentile', 'acc', 'bac', 'auc', 'auprc'])\n",
    "                    results.loc[0] = [MOOC, percentile, acc, bac, auc, aupr]\n",
    "                    results.to_csv(f\"../../seft_results/test_{MOOC}_{percentile}.csv\")\n",
    "                    print(results)\n",
    "                    \n",
    "                    #VALIDATION\n",
    "                    \n",
    "                    out_val = evaluate_standard(model, Pval_tensor, Pval_time_tensor, Pval_static_tensor, n_classes=n_classes, static=static_info).detach().cpu().numpy()\n",
    "                    out_val = np.nan_to_num(out_val)\n",
    "                    ypred = np.argmax(out_val, axis=1)\n",
    "                    \n",
    "                    \n",
    "                    # Adding zero interaction students\n",
    "                    yval = np.append(yval, zero_yval, axis=0)\n",
    "                    ypred = np.append(ypred, np.zeros([1, len(zero_yval)]))\n",
    "                    \n",
    "\n",
    "                    denoms = np.sum(np.exp(out_val), axis=1).reshape((-1, 1))\n",
    "                    probs = np.exp(out_val) / denoms\n",
    "                    probs = np.nan_to_num(probs)\n",
    "                    \n",
    "                    # Adding zero interaction students\n",
    "                    probs = np.append(probs, np.zeros([len(zero_yval), 2]), axis=0)\n",
    "\n",
    "                    acc = np.sum(yval.ravel() == ypred.ravel()) / yval.shape[0]\n",
    "                    bac = balanced_accuracy_score(yval.ravel(), ypred.ravel())\n",
    "\n",
    "\n",
    "                    auc = roc_auc_score(yval, probs[:, 1])\n",
    "                    aupr = average_precision_score(yval, probs[:, 1])\n",
    "\n",
    "                    \n",
    "                    results_val = pd.DataFrame(columns=['course', 'percentile', 'acc', 'bac', 'auc', 'auprc'])\n",
    "                    results_val.loc[0] = [MOOC, percentile, acc, bac, auc, aupr]\n",
    "                    results_val.to_csv(f\"../../seft_results/val_{MOOC}_{percentile}.csv\")\n",
    "                    print(results_val)\n",
    "\n",
    "                    \n",
    "\n",
    "\n",
    "\n",
    "        # save in numpy file\n",
    "        # np.save('./results/' + arch + '_phy12_setfunction.npy', [acc_vec, auprc_vec, auroc_vec])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e305ae9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341f5737",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5896511d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b98253a",
   "metadata": {},
   "source": [
    "# TRANSFORMER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fc11f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "MOOCs_list = [\n",
    "'villesafricaines_002.csv',\n",
    " 'villesafricaines_003.csv',\n",
    " 'microcontroleurs_004.csv',\n",
    " 'dsp_004.csv'\n",
    "]\n",
    "MOOCs_list = [i.replace(\"_\", \"-\").split('.')[0] for i in MOOCs_list]\n",
    "\n",
    "dims4 = [\n",
    "12,\n",
    " 12,\n",
    " 13,\n",
    " 12\n",
    "]\n",
    "\n",
    "dims6 = [\n",
    "12,\n",
    " 12,\n",
    " 13,\n",
    " 12,\n",
    "]\n",
    "dims = {40: dims4, 60: dims6}\n",
    "data_path = '/../data'\n",
    "percentile = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03ccc6cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split id: 1\n",
      "1966 234 247 1966 234 247\n",
      "[176] [21] [24]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1966, 1000, 24]) torch.Size([1966, 9]) torch.Size([1966, 1000, 1]) torch.Size([1966])\n",
      "- - Run 1 - -\n",
      "Validation: Epoch 0,  val_loss:0.6890, aupr_val: 23.22, auc_val: 66.87\n",
      "**[S] Epoch 0, aupr_val: 23.2160, auc_val: 66.8679 **\n",
      "Validation: Epoch 1,  val_loss:0.6868, aupr_val: 29.62, auc_val: 74.87\n",
      "**[S] Epoch 1, aupr_val: 29.6200, auc_val: 74.8715 **\n",
      "Validation: Epoch 2,  val_loss:0.6898, aupr_val: 29.69, auc_val: 75.68\n",
      "**[S] Epoch 2, aupr_val: 29.6866, auc_val: 75.6763 **\n",
      "Validation: Epoch 3,  val_loss:0.6916, aupr_val: 33.24, auc_val: 79.50\n",
      "**[S] Epoch 3, aupr_val: 33.2375, auc_val: 79.4992 **\n",
      "Validation: Epoch 4,  val_loss:0.6817, aupr_val: 33.37, auc_val: 80.21\n",
      "**[S] Epoch 4, aupr_val: 33.3670, auc_val: 80.2146 **\n",
      "Validation: Epoch 5,  val_loss:0.6695, aupr_val: 37.46, auc_val: 82.88\n",
      "**[S] Epoch 5, aupr_val: 37.4617, auc_val: 82.8750 **\n",
      "Validation: Epoch 6,  val_loss:0.6573, aupr_val: 37.46, auc_val: 82.88\n",
      "Validation: Epoch 7,  val_loss:0.6734, aupr_val: 37.46, auc_val: 82.88\n",
      "Epoch 00008: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Validation: Epoch 8,  val_loss:0.6658, aupr_val: 37.46, auc_val: 82.88\n",
      "Validation: Epoch 9,  val_loss:0.6522, aupr_val: 37.46, auc_val: 82.88\n",
      "Epoch 00010: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Validation: Epoch 10,  val_loss:0.6514, aupr_val: 37.46, auc_val: 82.88\n",
      "Validation: Epoch 11,  val_loss:0.6514, aupr_val: 37.57, auc_val: 82.90\n",
      "**[S] Epoch 11, aupr_val: 37.5707, auc_val: 82.8974 **\n",
      "Validation: Epoch 12,  val_loss:0.6518, aupr_val: 37.46, auc_val: 82.88\n",
      "Validation: Epoch 13,  val_loss:0.6522, aupr_val: 37.46, auc_val: 82.88\n",
      "Epoch 00014: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Validation: Epoch 14,  val_loss:0.6522, aupr_val: 37.46, auc_val: 82.88\n",
      "Validation: Epoch 15,  val_loss:0.6522, aupr_val: 37.46, auc_val: 82.88\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Validation: Epoch 16,  val_loss:0.6523, aupr_val: 37.46, auc_val: 82.88\n",
      "Validation: Epoch 17,  val_loss:0.6523, aupr_val: 37.46, auc_val: 82.88\n",
      "Epoch 00018: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Validation: Epoch 18,  val_loss:0.6523, aupr_val: 37.46, auc_val: 82.88\n",
      "Validation: Epoch 19,  val_loss:0.6523, aupr_val: 37.46, auc_val: 82.88\n",
      "Total Time elapsed: 0.518 mins\n",
      "                 course  percentile       acc      bac       auc    auprc\n",
      "0  villesafricaines-002          40  0.923333  0.59692  0.418856  0.14582\n",
      "                 course  percentile   acc       bac       auc     auprc\n",
      "0  villesafricaines-002          40  0.93  0.563412  0.468215  0.191176\n",
      "Split id: 1\n",
      "2125 263 268 2125 263 268\n",
      "[186] [23] [24]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2125, 1000, 24]) torch.Size([2125, 9]) torch.Size([2125, 1000, 1]) torch.Size([2125])\n",
      "- - Run 1 - -\n",
      "Validation: Epoch 0,  val_loss:0.6827, aupr_val: 38.26, auc_val: 75.47\n",
      "**[S] Epoch 0, aupr_val: 38.2576, auc_val: 75.4710 **\n",
      "Validation: Epoch 1,  val_loss:0.6816, aupr_val: 47.84, auc_val: 82.46\n",
      "**[S] Epoch 1, aupr_val: 47.8404, auc_val: 82.4638 **\n",
      "Validation: Epoch 2,  val_loss:0.6785, aupr_val: 47.14, auc_val: 82.21\n",
      "Validation: Epoch 3,  val_loss:0.6730, aupr_val: 46.50, auc_val: 82.14\n",
      "Epoch 00004: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Validation: Epoch 4,  val_loss:0.6725, aupr_val: 45.86, auc_val: 81.88\n",
      "Validation: Epoch 5,  val_loss:0.6712, aupr_val: 45.86, auc_val: 81.88\n",
      "Epoch 00006: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Validation: Epoch 6,  val_loss:0.6711, aupr_val: 45.86, auc_val: 81.88\n",
      "Validation: Epoch 7,  val_loss:0.6710, aupr_val: 45.86, auc_val: 81.88\n",
      "Epoch 00008: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Validation: Epoch 8,  val_loss:0.6710, aupr_val: 45.86, auc_val: 81.88\n",
      "Validation: Epoch 9,  val_loss:0.6710, aupr_val: 45.86, auc_val: 81.88\n",
      "Epoch 00010: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Validation: Epoch 10,  val_loss:0.6710, aupr_val: 45.86, auc_val: 81.88\n",
      "Validation: Epoch 11,  val_loss:0.6710, aupr_val: 45.86, auc_val: 81.88\n",
      "Epoch 00012: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Validation: Epoch 12,  val_loss:0.6710, aupr_val: 45.86, auc_val: 81.88\n",
      "Validation: Epoch 13,  val_loss:0.6710, aupr_val: 45.86, auc_val: 81.88\n",
      "Validation: Epoch 14,  val_loss:0.6710, aupr_val: 45.86, auc_val: 81.88\n",
      "Validation: Epoch 15,  val_loss:0.6710, aupr_val: 45.86, auc_val: 81.88\n",
      "Validation: Epoch 16,  val_loss:0.6710, aupr_val: 45.86, auc_val: 81.88\n",
      "Validation: Epoch 17,  val_loss:0.6710, aupr_val: 45.86, auc_val: 81.88\n",
      "Validation: Epoch 18,  val_loss:0.6710, aupr_val: 45.86, auc_val: 81.88\n",
      "Validation: Epoch 19,  val_loss:0.6710, aupr_val: 45.86, auc_val: 81.88\n",
      "Total Time elapsed: 0.557 mins\n",
      "                 course  percentile       acc       bac       auc     auprc\n",
      "0  villesafricaines-002          60  0.923333  0.615942  0.415157  0.219742\n",
      "                 course  percentile   acc      bac       auc     auprc\n",
      "0  villesafricaines-002          60  0.94  0.62863  0.505886  0.309782\n",
      "Split id: 1\n",
      "1431 174 180 1431 174 180\n",
      "[180] [22] [23]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1431, 1000, 24]) torch.Size([1431, 9]) torch.Size([1431, 1000, 1]) torch.Size([1431])\n",
      "- - Run 1 - -\n",
      "Validation: Epoch 0,  val_loss:0.6946, aupr_val: 21.13, auc_val: 61.53\n",
      "**[S] Epoch 0, aupr_val: 21.1301, auc_val: 61.5281 **\n",
      "Validation: Epoch 1,  val_loss:0.6886, aupr_val: 33.65, auc_val: 75.61\n",
      "**[S] Epoch 1, aupr_val: 33.6468, auc_val: 75.6130 **\n",
      "Validation: Epoch 2,  val_loss:0.6913, aupr_val: 35.99, auc_val: 79.29\n",
      "**[S] Epoch 2, aupr_val: 35.9921, auc_val: 79.2913 **\n",
      "Validation: Epoch 3,  val_loss:0.6879, aupr_val: 35.34, auc_val: 78.78\n",
      "Validation: Epoch 4,  val_loss:0.6864, aupr_val: 36.00, auc_val: 78.30\n",
      "Validation: Epoch 5,  val_loss:0.6757, aupr_val: 26.87, auc_val: 44.00\n",
      "Validation: Epoch 6,  val_loss:0.6695, aupr_val: 26.62, auc_val: 43.94\n",
      "Epoch 00007: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Validation: Epoch 7,  val_loss:0.6720, aupr_val: 26.62, auc_val: 43.94\n",
      "Validation: Epoch 8,  val_loss:0.6788, aupr_val: 29.12, auc_val: 59.79\n",
      "Epoch 00009: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Validation: Epoch 9,  val_loss:0.6785, aupr_val: 28.36, auc_val: 56.50\n",
      "Validation: Epoch 10,  val_loss:0.6779, aupr_val: 28.43, auc_val: 55.16\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Validation: Epoch 11,  val_loss:0.6778, aupr_val: 28.94, auc_val: 55.76\n",
      "Validation: Epoch 12,  val_loss:0.6778, aupr_val: 29.23, auc_val: 55.92\n",
      "Epoch 00013: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Validation: Epoch 13,  val_loss:0.6778, aupr_val: 29.23, auc_val: 55.94\n",
      "Validation: Epoch 14,  val_loss:0.6778, aupr_val: 29.23, auc_val: 55.91\n",
      "Epoch 00015: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Validation: Epoch 15,  val_loss:0.6778, aupr_val: 29.23, auc_val: 55.92\n",
      "Validation: Epoch 16,  val_loss:0.6778, aupr_val: 29.23, auc_val: 55.94\n",
      "Validation: Epoch 17,  val_loss:0.6778, aupr_val: 29.23, auc_val: 55.94\n",
      "Validation: Epoch 18,  val_loss:0.6778, aupr_val: 29.23, auc_val: 55.91\n",
      "Validation: Epoch 19,  val_loss:0.6778, aupr_val: 29.23, auc_val: 55.91\n",
      "Total Time elapsed: 0.567 mins\n",
      "                 course  percentile       acc       bac       auc     auprc\n",
      "0  villesafricaines-003          40  0.893519  0.653188  0.910903  0.494994\n",
      "                 course  percentile       acc       bac       auc     auprc\n",
      "0  villesafricaines-003          40  0.888372  0.615638  0.828427  0.354032\n",
      "Split id: 1\n",
      "1532 187 194 1532 187 194\n",
      "[181] [22] [23]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1532, 1000, 24]) torch.Size([1532, 9]) torch.Size([1532, 1000, 1]) torch.Size([1532])\n",
      "- - Run 1 - -\n",
      "Validation: Epoch 0,  val_loss:0.6902, aupr_val: 21.89, auc_val: 38.79\n",
      "**[S] Epoch 0, aupr_val: 21.8877, auc_val: 38.7879 **\n",
      "Validation: Epoch 1,  val_loss:0.6628, aupr_val: 22.69, auc_val: 38.84\n",
      "**[S] Epoch 1, aupr_val: 22.6940, auc_val: 38.8430 **\n",
      "Validation: Epoch 2,  val_loss:0.6684, aupr_val: 25.72, auc_val: 38.90\n",
      "**[S] Epoch 2, aupr_val: 25.7243, auc_val: 38.8981 **\n",
      "Validation: Epoch 3,  val_loss:0.6566, aupr_val: 25.29, auc_val: 38.87\n",
      "Validation: Epoch 4,  val_loss:0.6542, aupr_val: 25.29, auc_val: 38.87\n",
      "Epoch 00005: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Validation: Epoch 5,  val_loss:0.6526, aupr_val: 25.29, auc_val: 38.87\n",
      "Validation: Epoch 6,  val_loss:0.6519, aupr_val: 25.29, auc_val: 38.87\n",
      "Epoch 00007: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Validation: Epoch 7,  val_loss:0.6518, aupr_val: 25.29, auc_val: 38.87\n",
      "Validation: Epoch 8,  val_loss:0.6516, aupr_val: 25.29, auc_val: 38.87\n",
      "Epoch 00009: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Validation: Epoch 9,  val_loss:0.6516, aupr_val: 25.29, auc_val: 38.87\n",
      "Validation: Epoch 10,  val_loss:0.6516, aupr_val: 25.29, auc_val: 38.87\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Validation: Epoch 11,  val_loss:0.6516, aupr_val: 25.29, auc_val: 38.87\n",
      "Validation: Epoch 12,  val_loss:0.6516, aupr_val: 25.29, auc_val: 38.87\n",
      "Epoch 00013: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Validation: Epoch 13,  val_loss:0.6516, aupr_val: 25.29, auc_val: 38.87\n",
      "Validation: Epoch 14,  val_loss:0.6516, aupr_val: 25.29, auc_val: 38.87\n",
      "Validation: Epoch 15,  val_loss:0.6516, aupr_val: 25.29, auc_val: 38.87\n",
      "Validation: Epoch 16,  val_loss:0.6516, aupr_val: 25.29, auc_val: 38.87\n",
      "Validation: Epoch 17,  val_loss:0.6516, aupr_val: 25.29, auc_val: 38.87\n",
      "Validation: Epoch 18,  val_loss:0.6516, aupr_val: 25.29, auc_val: 38.87\n",
      "Validation: Epoch 19,  val_loss:0.6516, aupr_val: 25.29, auc_val: 38.87\n",
      "Total Time elapsed: 0.585 mins\n",
      "                 course  percentile       acc       bac      auc     auprc\n",
      "0  villesafricaines-003          60  0.902778  0.696666  0.55891  0.310336\n",
      "                 course  percentile       acc       bac       auc     auprc\n",
      "0  villesafricaines-003          60  0.893023  0.618229  0.477862  0.283091\n",
      "Split id: 1\n",
      "1740 215 219 1740 215 219\n",
      "[181] [23] [22]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1740, 1000, 26]) torch.Size([1740, 9]) torch.Size([1740, 1000, 1]) torch.Size([1740])\n",
      "- - Run 1 - -\n",
      "Validation: Epoch 0,  val_loss:0.6939, aupr_val: 6.38, auc_val: 16.41\n",
      "**[S] Epoch 0, aupr_val: 6.3807, auc_val: 16.4062 **\n",
      "Validation: Epoch 1,  val_loss:0.6936, aupr_val: 6.38, auc_val: 16.41\n",
      "Validation: Epoch 2,  val_loss:0.6897, aupr_val: 6.38, auc_val: 16.41\n",
      "Epoch 00003: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Validation: Epoch 3,  val_loss:0.6902, aupr_val: 6.38, auc_val: 16.41\n",
      "Validation: Epoch 4,  val_loss:0.6904, aupr_val: 6.38, auc_val: 16.41\n",
      "Epoch 00005: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Validation: Epoch 5,  val_loss:0.6903, aupr_val: 6.38, auc_val: 16.41\n",
      "Validation: Epoch 6,  val_loss:0.6904, aupr_val: 6.38, auc_val: 16.41\n",
      "Epoch 00007: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Validation: Epoch 7,  val_loss:0.6904, aupr_val: 6.38, auc_val: 16.41\n",
      "Validation: Epoch 8,  val_loss:0.6904, aupr_val: 6.38, auc_val: 16.41\n",
      "Epoch 00009: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Validation: Epoch 9,  val_loss:0.6904, aupr_val: 6.38, auc_val: 16.41\n",
      "Validation: Epoch 10,  val_loss:0.6904, aupr_val: 6.38, auc_val: 16.41\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Validation: Epoch 11,  val_loss:0.6904, aupr_val: 6.38, auc_val: 16.41\n",
      "Validation: Epoch 12,  val_loss:0.6904, aupr_val: 6.38, auc_val: 16.41\n",
      "Validation: Epoch 13,  val_loss:0.6904, aupr_val: 6.38, auc_val: 16.41\n",
      "Validation: Epoch 14,  val_loss:0.6904, aupr_val: 6.38, auc_val: 16.41\n",
      "Validation: Epoch 15,  val_loss:0.6904, aupr_val: 6.38, auc_val: 16.41\n",
      "Validation: Epoch 16,  val_loss:0.6904, aupr_val: 6.38, auc_val: 16.41\n",
      "Validation: Epoch 17,  val_loss:0.6904, aupr_val: 6.38, auc_val: 16.41\n",
      "Validation: Epoch 18,  val_loss:0.6904, aupr_val: 6.38, auc_val: 16.41\n",
      "Validation: Epoch 19,  val_loss:0.6904, aupr_val: 6.38, auc_val: 16.41\n",
      "Total Time elapsed: 0.667 mins\n",
      "                 course  percentile       acc       bac       auc     auprc\n",
      "0  microcontroleurs-004          40  0.611307  0.372324  0.389548  0.110775\n",
      "                 course  percentile       acc       bac       auc     auprc\n",
      "0  microcontroleurs-004          40  0.646643  0.371739  0.382692  0.063807\n",
      "Split id: 1\n",
      "1965 246 243 1965 246 243\n",
      "[186] [23] [23]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1965, 1000, 26]) torch.Size([1965, 9]) torch.Size([1965, 1000, 1]) torch.Size([1965])\n",
      "- - Run 1 - -\n",
      "Validation: Epoch 0,  val_loss:0.6861, aupr_val: 38.37, auc_val: 78.67\n",
      "**[S] Epoch 0, aupr_val: 38.3711, auc_val: 78.6703 **\n",
      "Validation: Epoch 1,  val_loss:0.6807, aupr_val: 23.91, auc_val: 44.89\n",
      "Validation: Epoch 2,  val_loss:0.6687, aupr_val: 22.93, auc_val: 44.50\n",
      "Epoch 00003: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Validation: Epoch 3,  val_loss:0.6697, aupr_val: 24.25, auc_val: 44.76\n",
      "Validation: Epoch 4,  val_loss:0.6719, aupr_val: 24.60, auc_val: 44.93\n",
      "Epoch 00005: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Validation: Epoch 5,  val_loss:0.6718, aupr_val: 24.60, auc_val: 44.93\n",
      "Validation: Epoch 6,  val_loss:0.6717, aupr_val: 24.60, auc_val: 44.93\n",
      "Epoch 00007: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Validation: Epoch 7,  val_loss:0.6716, aupr_val: 24.60, auc_val: 44.93\n",
      "Validation: Epoch 8,  val_loss:0.6716, aupr_val: 24.60, auc_val: 44.93\n",
      "Epoch 00009: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Validation: Epoch 9,  val_loss:0.6716, aupr_val: 24.60, auc_val: 44.93\n",
      "Validation: Epoch 10,  val_loss:0.6716, aupr_val: 24.60, auc_val: 44.93\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Validation: Epoch 11,  val_loss:0.6716, aupr_val: 24.60, auc_val: 44.93\n",
      "Validation: Epoch 12,  val_loss:0.6716, aupr_val: 24.60, auc_val: 44.93\n",
      "Validation: Epoch 13,  val_loss:0.6716, aupr_val: 24.60, auc_val: 44.93\n",
      "Validation: Epoch 14,  val_loss:0.6716, aupr_val: 24.60, auc_val: 44.93\n",
      "Validation: Epoch 15,  val_loss:0.6716, aupr_val: 24.60, auc_val: 44.93\n",
      "Validation: Epoch 16,  val_loss:0.6716, aupr_val: 24.60, auc_val: 44.93\n",
      "Validation: Epoch 17,  val_loss:0.6716, aupr_val: 24.60, auc_val: 44.93\n",
      "Validation: Epoch 18,  val_loss:0.6716, aupr_val: 24.60, auc_val: 44.93\n",
      "Validation: Epoch 19,  val_loss:0.6716, aupr_val: 24.60, auc_val: 44.93\n",
      "Total Time elapsed: 0.522 mins\n",
      "                 course  percentile      acc       bac       auc    auprc\n",
      "0  microcontroleurs-004          60  0.90106  0.549833  0.458863  0.14709\n",
      "                 course  percentile       acc       bac       auc   auprc\n",
      "0  microcontroleurs-004          60  0.918728  0.559448  0.529849  0.2635\n",
      "Split id: 1\n",
      "1317 164 164 1317 164 164\n",
      "[226] [28] [29]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1317, 1000, 24]) torch.Size([1317, 9]) torch.Size([1317, 1000, 1]) torch.Size([1317])\n",
      "- - Run 1 - -\n",
      "Validation: Epoch 0,  val_loss:0.6716, aupr_val: 18.60, auc_val: 46.61\n",
      "**[S] Epoch 0, aupr_val: 18.6005, auc_val: 46.6124 **\n",
      "Validation: Epoch 1,  val_loss:0.6669, aupr_val: 21.19, auc_val: 48.98\n",
      "**[S] Epoch 1, aupr_val: 21.1854, auc_val: 48.9758 **\n",
      "Validation: Epoch 2,  val_loss:0.6663, aupr_val: 20.84, auc_val: 49.03\n",
      "**[S] Epoch 2, aupr_val: 20.8410, auc_val: 49.0284 **\n",
      "Validation: Epoch 3,  val_loss:0.6603, aupr_val: 21.20, auc_val: 49.00\n",
      "Validation: Epoch 4,  val_loss:0.6578, aupr_val: 22.24, auc_val: 49.08\n",
      "**[S] Epoch 4, aupr_val: 22.2402, auc_val: 49.0809 **\n",
      "Validation: Epoch 5,  val_loss:0.6600, aupr_val: 24.71, auc_val: 49.29\n",
      "**[S] Epoch 5, aupr_val: 24.7139, auc_val: 49.2910 **\n",
      "Validation: Epoch 6,  val_loss:0.6639, aupr_val: 23.44, auc_val: 49.21\n",
      "Validation: Epoch 7,  val_loss:0.6579, aupr_val: 24.08, auc_val: 49.29\n",
      "Epoch 00008: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Validation: Epoch 8,  val_loss:0.6584, aupr_val: 24.08, auc_val: 49.29\n",
      "Validation: Epoch 9,  val_loss:0.6599, aupr_val: 24.09, auc_val: 49.32\n",
      "Epoch 00010: reducing learning rate of group 0 to 1.0000e-05.\n",
      "**[S] Epoch 9, aupr_val: 24.0883, auc_val: 49.3172 **\n",
      "Validation: Epoch 10,  val_loss:0.6600, aupr_val: 24.09, auc_val: 49.32\n",
      "Validation: Epoch 11,  val_loss:0.6602, aupr_val: 24.09, auc_val: 49.32\n",
      "Epoch 00012: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Validation: Epoch 12,  val_loss:0.6602, aupr_val: 24.09, auc_val: 49.32\n",
      "Validation: Epoch 13,  val_loss:0.6602, aupr_val: 24.09, auc_val: 49.32\n",
      "Epoch 00014: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Validation: Epoch 14,  val_loss:0.6602, aupr_val: 24.09, auc_val: 49.32\n",
      "Validation: Epoch 15,  val_loss:0.6602, aupr_val: 24.09, auc_val: 49.32\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Validation: Epoch 16,  val_loss:0.6602, aupr_val: 24.09, auc_val: 49.32\n",
      "Validation: Epoch 17,  val_loss:0.6602, aupr_val: 24.09, auc_val: 49.32\n",
      "Validation: Epoch 18,  val_loss:0.6602, aupr_val: 24.09, auc_val: 49.32\n",
      "Validation: Epoch 19,  val_loss:0.6602, aupr_val: 24.09, auc_val: 49.32\n",
      "Total Time elapsed: 0.878 mins\n",
      "    course  percentile       acc       bac       auc     auprc\n",
      "0  dsp-004          40  0.804598  0.675862  0.702854  0.448824\n",
      "    course  percentile       acc       bac       auc     auprc\n",
      "0  dsp-004          40  0.774566  0.534113  0.525616  0.256761\n",
      "Split id: 1\n",
      "1350 167 166 1350 167 166\n",
      "[226] [28] [29]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "/opt/conda/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1350, 1000, 24]) torch.Size([1350, 9]) torch.Size([1350, 1000, 1]) torch.Size([1350])\n",
      "- - Run 1 - -\n",
      "Validation: Epoch 0,  val_loss:0.6468, aupr_val: 30.01, auc_val: 53.92\n",
      "**[S] Epoch 0, aupr_val: 30.0056, auc_val: 53.9183 **\n",
      "Validation: Epoch 1,  val_loss:0.6457, aupr_val: 30.54, auc_val: 54.02\n",
      "**[S] Epoch 1, aupr_val: 30.5386, auc_val: 54.0211 **\n",
      "Validation: Epoch 2,  val_loss:0.6304, aupr_val: 31.04, auc_val: 54.20\n",
      "**[S] Epoch 2, aupr_val: 31.0360, auc_val: 54.2009 **\n",
      "Validation: Epoch 3,  val_loss:0.6376, aupr_val: 32.11, auc_val: 54.36\n",
      "**[S] Epoch 3, aupr_val: 32.1106, auc_val: 54.3551 **\n",
      "Validation: Epoch 4,  val_loss:0.6355, aupr_val: 34.59, auc_val: 54.46\n",
      "**[S] Epoch 4, aupr_val: 34.5872, auc_val: 54.4579 **\n",
      "Validation: Epoch 5,  val_loss:0.6344, aupr_val: 34.95, auc_val: 54.48\n",
      "**[S] Epoch 5, aupr_val: 34.9468, auc_val: 54.4836 **\n",
      "Validation: Epoch 6,  val_loss:0.6401, aupr_val: 35.69, auc_val: 54.48\n",
      "Validation: Epoch 7,  val_loss:0.6397, aupr_val: 39.00, auc_val: 54.74\n",
      "**[S] Epoch 7, aupr_val: 39.0050, auc_val: 54.7405 **\n",
      "Validation: Epoch 8,  val_loss:0.6284, aupr_val: 39.66, auc_val: 54.79\n",
      "**[S] Epoch 8, aupr_val: 39.6620, auc_val: 54.7919 **\n",
      "Validation: Epoch 9,  val_loss:0.6385, aupr_val: 40.61, auc_val: 54.97\n",
      "**[S] Epoch 9, aupr_val: 40.6131, auc_val: 54.9717 **\n",
      "Validation: Epoch 10,  val_loss:0.6235, aupr_val: 41.14, auc_val: 55.28\n",
      "**[S] Epoch 10, aupr_val: 41.1389, auc_val: 55.2801 **\n",
      "Validation: Epoch 11,  val_loss:0.6279, aupr_val: 41.78, auc_val: 56.51\n",
      "**[S] Epoch 11, aupr_val: 41.7752, auc_val: 56.5134 **\n",
      "Validation: Epoch 12,  val_loss:0.6232, aupr_val: 42.12, auc_val: 58.88\n",
      "**[S] Epoch 12, aupr_val: 42.1183, auc_val: 58.8772 **\n",
      "Validation: Epoch 13,  val_loss:0.6277, aupr_val: 42.12, auc_val: 60.29\n",
      "**[S] Epoch 13, aupr_val: 42.1208, auc_val: 60.2903 **\n",
      "Validation: Epoch 14,  val_loss:0.6179, aupr_val: 41.58, auc_val: 57.90\n",
      "Epoch 00015: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Validation: Epoch 15,  val_loss:0.6219, aupr_val: 41.65, auc_val: 57.90\n",
      "Validation: Epoch 16,  val_loss:0.6150, aupr_val: 41.57, auc_val: 57.67\n",
      "Epoch 00017: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Validation: Epoch 17,  val_loss:0.6152, aupr_val: 41.57, auc_val: 57.67\n",
      "Validation: Epoch 18,  val_loss:0.6159, aupr_val: 41.57, auc_val: 57.67\n",
      "Epoch 00019: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Validation: Epoch 19,  val_loss:0.6160, aupr_val: 41.57, auc_val: 57.67\n",
      "Total Time elapsed: 0.659 mins\n",
      "    course  percentile       acc       bac       auc     auprc\n",
      "0  dsp-004          60  0.885057  0.765517  0.789417  0.650628\n",
      "    course  percentile      acc      bac       auc     auprc\n",
      "0  dsp-004          60  0.83815  0.62968  0.609236  0.424218\n"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "\n",
    "\n",
    "imputation = 'no_imputation'\n",
    "\n",
    "arch = 'standard'\n",
    "\n",
    "model_path = '../../models/transformer/'\n",
    "\n",
    "dataset = 'P12'\n",
    "\n",
    "\n",
    "base_path = '/../data/prep_data'\n",
    "data_path = '/../data'\n",
    "\n",
    "\n",
    "\n",
    "def one_hot(y_):\n",
    "    y_ = y_.reshape(len(y_))\n",
    "    y_ = [int(x) for x in y_]\n",
    "    n_values = np.max(y_) + 1\n",
    "    return np.eye(n_values)[np.array(y_, dtype=np.int32)]\n",
    "\n",
    "feature_removal_level = 'no_removal'\n",
    "\n",
    "\n",
    "for MOOC_idx, MOOC in enumerate(MOOCs_list):\n",
    "    \n",
    "    for percentile in [40, 60]:\n",
    "        \n",
    "        \n",
    "        d_inp = dims[percentile][MOOC_idx]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        missing_ratio = 0\n",
    "\n",
    "\n",
    "        num_epochs = 20\n",
    "        learning_rate = 0.001\n",
    "\n",
    "\n",
    "        d_static = 9\n",
    "\n",
    "        static_info = 0\n",
    "\n",
    "\n",
    "        d_model = d_inp\n",
    "        nhid = 2 * d_model\n",
    "        nlayers = 1\n",
    "        nhead = 1\n",
    "\n",
    "        dropout = 0.3\n",
    "\n",
    "\n",
    "        max_len = 1000\n",
    "        n_classes = 2\n",
    "\n",
    "\n",
    "        aggreg = 'mean'\n",
    "\n",
    "        MAX = 100\n",
    "\n",
    "        n_runs = 1\n",
    "        n_splits = 1\n",
    "        subset = False\n",
    "\n",
    "        split = 'random'\n",
    "        reverse = False\n",
    "        baseline = True\n",
    "\n",
    "        acc_arr = np.zeros((n_splits, n_runs))\n",
    "        auprc_arr = np.zeros((n_splits, n_runs))\n",
    "        auroc_arr = np.zeros((n_splits, n_runs))\n",
    "        precision_arr = np.zeros((n_splits, n_runs))\n",
    "        recall_arr = np.zeros((n_splits, n_runs))\n",
    "        F1_arr = np.zeros((n_splits, n_runs))\n",
    "\n",
    "        for k in range(n_splits):\n",
    "            split_idx = k + 1\n",
    "            print('Split id: %d' % split_idx)\n",
    "\n",
    "            #if dataset == 'P12':\n",
    "            #    if subset == True:\n",
    "            #        split_path = '/splits/phy12_split_subset' + str(split_idx) + '.npy'\n",
    "            #    else:\n",
    "            #        split_path = '/splits/phy12_split' + str(split_idx) + '.npy'\n",
    "            #elif dataset == 'P19':\n",
    "            #    split_path = '/splits/phy19_split' + str(split_idx) + '_new.npy'\n",
    "            #elif dataset == 'eICU':\n",
    "            #    split_path = '/splits/eICU_split' + str(split_idx) + '.npy'\n",
    "            #elif dataset == 'PAM':\n",
    "            #    split_path = '/splits/PAM_split_' + str(split_idx) + '.npy'\n",
    "\n",
    "\n",
    "            # prepare the data:\n",
    "            #Ptrain, Pval, Ptest, ytrain, yval, ytest = get_data_split(base_path, split_path, split_type=split,\n",
    "            #                                                          reverse=reverse, baseline=baseline, dataset=dataset,\n",
    "            #                                                          predictive_label=args.predictive_label)\n",
    "            Pdict_list = np.load(os.path.join(base_path, f\"{MOOC}_{percentile}_data_hard_fail.npy\"), allow_pickle=True)\n",
    "            arr_outcomes = np.load(os.path.join(base_path, f\"{MOOC}_{percentile}_y_hard_fail.npy\"), allow_pickle=True)\n",
    "\n",
    "            #Ptrain, Ptest, ytrain, ytest = train_test_split(Pdict_list, arr_outcomes, test_size=0.1, random_state=1)\n",
    "            #Ptrain, Pval, ytrain, yval = train_test_split(Ptrain, ytrain, test_size=1/9, random_state=1)\n",
    "            args_train, args_val, args_test = np.load(os.path.join(data_path, \n",
    "                                                                           'split_args', f\"split_{MOOC.replace('-', '_')}.npy\"),\n",
    "                                                             allow_pickle=True)\n",
    "            Ptrain = Pdict_list[args_train]\n",
    "            Pval = Pdict_list[args_val]\n",
    "            Ptest = Pdict_list[args_test]\n",
    "            ytrain = arr_outcomes[args_train, :]\n",
    "            yval = arr_outcomes[args_val, :]\n",
    "            ytest = arr_outcomes[args_test, :]\n",
    "\n",
    "\n",
    "            zero_indices = [i for i, item in enumerate(Ptrain) if item['length'] == 0]\n",
    "            #zero_Ptrain = Ptrain[zero_indices]\n",
    "            Ptrain = np.delete(Ptrain, zero_indices, axis=0)\n",
    "            ytrain = np.delete(ytrain, zero_indices, axis=0)\n",
    "\n",
    "            zero_indices = [i for i, item in enumerate(Pval) if item['length'] == 0]\n",
    "            zero_yval = yval[zero_indices]\n",
    "            Pval = np.delete(Pval, zero_indices, axis=0)\n",
    "            yval = np.delete(yval, zero_indices, axis=0)\n",
    "\n",
    "            zero_indices = [i for i, item in enumerate(Ptest) if item['length'] == 0]\n",
    "            zero_ytest = ytest[zero_indices]\n",
    "            Ptest = np.delete(Ptest, zero_indices, axis=0)  \n",
    "            ytest = np.delete(ytest, zero_indices, axis=0)\n",
    "\n",
    "            print(len(Ptrain), len(Pval), len(Ptest), len(ytrain), len(yval), len(ytest))\n",
    "            print(sum(ytrain), sum(yval), sum(ytest))\n",
    "\n",
    "            # impute missing values\n",
    "            if imputation != 'no_imputation':\n",
    "                if dataset == 'P12' or dataset == 'P19' or dataset == 'eICU':\n",
    "                    X_features_train = np.array([d['arr'] for d in Ptrain])\n",
    "                    X_time_train = np.array([d['time'] for d in Ptrain])\n",
    "                    X_features_val = np.array([d['arr'] for d in Pval])\n",
    "                    X_time_val = np.array([d['time'] for d in Pval])\n",
    "                    X_features_test = np.array([d['arr'] for d in Ptest])\n",
    "                    X_time_test = np.array([d['time'] for d in Ptest])\n",
    "                elif dataset == 'PAM':\n",
    "                    X_features_train = Ptrain\n",
    "                    X_time_train = np.array([np.arange(1, Ptrain.shape[1] + 1)[..., np.newaxis] for d in Ptrain])\n",
    "                    X_features_val = Pval\n",
    "                    X_time_val = np.array([np.arange(1, Pval.shape[1] + 1)[..., np.newaxis] for d in Pval])\n",
    "                    X_features_test = Ptest\n",
    "                    X_time_test = np.array([np.arange(1, Ptest.shape[1] + 1)[..., np.newaxis] for d in Ptest])\n",
    "\n",
    "                if dataset == 'P12' or dataset == 'P19' or dataset == 'PAM':\n",
    "                    missing_value_num = 0\n",
    "                elif dataset == 'eICU':\n",
    "                    missing_value_num = -1\n",
    "\n",
    "                if imputation == 'mean':\n",
    "                    features_means = get_features_mean(X_features_train)\n",
    "                    X_features_train = mean_imputation(X_features_train, X_time_train, features_means, missing_value_num)\n",
    "                    X_features_val = mean_imputation(X_features_val, X_time_val, features_means, missing_value_num)\n",
    "                    X_features_test = mean_imputation(X_features_test, X_time_test, features_means, missing_value_num)\n",
    "                elif imputation == 'forward':\n",
    "                    X_features_train = forward_imputation(X_features_train, X_time_train, missing_value_num)\n",
    "                    X_features_val = forward_imputation(X_features_val, X_time_val, missing_value_num)\n",
    "                    X_features_test = forward_imputation(X_features_test, X_time_test, missing_value_num)\n",
    "                elif imputation == 'cubic_spline':\n",
    "                    X_features_train = cubic_spline_imputation(X_features_train, X_time_train, missing_value_num)\n",
    "                    X_features_val = cubic_spline_imputation(X_features_val, X_time_val, missing_value_num)\n",
    "                    X_features_test = cubic_spline_imputation(X_features_test, X_time_test, missing_value_num)\n",
    "\n",
    "                if dataset == 'P12' or dataset == 'P19' or dataset == 'eICU':\n",
    "                    for i, pat in enumerate(X_features_train):\n",
    "                        Ptrain[i]['arr'] = pat\n",
    "                    for i, pat in enumerate(X_features_val):\n",
    "                        Pval[i]['arr'] = pat\n",
    "                    for i, pat in enumerate(X_features_test):\n",
    "                        Ptest[i]['arr'] = pat\n",
    "                elif dataset == 'PAM':\n",
    "                    for i, pat in enumerate(X_features_train):\n",
    "                        Ptrain[i] = pat\n",
    "                    for i, pat in enumerate(X_features_val):\n",
    "                        Pval[i] = pat\n",
    "                    for i, pat in enumerate(X_features_test):\n",
    "                        Ptest[i] = pat\n",
    "\n",
    "            if dataset == 'P12' or dataset == 'P19' or dataset == 'eICU':\n",
    "                T, F = Ptrain[0]['arr'].shape\n",
    "                D = len(Ptrain[0]['extended_static'])\n",
    "\n",
    "                Ptrain_tensor = np.zeros((len(Ptrain), T, F))\n",
    "                Ptrain_static_tensor = np.zeros((len(Ptrain), D))\n",
    "\n",
    "                for i in range(len(Ptrain)):\n",
    "                    Ptrain_tensor[i] = Ptrain[i]['arr']\n",
    "                    Ptrain_static_tensor[i] = Ptrain[i]['extended_static']\n",
    "\n",
    "                mf, stdf = getStats(Ptrain_tensor)\n",
    "                ms, ss = getStats_static(Ptrain_static_tensor, dataset=dataset)\n",
    "\n",
    "                Ptrain_tensor, Ptrain_static_tensor, Ptrain_time_tensor, ytrain_tensor = tensorize_normalize(Ptrain, ytrain,\n",
    "                                                                                                             mf,\n",
    "                                                                                                             stdf, ms, ss)\n",
    "                Pval_tensor, Pval_static_tensor, Pval_time_tensor, yval_tensor = tensorize_normalize(Pval, yval, mf, stdf,\n",
    "                                                                                                     ms, ss)\n",
    "                Ptest_tensor, Ptest_static_tensor, Ptest_time_tensor, ytest_tensor = tensorize_normalize(Ptest, ytest, mf,\n",
    "                                                                                                         stdf, ms,\n",
    "                                                                                                         ss)\n",
    "\n",
    "                print(Ptrain_tensor.shape, Ptrain_static_tensor.shape, Ptrain_time_tensor.shape, ytrain_tensor.shape)\n",
    "            elif dataset == 'PAM':\n",
    "                T, F = Ptrain[0].shape\n",
    "                D = 1\n",
    "\n",
    "                Ptrain_tensor = Ptrain\n",
    "                Ptrain_static_tensor = np.zeros((len(Ptrain), D))\n",
    "\n",
    "                mf, stdf = getStats(Ptrain)\n",
    "                Ptrain_tensor, Ptrain_static_tensor, Ptrain_time_tensor, ytrain_tensor = tensorize_normalize_other(Ptrain, ytrain, mf, stdf)\n",
    "                Pval_tensor, Pval_static_tensor, Pval_time_tensor, yval_tensor = tensorize_normalize_other(Pval, yval, mf, stdf)\n",
    "                Ptest_tensor, Ptest_static_tensor, Ptest_time_tensor, ytest_tensor = tensorize_normalize_other(Ptest, ytest, mf, stdf)\n",
    "\n",
    "            # remove part of variables in validation and test set\n",
    "            if missing_ratio > 0:\n",
    "                num_all_features = Pval_tensor.shape[2]\n",
    "                num_missing_features = round(missing_ratio * num_all_features)\n",
    "                if feature_removal_level == 'sample':\n",
    "                    for i, patient in enumerate(Pval_tensor):\n",
    "                        idx = np.random.choice(num_all_features, num_missing_features, replace=False)\n",
    "                        patient[:, idx] = torch.zeros(Pval_tensor.shape[1], num_missing_features)  # values\n",
    "                        Pval_tensor[i] = patient\n",
    "                    for i, patient in enumerate(Ptest_tensor):\n",
    "                        idx = np.random.choice(num_all_features, num_missing_features, replace=False)\n",
    "                        patient[:, idx] = torch.zeros(Ptest_tensor.shape[1], num_missing_features)   # values\n",
    "                        Ptest_tensor[i] = patient\n",
    "                elif feature_removal_level == 'set':\n",
    "                    density_score_indices = np.load('saved/IG_density_scores_' + dataset + '.npy', allow_pickle=True)[:, 0]\n",
    "                    idx = density_score_indices[:num_missing_features].astype(int)\n",
    "                    Pval_tensor[:, :, idx] = torch.zeros(Pval_tensor.shape[0], Pval_tensor.shape[1], num_missing_features)\n",
    "                    Ptest_tensor[:, :, idx] = torch.zeros(Ptest_tensor.shape[0], Ptest_tensor.shape[1], num_missing_features)\n",
    "\n",
    "            Ptrain_tensor = Ptrain_tensor.permute(1, 0, 2)\n",
    "            Pval_tensor = Pval_tensor.permute(1, 0, 2)\n",
    "            Ptest_tensor = Ptest_tensor.permute(1, 0, 2)\n",
    "\n",
    "            Ptrain_time_tensor = Ptrain_time_tensor.squeeze(2).permute(1, 0)\n",
    "            Pval_time_tensor = Pval_time_tensor.squeeze(2).permute(1, 0)\n",
    "            Ptest_time_tensor = Ptest_time_tensor.squeeze(2).permute(1, 0)\n",
    "\n",
    "            for m in range(n_runs):\n",
    "                print('- - Run %d - -' % (m + 1))\n",
    "\n",
    "                if dataset == 'P12' or dataset == 'P19' or dataset == 'eICU':\n",
    "                    model = TransformerModel2(d_inp, d_model, nhead, nhid, nlayers, dropout, max_len,\n",
    "                                              d_static, MAX, 0.5, aggreg, n_classes, static=False)\n",
    "                elif dataset == 'PAM':\n",
    "                    model = TransformerModel2(d_inp, d_model, nhead, nhid, nlayers, dropout, max_len,\n",
    "                                              d_static, MAX, 0.5, aggreg, n_classes, static=False)\n",
    "\n",
    "                model = model.cuda()\n",
    "\n",
    "                criterion = torch.nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "                scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.1,\n",
    "                                                                       patience=1, threshold=0.0001, threshold_mode='rel',\n",
    "                                                                       cooldown=0, min_lr=1e-8, eps=1e-08, verbose=True)\n",
    "\n",
    "                idx_0 = np.where(ytrain == 0)[0]\n",
    "                idx_1 = np.where(ytrain == 1)[0]\n",
    "\n",
    "                if dataset == 'P12' or dataset == 'P19' or dataset == 'eICU':\n",
    "                    strategy = 2\n",
    "                elif dataset == 'PAM':\n",
    "                    strategy = 3\n",
    "\n",
    "                n0, n1 = len(idx_0), len(idx_1)\n",
    "                expanded_idx_1 = np.concatenate([idx_1, idx_1, idx_1], axis=0)\n",
    "                expanded_n1 = len(expanded_idx_1)\n",
    "\n",
    "                batch_size = 128\n",
    "                if strategy == 1:\n",
    "                    n_batches = 10\n",
    "                elif strategy == 2:\n",
    "                    K0 = n0 // int(batch_size / 2)\n",
    "                    K1 = expanded_n1 // int(batch_size / 2)\n",
    "                    n_batches = np.min([K0, K1])\n",
    "                elif strategy == 3:\n",
    "                    n_batches = 30\n",
    "\n",
    "                best_aupr_val = best_auc_val = 0.0\n",
    "\n",
    "                start = time.time()\n",
    "                if wandb:\n",
    "                    wandb.watch(model)\n",
    "                for epoch in range(num_epochs):\n",
    "                    model.train()\n",
    "\n",
    "                    if strategy == 2:\n",
    "                        np.random.shuffle(expanded_idx_1)\n",
    "                        I1 = expanded_idx_1\n",
    "                        np.random.shuffle(idx_0)\n",
    "                        I0 = idx_0\n",
    "\n",
    "                    for n in range(n_batches):\n",
    "                        if strategy == 1:\n",
    "                            idx = random_sample(idx_0, idx_1, batch_size)\n",
    "                        elif strategy == 2:\n",
    "                            \"\"\"In each batch=128, 64 positive samples, 64 negative samples\"\"\"\n",
    "                            idx0_batch = I0[n * int(batch_size / 2):(n + 1) * int(batch_size / 2)]\n",
    "                            idx1_batch = I1[n * int(batch_size / 2):(n + 1) * int(batch_size / 2)]\n",
    "                            idx = np.concatenate([idx0_batch, idx1_batch], axis=0)\n",
    "                        elif strategy == 3:\n",
    "                            idx = np.random.choice(list(range(Ptrain_tensor.shape[1])), size=int(batch_size), replace=False)\n",
    "                            # idx = random_sample_8(ytrain, batch_size)   # to balance dataset\n",
    "\n",
    "                        if dataset == 'P12' or dataset == 'P19' or dataset == 'eICU':\n",
    "                            P, Ptime, Pstatic, y = Ptrain_tensor[:, idx, :].cuda(), Ptrain_time_tensor[:, idx].cuda(), \\\n",
    "                                                   Ptrain_static_tensor[idx].cuda(), ytrain_tensor[idx].cuda()\n",
    "                        elif dataset == 'PAM':\n",
    "                            P, Ptime, Pstatic, y = Ptrain_tensor[:, idx, :].cuda(), Ptrain_time_tensor[:, idx].cuda(), \\\n",
    "                                                   None, ytrain_tensor[idx].cuda()\n",
    "\n",
    "                        lengths = torch.sum(Ptime > 0, dim=0)\n",
    "\n",
    "                        #print(P)\n",
    "                        #print(Ptime)\n",
    "                        #print(static_info)\n",
    "                        #print(Pstatic)\n",
    "                        #print(Pstatic.shape)\n",
    "\n",
    "                        outputs = evaluate_standard(model, P, Ptime, Pstatic, static=None)\n",
    "\n",
    "                        #print(outputs)\n",
    "\n",
    "                        optimizer.zero_grad()\n",
    "                        loss = criterion(outputs, y)\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                    if dataset == 'P12' or dataset == 'P19' or dataset == 'eICU':\n",
    "                        train_probs = torch.squeeze(torch.sigmoid(outputs))\n",
    "                        train_probs = train_probs.cpu().detach().numpy()\n",
    "                        train_y = y.cpu().detach().numpy()\n",
    "                        train_auroc = roc_auc_score(train_y, train_probs[:, 1])\n",
    "                        train_auprc = average_precision_score(train_y, train_probs[:, 1])\n",
    "                    elif dataset == 'PAM':\n",
    "                        train_probs = torch.squeeze(nn.functional.softmax(outputs, dim=1))\n",
    "                        train_probs = train_probs.cpu().detach().numpy()\n",
    "                        train_y = y.cpu().detach().numpy()\n",
    "                        train_auroc = roc_auc_score(one_hot(train_y), train_probs)\n",
    "                        train_auprc = average_precision_score(one_hot(train_y), train_probs)\n",
    "\n",
    "                    if wandb:\n",
    "                        wandb.log({ \"train_loss\": loss.item(), \"train_auprc\": train_auprc, \"train_auroc\": train_auroc})\n",
    "\n",
    "                    \"\"\"Validation\"\"\"\n",
    "                    model.eval()\n",
    "                    if epoch ==0 or epoch % 1 == 0:\n",
    "                        with torch.no_grad():\n",
    "                            out_val = evaluate_standard(model, Pval_tensor, Pval_time_tensor, Pval_static_tensor, static=static_info)\n",
    "                            out_val = torch.squeeze(torch.sigmoid(out_val))\n",
    "                            out_val = out_val.detach().cpu().numpy()\n",
    "\n",
    "                            val_loss = criterion(torch.from_numpy(out_val), torch.from_numpy(yval.squeeze(1)).long())\n",
    "\n",
    "                            if dataset == 'P12' or dataset == 'P19' or dataset == 'eICU':\n",
    "                                auc_val = roc_auc_score(yval, out_val[:, 1])\n",
    "                                aupr_val = average_precision_score(yval, out_val[:, 1])\n",
    "                            elif dataset == 'PAM':\n",
    "                                auc_val = roc_auc_score(one_hot(yval), out_val)\n",
    "                                aupr_val = average_precision_score(one_hot(yval), out_val)\n",
    "\n",
    "                            print(\"Validation: Epoch %d,  val_loss:%.4f, aupr_val: %.2f, auc_val: %.2f\" % (epoch,\n",
    "                              val_loss.item(), aupr_val * 100, auc_val * 100))\n",
    "\n",
    "                            if wandb:\n",
    "                                wandb.log({ \"val_loss\": val_loss.item(), \"val_auprc\": aupr_val, \"val_auroc\": auc_val})\n",
    "\n",
    "                            scheduler.step(aupr_val)\n",
    "                            if auc_val > best_auc_val:\n",
    "                                best_auc_val = auc_val\n",
    "                                print(\n",
    "                                    \"**[S] Epoch %d, aupr_val: %.4f, auc_val: %.4f **\" % (epoch, aupr_val * 100, auc_val * 100))\n",
    "                                torch.save(model.state_dict(), model_path + arch + '_' + str(split_idx) + '.pt')\n",
    "\n",
    "                end = time.time()\n",
    "                time_elapsed = end - start\n",
    "                print('Total Time elapsed: %.3f mins' % (time_elapsed / 60.0))\n",
    "\n",
    "                \"\"\"Testing\"\"\"\n",
    "                model.load_state_dict(torch.load(model_path + arch + '_' + str(split_idx) + '.pt'))\n",
    "                model.eval()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    out_test = evaluate(model, Ptest_tensor, Ptest_time_tensor, Ptest_static_tensor, n_classes=n_classes, static=None).numpy()\n",
    "                    ypred = np.argmax(out_test, axis=1)\n",
    "\n",
    "                    # Adding zero interaction students\n",
    "                    ytest = np.append(ytest, zero_ytest, axis=0)\n",
    "                    ypred = np.append(ypred, np.zeros([1, len(zero_ytest)]))\n",
    "\n",
    "                    denoms = np.sum(np.exp(out_test), axis=1).reshape((-1, 1))\n",
    "                    probs = np.exp(out_test) / denoms\n",
    "                    \n",
    "                    # Adding zero interaction students\n",
    "                    probs = np.append(probs, np.zeros([len(zero_ytest), 2]), axis=0)\n",
    "                    probs = np.nan_to_num(probs)\n",
    "\n",
    "                    acc = np.sum(ytest.ravel() == ypred.ravel()) / ytest.shape[0]\n",
    "                    bac = balanced_accuracy_score(ytest.ravel(), ypred.ravel())\n",
    "\n",
    "\n",
    "                    auc = roc_auc_score(ytest, probs[:, 1])\n",
    "                    aupr = average_precision_score(ytest, probs[:, 1])\n",
    "\n",
    "\n",
    "                # store\n",
    "                    results = pd.DataFrame(columns=['course', 'percentile', 'acc', 'bac', 'auc', 'auprc'])\n",
    "                    results.loc[0] = [MOOC, percentile, acc, bac, auc, aupr]\n",
    "                    results.to_csv(f\"../../transformer_results/test_{MOOC}_{percentile}.csv\")\n",
    "                    print(results)\n",
    "\n",
    "                    \n",
    "                    \n",
    "                    #VALIDATION\n",
    "                    out_val = evaluate(model, Pval_tensor, Pval_time_tensor, Pval_static_tensor, n_classes=n_classes, static=None).numpy()\n",
    "                    ypred = np.argmax(out_val, axis=1)\n",
    "\n",
    "                    # Adding zero interaction students\n",
    "                    yval = np.append(yval, zero_yval, axis=0)\n",
    "                    ypred = np.append(ypred, np.zeros([1, len(zero_yval)]))\n",
    "\n",
    "                    denoms = np.sum(np.exp(out_val), axis=1).reshape((-1, 1))\n",
    "                    probs = np.exp(out_val) / denoms\n",
    "                    probs = np.nan_to_num(probs)\n",
    "                    \n",
    "                    # Adding zero interaction students\n",
    "                    probs = np.append(probs, np.zeros([len(zero_yval), 2]), axis=0)\n",
    "\n",
    "                    acc = np.sum(yval.ravel() == ypred.ravel()) / yval.shape[0]\n",
    "                    bac = balanced_accuracy_score(yval.ravel(), ypred.ravel())\n",
    "\n",
    "\n",
    "                    auc = roc_auc_score(yval, probs[:, 1])\n",
    "                    aupr = average_precision_score(yval, probs[:, 1])\n",
    "\n",
    "\n",
    "                # store\n",
    "                    results_val = pd.DataFrame(columns=['course', 'percentile', 'acc', 'bac', 'auc', 'auprc'])\n",
    "                    results_val.loc[0] = [MOOC, percentile, acc, bac, auc, aupr]\n",
    "                    results_val.to_csv(f\"../../transformer_results/val_{MOOC}_{percentile}.csv\")\n",
    "                    print(results_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216ae60d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b4acb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c186f18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087299f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b6b73b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf0dd72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4408f9f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1c99f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d465a78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d8af04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c18e48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c5a255",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf1a832",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea721a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48edbfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4ada07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e68675",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813c7bd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3880186",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3176bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
