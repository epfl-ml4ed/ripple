{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44b48407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/../ML4ED/Raindrop/code\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix, average_precision_score, precision_score, recall_score, f1_score, balanced_accuracy_score\n",
    "%cd Raindrop/code\n",
    "from models_rd import *\n",
    "from utils_rd import *\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0423c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MOOCs_list = [\n",
    "'villesafricaines_002.csv',\n",
    " 'villesafricaines_003.csv',\n",
    " 'microcontroleurs_004.csv',\n",
    " 'dsp_004.csv',\n",
    " 'hwts_001.csv',\n",
    " 'dsp_001.csv',\n",
    " 'progfun_002.csv',\n",
    " 'microcontroleurs_003.csv',\n",
    " 'geomatique_003.csv',\n",
    " 'villesafricaines_001.csv',\n",
    " 'progfun_003.csv',\n",
    " 'dsp_002.csv',\n",
    " 'structures_002.csv',\n",
    " 'initprogcpp_001.csv',\n",
    " 'analysenumerique_003.csv',\n",
    " 'microcontroleurs_006.csv',\n",
    " 'dsp_005.csv',\n",
    " 'hwts_002.csv',\n",
    " 'dsp_006.csv',\n",
    " 'analysenumerique_002.csv',\n",
    " 'structures_003.csv',\n",
    " 'microcontroleurs_005.csv',\n",
    " 'venture_001.csv',\n",
    " 'analysenumerique_001.csv',\n",
    " 'cpp_fr_001.csv',\n",
    " 'structures_001.csv'\n",
    "]\n",
    "MOOCs_list = [i.replace(\"_\", \"-\").split('.')[0] for i in MOOCs_list]\n",
    "\n",
    "dims4 = [\n",
    " 12,\n",
    " 12,\n",
    " 13,\n",
    " 12,\n",
    " 12,\n",
    " 6,\n",
    " 12,\n",
    " 13,\n",
    " 12,\n",
    " 12,\n",
    " 12,\n",
    " 12,\n",
    " 13,\n",
    " 13,\n",
    " 6,\n",
    " 13,\n",
    " 12,\n",
    " 12,\n",
    " 12,\n",
    " 12,\n",
    " 13,\n",
    " 13,\n",
    " 12,\n",
    " 6,\n",
    " 13,\n",
    " 12\n",
    "]\n",
    "\n",
    "dims6 = [\n",
    " 12,\n",
    " 12,\n",
    " 13,\n",
    " 12,\n",
    " 12,\n",
    " 6,\n",
    " 12,\n",
    " 13,\n",
    " 12,\n",
    " 12,\n",
    " 12,\n",
    " 12,\n",
    " 13,\n",
    " 13,\n",
    " 6,\n",
    " 13,\n",
    " 12,\n",
    " 12,\n",
    " 13,\n",
    " 12,\n",
    " 13,\n",
    " 13,\n",
    " 12,\n",
    " 6,\n",
    " 13,\n",
    " 12\n",
    "]\n",
    "dims = {40: dims4, 60: dims6}\n",
    "data_path = '/../data'\n",
    "percentile = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "736b86ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop epochs: 25, Batches/epoch: 1, Total batches: 25\n",
      "[[  0 128]\n",
      " [  0 128]]\n",
      "Validation: Epoch 0,  val_loss:0.7485, aupr_val: 26.86, auc_val: 89.83\n",
      "**[S] Epoch 0, aupr_val: 26.8567, auc_val: 89.8336 **\n",
      "Validation: Epoch 1,  val_loss:0.7448, aupr_val: 24.61, auc_val: 89.74\n",
      "Validation: Epoch 2,  val_loss:0.7412, aupr_val: 24.92, auc_val: 89.47\n",
      "Epoch 00003: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Validation: Epoch 3,  val_loss:0.7408, aupr_val: 24.72, auc_val: 89.43\n",
      "Validation: Epoch 4,  val_loss:0.7404, aupr_val: 30.27, auc_val: 89.47\n",
      "Validation: Epoch 5,  val_loss:0.7401, aupr_val: 30.27, auc_val: 89.47\n",
      "Validation: Epoch 6,  val_loss:0.7397, aupr_val: 30.09, auc_val: 89.43\n",
      "Epoch 00007: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Validation: Epoch 7,  val_loss:0.7397, aupr_val: 30.09, auc_val: 89.43\n",
      "Validation: Epoch 8,  val_loss:0.7396, aupr_val: 30.09, auc_val: 89.43\n",
      "Epoch 00009: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Validation: Epoch 9,  val_loss:0.7396, aupr_val: 30.09, auc_val: 89.43\n",
      "Validation: Epoch 10,  val_loss:0.7396, aupr_val: 30.09, auc_val: 89.43\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Validation: Epoch 11,  val_loss:0.7396, aupr_val: 30.09, auc_val: 89.43\n",
      "Validation: Epoch 12,  val_loss:0.7396, aupr_val: 30.09, auc_val: 89.43\n",
      "Validation: Epoch 13,  val_loss:0.7396, aupr_val: 30.09, auc_val: 89.43\n",
      "Validation: Epoch 14,  val_loss:0.7396, aupr_val: 30.09, auc_val: 89.43\n",
      "Validation: Epoch 15,  val_loss:0.7396, aupr_val: 30.09, auc_val: 89.43\n",
      "Validation: Epoch 16,  val_loss:0.7396, aupr_val: 30.09, auc_val: 89.43\n",
      "Validation: Epoch 17,  val_loss:0.7396, aupr_val: 30.09, auc_val: 89.43\n",
      "Validation: Epoch 18,  val_loss:0.7396, aupr_val: 30.09, auc_val: 89.43\n",
      "Validation: Epoch 19,  val_loss:0.7396, aupr_val: 30.09, auc_val: 89.43\n",
      "Validation: Epoch 20,  val_loss:0.7396, aupr_val: 30.09, auc_val: 89.43\n",
      "Validation: Epoch 21,  val_loss:0.7396, aupr_val: 30.09, auc_val: 89.43\n",
      "Validation: Epoch 22,  val_loss:0.7396, aupr_val: 30.09, auc_val: 89.43\n",
      "Validation: Epoch 23,  val_loss:0.7396, aupr_val: 30.09, auc_val: 89.43\n",
      "[[  0 128]\n",
      " [  0 128]]\n",
      "Validation: Epoch 24,  val_loss:0.7396, aupr_val: 30.09, auc_val: 89.43\n",
      "Total Time elapsed: 0.920 mins\n",
      "        course  percentile       acc       bac        f1       auc    auprc\n",
      "0  venture-001          40  0.311526  0.644695  0.082988  0.868167  0.21828\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "for MOOC_idx, MOOC in enumerate(MOOCs_list):\n",
    "    \n",
    "    d_inp = dims[percentile][MOOC_idx]\n",
    "    max_len = 1000\n",
    "    n_classes = 2\n",
    "    global_structure = torch.ones(d_inp, d_inp)\n",
    "    saved_model_path = f\"../models/n1_mlp2_best_raindrop_{MOOC}_{percentile}.pt\"\n",
    "    batch_size = 256\n",
    "    n_splits = 1\n",
    "\n",
    "    d_static = 9\n",
    "    static_info = 1\n",
    "    d_ob = 4\n",
    "    d_model = d_inp * d_ob\n",
    "    nhid = 2 * d_model\n",
    "    nlayers = 1\n",
    "    nhead = 2\n",
    "    dropout = 0.2\n",
    "    sensor_wise_mask = False\n",
    "    MAX = 100\n",
    "    aggreg = 'mean'\n",
    "    n_runs=1\n",
    "\n",
    "    learning_rate = 0.0001\n",
    "    num_epochs = 25\n",
    "\n",
    "    acc_arr = np.zeros((n_splits, n_runs))\n",
    "    auprc_arr = np.zeros((n_splits, n_runs))\n",
    "    auroc_arr = np.zeros((n_splits, n_runs))\n",
    "    precision_arr = np.zeros((n_splits, n_runs))\n",
    "    recall_arr = np.zeros((n_splits, n_runs))\n",
    "    F1_arr = np.zeros((n_splits, n_runs))\n",
    "\n",
    "\n",
    "    base_path = '/../data/prep_data'\n",
    "    Pdict_list = np.load(os.path.join(base_path, f\"{MOOC}_{percentile}_data_hard_fail.npy\"), allow_pickle=True)\n",
    "    arr_outcomes = np.load(os.path.join(base_path, f\"{MOOC}_{percentile}_y_hard_fail.npy\"), allow_pickle=True)\n",
    "\n",
    "    #Ptrain, Ptest, ytrain, ytest = train_test_split(Pdict_list, arr_outcomes, test_size=0.1, random_state=1)\n",
    "    #Ptrain, Pval, ytrain, yval = train_test_split(Ptrain, ytrain, test_size=1/9, random_state=1)\n",
    "    args_train, args_val, args_test = np.load(os.path.join(data_path, \n",
    "                                                                   'split_args', f\"split_{MOOC.replace('-', '_')}.npy\"),\n",
    "                                                     allow_pickle=True)\n",
    "    Ptrain = Pdict_list[args_train]\n",
    "    Pval = Pdict_list[args_val]\n",
    "    Ptest = Pdict_list[args_test]\n",
    "    ytrain = arr_outcomes[args_train, :]\n",
    "    yval = arr_outcomes[args_val, :]\n",
    "    ytest = arr_outcomes[args_test, :]\n",
    "    \n",
    "    \n",
    "    zero_indices = [i for i, item in enumerate(Ptrain) if item['length'] == 0]\n",
    "    #zero_Ptrain = Ptrain[zero_indices]\n",
    "    Ptrain = np.delete(Ptrain, zero_indices, axis=0)\n",
    "    ytrain = np.delete(ytrain, zero_indices, axis=0)\n",
    "\n",
    "    zero_indices = [i for i, item in enumerate(Pval) if item['length'] == 0]\n",
    "    zero_yval = ytest[zero_indices]\n",
    "    Pval = np.delete(Pval, zero_indices, axis=0)\n",
    "    yval = np.delete(yval, zero_indices, axis=0)\n",
    "\n",
    "    zero_indices = [i for i, item in enumerate(Ptest) if item['length'] == 0]\n",
    "    zero_ytest = ytest[zero_indices]\n",
    "    Ptest = np.delete(Ptest, zero_indices, axis=0)  \n",
    "    ytest = np.delete(ytest, zero_indices, axis=0)\n",
    "    \n",
    "\n",
    "    T, F = Ptrain[0]['arr'].shape\n",
    "    D = len(Ptrain[0]['extended_static'])\n",
    "\n",
    "    Ptrain_tensor = np.zeros((len(Ptrain), T, F))\n",
    "    Ptrain_static_tensor = np.zeros((len(Ptrain), D))\n",
    "\n",
    "    for i in range(len(Ptrain)):\n",
    "        Ptrain_tensor[i] = Ptrain[i]['arr']\n",
    "        Ptrain_static_tensor[i] = Ptrain[i]['extended_static']\n",
    "\n",
    "    mf, stdf = getStats(Ptrain_tensor)\n",
    "    ms, ss = getStats_static(Ptrain_static_tensor, dataset='P12')\n",
    "\n",
    "    Ptrain_tensor, Ptrain_static_tensor, Ptrain_time_tensor, ytrain_tensor = tensorize_normalize(Ptrain, ytrain, mf,\n",
    "                                                                                                 stdf, ms, ss)\n",
    "    Pval_tensor, Pval_static_tensor, Pval_time_tensor, yval_tensor = tensorize_normalize(Pval, yval, mf, stdf, ms, ss)\n",
    "\n",
    "    Ptest_tensor, Ptest_static_tensor, Ptest_time_tensor, ytest_tensor = tensorize_normalize(Ptest, ytest, mf, stdf, ms, ss)\n",
    "\n",
    "    Ptrain_tensor = Ptrain_tensor.permute(1, 0, 2)\n",
    "    Pval_tensor = Pval_tensor.permute(1, 0, 2)\n",
    "    Ptest_tensor = Ptest_tensor.permute(1, 0, 2)\n",
    "\n",
    "    Ptrain_time_tensor = Ptrain_time_tensor.squeeze(2).permute(1, 0)\n",
    "    Pval_time_tensor = Pval_time_tensor.squeeze(2).permute(1, 0)\n",
    "    Ptest_time_tensor = Ptest_time_tensor.squeeze(2).permute(1, 0)\n",
    "    \n",
    "    model = Raindrop_v2(d_inp, d_model, nhead, nhid, nlayers, dropout, max_len,\n",
    "                                        d_static, MAX, 0.5, aggreg, n_classes, global_structure,\n",
    "                                        sensor_wise_mask=sensor_wise_mask)\n",
    "    #torch.save(model, '../models/raw_raindrop_model.pt')\n",
    "\n",
    "    model = model.cuda()\n",
    "    criterion = torch.nn.CrossEntropyLoss().cuda()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.1,\n",
    "                                                           patience=1, threshold=0.0001, threshold_mode='rel',\n",
    "                                                           cooldown=0, min_lr=1e-8, eps=1e-08, verbose=True)\n",
    "\n",
    "\n",
    "    idx_0 = np.where(ytrain == 0)[0]\n",
    "    idx_1 = np.where(ytrain == 1)[0]\n",
    "\n",
    "    n0, n1 = len(idx_0), len(idx_1)\n",
    "    expanded_idx_1 = np.concatenate([idx_1, idx_1, idx_1], axis=0)\n",
    "    expanded_n1 = len(expanded_idx_1)\n",
    "\n",
    "    K0 = n0 // int(batch_size / 2)\n",
    "    K1 = expanded_n1 // int(batch_size / 2)\n",
    "    n_batches = np.min([K0, K1])\n",
    "\n",
    "    best_aupr_val = best_auc_val = 0.0\n",
    "    best_loss_val = 100.0\n",
    "\n",
    "    print('Stop epochs: %d, Batches/epoch: %d, Total batches: %d' % (num_epochs, n_batches, num_epochs * n_batches))\n",
    "    start = time.time()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "\n",
    "\n",
    "        np.random.shuffle(expanded_idx_1)\n",
    "        I1 = expanded_idx_1\n",
    "        np.random.shuffle(idx_0)\n",
    "        I0 = idx_0\n",
    "\n",
    "        for n in range(n_batches):\n",
    "\n",
    "            idx0_batch = I0[n * int(batch_size / 2):(n + 1) * int(batch_size / 2)]\n",
    "            idx1_batch = I1[n * int(batch_size / 2):(n + 1) * int(batch_size / 2)]\n",
    "            idx = np.concatenate([idx0_batch, idx1_batch], axis=0)\n",
    "\n",
    "\n",
    "            P, Ptime, Pstatic, y = Ptrain_tensor[:, idx, :].cuda(), Ptrain_time_tensor[:, idx].cuda(), \\\n",
    "                                   Ptrain_static_tensor[idx].cuda(), ytrain_tensor[idx].cuda()\n",
    "\n",
    "\n",
    "            lengths = torch.sum(Ptime > 0, dim=0)\n",
    "            #outputs, local_structure_regularization, _ = model.forward(P, Pstatic, Ptime, lengths)\n",
    "            outputs = model.forward(P, Pstatic, Ptime, lengths)\n",
    "            outputs = torch.nan_to_num(outputs)\n",
    "            \n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "        train_probs = torch.squeeze(torch.sigmoid(outputs))\n",
    "        train_probs = train_probs.cpu().detach().numpy()\n",
    "        train_y = y.cpu().detach().numpy()\n",
    "        train_auroc = roc_auc_score(train_y, train_probs[:, 1])\n",
    "        train_auprc = average_precision_score(train_y, train_probs[:, 1])\n",
    "\n",
    "\n",
    "        if epoch == 0 or epoch == num_epochs - 1:\n",
    "            print(confusion_matrix(train_y, np.argmax(train_probs, axis=1), labels=[0, 1]))\n",
    "\n",
    "        \"\"\"Validation\"\"\"\n",
    "        model.eval()\n",
    "        if epoch == 0 or epoch % 1 == 0:\n",
    "            with torch.no_grad():\n",
    "                #out_val = evaluate_standard(model, Pval_tensor, Pval_time_tensor, Pval_static_tensor, static=static_info)\n",
    "                #Changed into:\n",
    "                out_val = evaluate(model, Pval_tensor, Pval_time_tensor, Pval_static_tensor,batch_size=batch_size, static=static_info)\n",
    "                out_val = torch.squeeze(torch.sigmoid(out_val))\n",
    "                out_val = out_val.detach().cpu().numpy()\n",
    "\n",
    "                val_loss = criterion(torch.from_numpy(out_val), torch.from_numpy(yval.squeeze(1)).long())\n",
    "\n",
    "\n",
    "                auc_val = roc_auc_score(yval, out_val[:, 1])\n",
    "                aupr_val = average_precision_score(yval, out_val[:, 1])\n",
    "\n",
    "                print(\"Validation: Epoch %d,  val_loss:%.4f, aupr_val: %.2f, auc_val: %.2f\" % (epoch,\n",
    "                                                                                                val_loss.item(),\n",
    "                                                                                                aupr_val * 100,\n",
    "                                                                                                auc_val * 100))\n",
    "\n",
    "                scheduler.step(aupr_val)\n",
    "                if auc_val > best_auc_val:\n",
    "                    best_auc_val = auc_val\n",
    "                    print(\n",
    "                        \"**[S] Epoch %d, aupr_val: %.4f, auc_val: %.4f **\" % (\n",
    "                        epoch, aupr_val * 100, auc_val * 100))\n",
    "                    torch.save(model.state_dict(), saved_model_path)\n",
    "\n",
    "    end = time.time()\n",
    "    time_elapsed = end - start\n",
    "    print('Total Time elapsed: %.3f mins' % (time_elapsed / 60.0))\n",
    "    \n",
    "    # Test\n",
    "    model.load_state_dict(torch.load(saved_model_path))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "                    out_test = evaluate(model, Ptest_tensor, Ptest_time_tensor, Ptest_static_tensor, n_classes=n_classes, static=static_info, batch_size=batch_size).numpy()\n",
    "                    ypred = np.argmax(out_test, axis=1)\n",
    "                \n",
    "                    # Adding zero interaction students\n",
    "                    ytest = np.append(ytest, zero_ytest, axis=0)\n",
    "                    ypred = np.append(ypred, np.zeros([1, len(zero_ytest)]))\n",
    "                    \n",
    "\n",
    "                    denoms = np.sum(np.exp(out_test), axis=1).reshape((-1, 1))\n",
    "                    probs = np.exp(out_test) / denoms\n",
    "                    \n",
    "                    # Adding zero interaction students\n",
    "                    probs = np.append(probs, np.zeros([len(zero_ytest), 2]), axis=0)\n",
    "\n",
    "                    acc = np.sum(ytest.ravel() == ypred.ravel()) / ytest.shape[0]\n",
    "                    bac = balanced_accuracy_score(ytest.ravel(), ypred.ravel())\n",
    "                    f1 = f1_score(ytest.ravel(), ypred.ravel())\n",
    "\n",
    "                    auc = roc_auc_score(ytest, probs[:, 1])\n",
    "                    aupr = average_precision_score(ytest, probs[:, 1])\n",
    "\n",
    "                    #print('Testing: AUROC = %.2f | AUPRC = %.2f | Accuracy = %.2f' % (auc * 100, aupr * 100, acc * 100))\n",
    "                    #print('classification report', classification_report(ytest, ypred))\n",
    "                    #print(confusion_matrix(ytest, ypred, labels=list(range(2))))\n",
    "                    results = pd.DataFrame(columns=['course', 'percentile', 'acc', 'bac', 'f1', 'auc', 'auprc'])\n",
    "                    results.loc[0] = [MOOC, percentile, acc, bac, f1, auc, aupr]\n",
    "                    results.to_csv(f\"../raindrop_results/test_{MOOC}_{percentile}.csv\")\n",
    "                    print(results)\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    out_val = evaluate(model, Pval_tensor, Pval_time_tensor, Pval_static_tensor, n_classes=n_classes, static=static_info, batch_size=batch_size).numpy()\n",
    "                    ypred = np.argmax(out_val, axis=1)\n",
    "                \n",
    "                    # Adding zero interaction students\n",
    "                    ytest = np.append(yval, zero_yval, axis=0)\n",
    "                    ypred = np.append(ypred, np.zeros([1, len(zero_yval)]))\n",
    "                    \n",
    "\n",
    "                    denoms = np.sum(np.exp(out_val), axis=1).reshape((-1, 1))\n",
    "                    probs = np.exp(out_val) / denoms\n",
    "                    \n",
    "                    # Adding zero interaction students\n",
    "                    probs = np.append(probs, np.zeros([len(zero_yval), 2]), axis=0)\n",
    "\n",
    "                    acc = np.sum(yval.ravel() == ypred.ravel()) / yval.shape[0]\n",
    "                    bac = balanced_accuracy_score(yval.ravel(), ypred.ravel())\n",
    "                    f1 = f1_score(yval.ravel(), ypred.ravel())\n",
    "\n",
    "                    auc = roc_auc_score(yval, probs[:, 1])\n",
    "                    aupr = average_precision_score(yval, probs[:, 1])\n",
    "\n",
    "                    results_val = pd.DataFrame(columns=['course', 'percentile', 'acc', 'bac', 'f1', 'auc', 'auprc'])\n",
    "                    results_val.loc[0] = [MOOC, percentile, acc, bac, f1, auc, aupr]\n",
    "                    results_val.to_csv(f\"../raindrop_results/val_{MOOC}_{percentile}.csv\")\n",
    "                    print(results_val)\n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26853333",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f9c1be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
