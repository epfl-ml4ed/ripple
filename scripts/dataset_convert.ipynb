{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f85bfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fca0d44",
   "metadata": {},
   "source": [
    "### Auxiliary functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77074066",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_bytes(x, distinction=False):\n",
    "    x = str(x)\n",
    "    if \"b'\" not in x:\n",
    "        val = x\n",
    "    else:\n",
    "        val = x.replace(\"b'\", \"\").split(\"\\\\x00\")[0]\n",
    "        if \"'\" in val:\n",
    "            val = val[:-1]\n",
    "    if distinction:\n",
    "        if val == \"\":\n",
    "            val = False\n",
    "        else:\n",
    "            val = True\n",
    "    return val\n",
    "\n",
    "def trunc(x_input, max_len):\n",
    "    x = x_input.copy()\n",
    "    x_video_len = x['video_length']\n",
    "    x_prob_len = x['prob_length']\n",
    "    x_len = x_video_len + x_prob_len\n",
    "    \n",
    "    diff = x_len - max_len\n",
    "    \n",
    "    if diff > 0:\n",
    "        x['video_id'] = x['video_id'][:-diff]\n",
    "        x['event_type'] = x['event_type'][:x_video_len-diff] + x['event_type'][x_video_len:]\n",
    "        x['pass-fail'] = x['pass-fail']\n",
    "        x['timestamp'] = x['timestamp'][:x_video_len-diff] + x['timestamp'][x_video_len:]\n",
    "        x['arr_value'] = x['arr_value'][:x_video_len-diff] + x['arr_value'][x_video_len:]\n",
    "        x['video_length'] = x['video_length'] - diff\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039954eb",
   "metadata": {},
   "source": [
    "# Only using video events:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d6e3a7e",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'F:/SeFT4ED/DATASETS/mooc/coursera/video_event/progfun-002.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m coursetype \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmooc/coursera\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m saved_filename \u001b[38;5;241m=\u001b[39m filename\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m----> 5\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mF:/SeFT4ED/DATASETS/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcoursetype\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/video_event/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfilename\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m outcome_dataset \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF:/SeFT4ED/DATASETS/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcoursetype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/grade/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mmerge(outcome_dataset, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minner\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.8/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.8/site-packages/pandas/io/parsers/readers.py:586\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    571\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    572\u001b[0m     dialect,\n\u001b[1;32m    573\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    582\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    583\u001b[0m )\n\u001b[1;32m    584\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 586\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.8/site-packages/pandas/io/parsers/readers.py:482\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    479\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    481\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 482\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    485\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.8/site-packages/pandas/io/parsers/readers.py:811\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwds:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 811\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1040\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1037\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown engine: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mengine\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (valid options are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmapping\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1038\u001b[0m     )\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;66;03m# error: Too many arguments for \"ParserBase\"\u001b[39;00m\n\u001b[0;32m-> 1040\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.8/site-packages/pandas/io/parsers/c_parser_wrapper.py:51\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     48\u001b[0m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musecols\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39musecols\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# open handles\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open_handles\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Have to pass int, would break tests using TextReader directly otherwise :(\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.8/site-packages/pandas/io/parsers/base_parser.py:222\u001b[0m, in \u001b[0;36mParserBase._open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_handles\u001b[39m(\u001b[38;5;28mself\u001b[39m, src: FilePathOrBuffer, kwds: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;124;03m    Let the readers open IOHandles after they are done with their potential raises.\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 222\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.8/site-packages/pandas/io/common.py:702\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    697\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    698\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    701\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 702\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    710\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    711\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'F:/SeFT4ED/DATASETS/mooc/coursera/video_event/progfun-002.csv'"
     ]
    }
   ],
   "source": [
    "filename = \"progfun-002.csv\"\n",
    "coursetype = \"mooc/coursera\"\n",
    "saved_filename = filename.split('.')[0]\n",
    "\n",
    "dataset = pd.read_csv(f\"F:/SeFT4ED/DATASETS/{coursetype}/video_event/{filename}\")\n",
    "outcome_dataset = pd.read_csv(f\"F:/SeFT4ED/DATASETS/{coursetype}/grade/{filename}\")\n",
    "\n",
    "dataset = dataset.merge(outcome_dataset, on='user_id', how='inner')\n",
    "if 'XXX-' in filename:\n",
    "    dataset['pass-fail'] = dataset['grade'].apply(lambda x: 'Passed' if x>=4 else 'Failed')\n",
    "\n",
    "min_len = 5\n",
    "max_len = 215\n",
    "\n",
    "#======\n",
    "#Adding numbers to arr_value\n",
    "list_of_video_events = list(np.unique(dataset['event_type']))\n",
    "temp_datasets = []\n",
    "for event_type in list_of_video_events:\n",
    "    temp_dataset = dataset[dataset.event_type == event_type].copy()\n",
    "    if event_type in ['Video.Error', 'Video.Pause', 'Video.Play']:\n",
    "        temp_dataset['arr_value'] = temp_dataset['current_time']\n",
    "    elif event_type in ['Video.Seek', 'Video.Stalled']:\n",
    "        temp_dataset['arr_value'] = temp_dataset['current_time'] - temp_dataset['old_time']\n",
    "    elif event_type == 'Video.SpeedChange':\n",
    "        temp_dataset['arr_value'] = temp_dataset['new_speed']\n",
    "    else:\n",
    "        temp_dataset['arr_value'] = 1\n",
    "    temp_datasets.append(temp_dataset)\n",
    "    \n",
    "merged_video = pd.concat(temp_datasets)\n",
    "dataset = merged_video[['user_id', 'video_id', 'event_type', 'timestamp', 'pass-fail', 'arr_value']].dropna()\n",
    "#======\n",
    "\n",
    "\n",
    "dataset_group = dataset.groupby('user_id')\n",
    "time_event_per_user = dataset_group[['timestamp', 'event_type', 'pass-fail', 'video_id', 'arr_value']].agg(lambda x: list(x))\n",
    "selected_users = time_event_per_user[(dataset_group.size() > min_len) & (dataset_group.size() < max_len)]\n",
    "selected_users = selected_users.reset_index(level=0)\n",
    "selected_users['timestamp'] = selected_users['timestamp'].apply(lambda x: np.pad(x, (0, max_len-len(x)), 'constant', constant_values=(0, 0)))\n",
    "selected_users['event_type'] = selected_users['event_type'].apply(lambda x: [list_of_video_events.index(i) for i in x])\n",
    "selected_users['pass-fail'] = selected_users['pass-fail'].apply(lambda x: 1 if x[0]=='Passed' else 0)\n",
    "\n",
    "P_users = []\n",
    "y_users = []\n",
    "for index, row in selected_users.iterrows():\n",
    "    user_id = row['user_id']\n",
    "    time = row['timestamp']\n",
    "    events = row['event_type']\n",
    "    arr = np.zeros([max_len, len(list_of_video_events)+1])\n",
    "    arr[np.arange(len(events)), events] = row['arr_value']\n",
    "    arr[:len(events), len(list_of_video_events)] = row['video_id']\n",
    "    \n",
    "    indices = np.argsort(time)\n",
    "    \n",
    "    P_users.append({'id': user_id, \n",
    "                    'static': tuple([0, 0, 0, 0, 0, 0]), \n",
    "                    'extended_static': [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                    'arr': arr[indices],\n",
    "                    'time': np.reshape(time[indices], (max_len, 1)), \n",
    "                    'length': len(events)\n",
    "                    })\n",
    "    \n",
    "    y_users.append(row['pass-fail'])\n",
    "y_users = np.reshape(np.array(y_users), (len(y_users), 1))\n",
    "\n",
    "np.save(f\"{filename.split('.')[0]}_data.npy\", P_users)\n",
    "np.save(f\"{filename.split('.')[0]}_y.npy\", y_users)\n",
    "\n",
    "print(f\"Dataset: {filename} \\n Users: {len(P_users)} \\n 1/all ratio: {sum(y_users)/len(y_users)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34470a4",
   "metadata": {},
   "source": [
    "# Video+Problem events:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6f4f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"progfun-002.csv\"\n",
    "coursetype = \"mooc/coursera\"\n",
    "saved_filename = filename.split('.')[0]\n",
    "\n",
    "video_dataset = pd.read_csv(f\"F:/SeFT4ED/DATASETS/{coursetype}/video_event/{filename}\")\n",
    "outcome_dataset = pd.read_csv(f\"F:/SeFT4ED/DATASETS/{coursetype}/grade/{filename}\")\n",
    "\n",
    "prob_dataset = pd.read_csv(f\"F:/SeFT4ED/DATASETS/{coursetype}/problem_event/{filename}\")\n",
    "\n",
    "video_dataset = video_dataset.merge(outcome_dataset, on='user_id', how='inner')\n",
    "if 'XXX-' in filename:\n",
    "    dataset['pass-fail'] = dataset['grade'].apply(lambda x: 'Passed' if x>=4 else 'Failed')\n",
    "\n",
    "min_len = 5\n",
    "max_len = 215\n",
    "\n",
    "#======\n",
    "#Adding numbers to arr_value, VIDEO\n",
    "list_of_video_events = list(np.unique(video_dataset['event_type']))\n",
    "temp_datasets = []\n",
    "for event_type in list_of_video_events:\n",
    "    temp_dataset = video_dataset[video_dataset.event_type == event_type].copy()\n",
    "    if event_type in ['Video.Error', 'Video.Pause', 'Video.Play']:\n",
    "        temp_dataset['arr_value'] = temp_dataset['current_time']\n",
    "    elif event_type in ['Video.Seek', 'Video.Stalled']:\n",
    "        temp_dataset['arr_value'] = temp_dataset['current_time'] - temp_dataset['old_time']\n",
    "    elif event_type == 'Video.SpeedChange':\n",
    "        temp_dataset['arr_value'] = temp_dataset['new_speed']\n",
    "    else:\n",
    "        temp_dataset['arr_value'] = 1\n",
    "    temp_datasets.append(temp_dataset)\n",
    "    \n",
    "merged_video = pd.concat(temp_datasets)\n",
    "video_dataset = merged_video[['user_id', 'video_id', 'event_type', 'timestamp', 'pass-fail', 'arr_value']].dropna()\n",
    "#======\n",
    "\n",
    "\n",
    "#======\n",
    "#Adding numbers to arr_value, PROBLEM\n",
    "list_of_prob_types = list(np.unique(prob_dataset['problem_type']))\n",
    "temp_datasets = []\n",
    "for prob_type in list_of_prob_types:\n",
    "    temp_dataset = prob_dataset[prob_dataset.problem_type == prob_type].copy()\n",
    "    \n",
    "    if prob_type == 'Assignment Part':\n",
    "        temp_dataset = temp_dataset.dropna()\n",
    "        temp_dataset['arr_value'] = temp_dataset['grade']\n",
    "    elif prob_type == 'Quiz':\n",
    "        temp_dataset['arr_value'] = 1\n",
    "    else:\n",
    "        print(\"New Problem type Found!!\")\n",
    "    temp_datasets.append(temp_dataset)\n",
    "    \n",
    "    \n",
    "merged_prob = pd.concat(temp_datasets)\n",
    "prob_dataset = merged_prob[['user_id', 'problem_id', 'problem_type', 'timestamp', 'submission_number', 'arr_value']].dropna()\n",
    "#======\n",
    "\n",
    "\n",
    "prob_group = prob_dataset.groupby('user_id').agg(list)\n",
    "video_group = video_dataset.groupby('user_id').agg(list)\n",
    "\n",
    "merged = video_group.merge(prob_group, on='user_id', how='inner', suffixes=['_video', '_prob'])\n",
    "\n",
    "merged['video_length'] = merged['timestamp_video'].apply(len)\n",
    "merged['prob_length'] = merged['timestamp_prob'].apply(len)\n",
    "merged['timestamp'] = merged['timestamp_video'] + merged['timestamp_prob']\n",
    "merged['event_type'] = merged['event_type'] + merged['problem_type']\n",
    "merged['arr_value'] = merged['arr_value_video'] + merged['arr_value_prob']\n",
    "\n",
    "dataset = merged.drop(columns=['timestamp_video', 'timestamp_prob', 'arr_value_video', 'arr_value_prob', 'problem_type'])\n",
    "\n",
    "list_of_events = list_of_video_events + list_of_prob_types\n",
    "\n",
    "selected_users = dataset[(dataset['video_length'] + dataset['prob_length'] > min_len) & (dataset['video_length'] + dataset['prob_length'] < max_len)]\n",
    "selected_users = selected_users.reset_index(level=0)\n",
    "selected_users['timestamp'] = selected_users['timestamp'].apply(lambda x: np.pad(x, (0, max_len-len(x)), 'constant', constant_values=(0, 0)))\n",
    "selected_users['event_type'] = selected_users['event_type'].apply(lambda x: [list_of_events.index(i) for i in x])\n",
    "selected_users['pass-fail'] = selected_users['pass-fail'].apply(lambda x: 1 if x[0]=='Passed' else 0)\n",
    "\n",
    "P_users = []\n",
    "y_users = []\n",
    "for index, row in selected_users.iterrows():\n",
    "    user_id = row['user_id']\n",
    "    time = row['timestamp']\n",
    "    events = row['event_type']\n",
    "    arr = np.zeros([max_len, len(list_of_events)+3])\n",
    "    arr[np.arange(len(events)), events] = row['arr_value']\n",
    "    arr[:row['video_length'], len(list_of_events)] = row['video_id']\n",
    "    arr[row['video_length']:row['video_length']+row['prob_length'], len(list_of_events)+1] = row['problem_id']\n",
    "    arr[row['video_length']:row['video_length']+row['prob_length'], len(list_of_events)+2] = row['submission_number']\n",
    "    \n",
    "    indices = np.argsort(time)\n",
    "    \n",
    "    P_users.append({'id': user_id, \n",
    "                    'static': tuple([0, 0, 0, 0, 0, 0]), \n",
    "                    'extended_static': [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                    'arr': arr[indices],\n",
    "                    'time': np.reshape(time[indices], (max_len, 1)), \n",
    "                    'length': len(events)\n",
    "                    })\n",
    "    \n",
    "    y_users.append(row['pass-fail'])\n",
    "y_users = np.reshape(np.array(y_users), (len(y_users), 1))\n",
    "\n",
    "np.save(f\"{filename.split('.')[0]}_data.npy\", P_users)\n",
    "np.save(f\"{filename.split('.')[0]}_y.npy\", y_users)\n",
    "\n",
    "print(f\"Dataset: {filename} \\n Users: {len(P_users)} \\n 1/all ratio: {sum(y_users)/len(y_users)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad65b14",
   "metadata": {},
   "source": [
    "# Testing without assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1154fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"progfun-002.csv\"\n",
    "coursetype = \"mooc/coursera\"\n",
    "saved_filename = filename.split('.')[0]\n",
    "\n",
    "meta_dataset = pd.read_csv('F:\\SeFT4ED\\DATASETS\\mooc\\metadata.csv')\n",
    "meta_dataset = meta_dataset[meta_dataset.course_id==saved_filename]\n",
    "\n",
    "start_timestamp = datetime.datetime.strptime(meta_dataset.start_date.to_numpy()[0], '%Y-%m-%d %H:%M:%S')\n",
    "start_timestamp = start_timestamp.timestamp()\n",
    "end_timestamp = datetime.datetime.strptime(meta_dataset.end_date.to_numpy()[0], '%Y-%m-%d %H:%M:%S')\n",
    "end_timestamp = end_timestamp.timestamp()\n",
    "\n",
    "x_percent = 1\n",
    "x_deadline = start_timestamp + x_percent*(end_timestamp-start_timestamp)\n",
    "\n",
    "video_dataset = pd.read_csv(f\"F:/SeFT4ED/DATASETS/{coursetype}/video_event/{filename}\")\n",
    "outcome_dataset = pd.read_csv(f\"F:/SeFT4ED/DATASETS/{coursetype}/grade/{filename}\")\n",
    "\n",
    "prob_dataset = pd.read_csv(f\"F:/SeFT4ED/DATASETS/{coursetype}/problem_event/{filename}\")\n",
    "\n",
    "video_dataset = video_dataset[(video_dataset.timestamp <= x_deadline)]\n",
    "prob_dataset = prob_dataset[(prob_dataset.timestamp <= x_deadline)]\n",
    "\n",
    "video_dataset = video_dataset.merge(outcome_dataset, on='user_id', how='inner')\n",
    "if 'XXX-' in filename:\n",
    "    dataset['pass-fail'] = dataset['grade'].apply(lambda x: 'Passed' if x>=4 else 'Failed')\n",
    "\n",
    "min_len = 1\n",
    "max_len = 1000\n",
    "\n",
    "#======\n",
    "#Adding numbers to arr_value, VIDEO\n",
    "list_of_video_events = list(np.unique(video_dataset['event_type']))\n",
    "temp_datasets = []\n",
    "for event_type in list_of_video_events:\n",
    "    temp_dataset = video_dataset[video_dataset.event_type == event_type].copy()\n",
    "    if event_type in ['Video.Error', 'Video.Pause', 'Video.Play']:\n",
    "        temp_dataset['arr_value'] = temp_dataset['current_time']\n",
    "    elif event_type in ['Video.Seek', 'Video.Stalled']:\n",
    "        temp_dataset['arr_value'] = temp_dataset['current_time'] - temp_dataset['old_time']\n",
    "    elif event_type == 'Video.SpeedChange':\n",
    "        temp_dataset['arr_value'] = temp_dataset['new_speed']\n",
    "    else:\n",
    "        temp_dataset['arr_value'] = 1\n",
    "    temp_datasets.append(temp_dataset)\n",
    "    \n",
    "merged_video = pd.concat(temp_datasets)\n",
    "video_dataset = merged_video[['user_id', 'video_id', 'event_type', 'timestamp', 'pass-fail', 'arr_value']].dropna()\n",
    "#======\n",
    "\n",
    "\n",
    "#======\n",
    "#Adding numbers to arr_value, PROBLEM\n",
    "list_of_prob_types = list(np.unique(prob_dataset['problem_type'])[0:])\n",
    "temp_datasets = []\n",
    "for prob_type in list_of_prob_types:\n",
    "    temp_dataset = prob_dataset[prob_dataset.problem_type == prob_type].copy()\n",
    "    \n",
    "    if prob_type == 'Assignment Part':\n",
    "        temp_dataset = temp_dataset.dropna()\n",
    "        temp_dataset['arr_value'] = temp_dataset['grade']\n",
    "    elif prob_type == 'Quiz':\n",
    "        temp_dataset['arr_value'] = 1\n",
    "    else:\n",
    "        print(\"New Problem type Found!!\")\n",
    "    temp_datasets.append(temp_dataset)\n",
    "    \n",
    "    \n",
    "merged_prob = pd.concat(temp_datasets)\n",
    "prob_dataset = merged_prob[['user_id', 'problem_id', 'problem_type', 'timestamp', 'submission_number', 'arr_value']].dropna()\n",
    "#======\n",
    "\n",
    "\n",
    "prob_group = prob_dataset.groupby('user_id').agg(list)\n",
    "video_group = video_dataset.groupby('user_id').agg(list)\n",
    "\n",
    "merged = video_group.merge(prob_group, on='user_id', how='inner', suffixes=['_video', '_prob'])\n",
    "\n",
    "merged['video_length'] = merged['timestamp_video'].apply(len)\n",
    "merged['prob_length'] = merged['timestamp_prob'].apply(len)\n",
    "merged['timestamp'] = merged['timestamp_video'] + merged['timestamp_prob']\n",
    "merged['event_type'] = merged['event_type'] + merged['problem_type']\n",
    "merged['arr_value'] = merged['arr_value_video'] + merged['arr_value_prob']\n",
    "\n",
    "dataset = merged.drop(columns=['timestamp_video', 'timestamp_prob', 'arr_value_video', 'arr_value_prob', 'problem_type'])\n",
    "\n",
    "list_of_events = list_of_video_events + list_of_prob_types\n",
    "\n",
    "selected_users = dataset[(dataset['video_length'] + dataset['prob_length'] > min_len) & (dataset['video_length'] + dataset['prob_length'] < max_len)]\n",
    "selected_users = selected_users.reset_index(level=0)\n",
    "selected_users['timestamp'] = selected_users['timestamp'].apply(lambda x: np.pad(x, (0, max_len-len(x)), 'constant', constant_values=(0, 0)))\n",
    "selected_users['event_type'] = selected_users['event_type'].apply(lambda x: [list_of_events.index(i) for i in x])\n",
    "selected_users['pass-fail'] = selected_users['pass-fail'].apply(lambda x: 1 if x[0]=='Passed' else 0)\n",
    "\n",
    "P_users = []\n",
    "y_users = []\n",
    "for index, row in selected_users.iterrows():\n",
    "    user_id = row['user_id']\n",
    "    time = row['timestamp']\n",
    "    events = row['event_type']\n",
    "    arr = np.zeros([max_len, len(list_of_events)+2])\n",
    "    arr[np.arange(len(events)), events] = row['arr_value']\n",
    "    arr[:, [0, 1, 2, 3, 4, 5, 6, 7, 8]] = arr[:, [0, 1, 2, 3, 4, 5, 6, 7, 9]]\n",
    "    arr[:, 9] = 0\n",
    "    arr[:row['video_length'], len(list_of_events)-1] = row['video_id']\n",
    "    arr[row['video_length']:row['video_length']+row['prob_length'], len(list_of_events)] = row['problem_id']\n",
    "    arr[row['video_length']:row['video_length']+row['prob_length'], len(list_of_events)+1] = row['submission_number']\n",
    "    \n",
    "    indices = np.argsort(time)\n",
    "    \n",
    "    P_users.append({'id': user_id, \n",
    "                    'static': tuple([0, 0, 0, 0, 0, 0]), \n",
    "                    'extended_static': [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                    'arr': arr[indices],\n",
    "                    'time': np.reshape(time[indices], (max_len, 1)), \n",
    "                    'length': len(events)\n",
    "                    })\n",
    "    \n",
    "    y_users.append(row['pass-fail'])\n",
    "    \n",
    "    del arr\n",
    "    \n",
    "y_users = np.reshape(np.array(y_users), (len(y_users), 1))\n",
    "\n",
    "np.save(f\"{filename.split('.')[0]}_data.npy\", P_users)\n",
    "np.save(f\"{filename.split('.')[0]}_y.npy\", y_users)\n",
    "\n",
    "print(f\"Dataset: {filename} \\n Users: {len(P_users)} \\n 1/all ratio: {sum(y_users)/len(y_users)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16451249",
   "metadata": {},
   "source": [
    "# Testing without assignment (and quizes) at all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74492780",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"progfun-002.csv\"\n",
    "coursetype = \"mooc/coursera\"\n",
    "saved_filename = filename.split('.')[0]\n",
    "\n",
    "video_dataset = pd.read_csv(f\"F:/SeFT4ED/DATASETS/{coursetype}/video_event/{filename}\")\n",
    "outcome_dataset = pd.read_csv(f\"F:/SeFT4ED/DATASETS/{coursetype}/grade/{filename}\")\n",
    "\n",
    "prob_dataset = pd.read_csv(f\"F:/SeFT4ED/DATASETS/{coursetype}/problem_event/{filename}\")\n",
    "\n",
    "video_dataset = video_dataset.merge(outcome_dataset, on='user_id', how='inner')\n",
    "if 'XXX-' in filename:\n",
    "    dataset['pass-fail'] = dataset['grade'].apply(lambda x: 'Passed' if x>=4 else 'Failed')\n",
    "\n",
    "min_len = 5\n",
    "max_len = 215\n",
    "\n",
    "#======\n",
    "#Adding numbers to arr_value, VIDEO\n",
    "list_of_video_events = list(np.unique(video_dataset['event_type']))\n",
    "temp_datasets = []\n",
    "for event_type in list_of_video_events:\n",
    "    temp_dataset = video_dataset[video_dataset.event_type == event_type].copy()\n",
    "    if event_type in ['Video.Error', 'Video.Pause', 'Video.Play']:\n",
    "        temp_dataset['arr_value'] = temp_dataset['current_time']\n",
    "    elif event_type in ['Video.Seek', 'Video.Stalled']:\n",
    "        temp_dataset['arr_value'] = temp_dataset['current_time'] - temp_dataset['old_time']\n",
    "    elif event_type == 'Video.SpeedChange':\n",
    "        temp_dataset['arr_value'] = temp_dataset['new_speed']\n",
    "    else:\n",
    "        temp_dataset['arr_value'] = 1\n",
    "    temp_datasets.append(temp_dataset)\n",
    "    \n",
    "merged_video = pd.concat(temp_datasets)\n",
    "video_dataset = merged_video[['user_id', 'video_id', 'event_type', 'timestamp', 'pass-fail', 'arr_value']].dropna()\n",
    "#======\n",
    "\n",
    "\n",
    "#======\n",
    "#Adding numbers to arr_value, PROBLEM\n",
    "list_of_prob_types = list(np.unique(prob_dataset['problem_type'])[0:])\n",
    "temp_datasets = []\n",
    "for prob_type in list_of_prob_types:\n",
    "    temp_dataset = prob_dataset[prob_dataset.problem_type == prob_type].copy()\n",
    "    \n",
    "    if prob_type == 'Assignment Part':\n",
    "        temp_dataset = temp_dataset.dropna()\n",
    "        temp_dataset['arr_value'] = temp_dataset['grade']\n",
    "    elif prob_type == 'Quiz':\n",
    "        temp_dataset['arr_value'] = 1\n",
    "    else:\n",
    "        print(\"New Problem type Found!!\")\n",
    "    temp_datasets.append(temp_dataset)\n",
    "    \n",
    "    \n",
    "merged_prob = pd.concat(temp_datasets)\n",
    "prob_dataset = merged_prob[['user_id', 'problem_id', 'problem_type', 'timestamp', 'submission_number', 'arr_value']].dropna()\n",
    "#======\n",
    "\n",
    "\n",
    "prob_group = prob_dataset.groupby('user_id').agg(list)\n",
    "video_group = video_dataset.groupby('user_id').agg(list)\n",
    "\n",
    "merged = video_group.merge(prob_group, on='user_id', how='inner', suffixes=['_video', '_prob'])\n",
    "\n",
    "merged['video_length'] = merged['timestamp_video'].apply(len)\n",
    "merged['prob_length'] = merged['timestamp_prob'].apply(len)\n",
    "merged['timestamp'] = merged['timestamp_video'] + merged['timestamp_prob']\n",
    "merged['event_type'] = merged['event_type'] + merged['problem_type']\n",
    "merged['arr_value'] = merged['arr_value_video'] + merged['arr_value_prob']\n",
    "\n",
    "dataset = merged.drop(columns=['timestamp_video', 'timestamp_prob', 'arr_value_video', 'arr_value_prob', 'problem_type'])\n",
    "\n",
    "list_of_events = list_of_video_events + list_of_prob_types\n",
    "\n",
    "selected_users = dataset[(dataset['video_length'] + dataset['prob_length'] > min_len) & (dataset['video_length'] + dataset['prob_length'] < max_len)]\n",
    "selected_users = selected_users.reset_index(level=0)\n",
    "selected_users['timestamp'] = selected_users['timestamp'].apply(lambda x: np.pad(x, (0, max_len-len(x)), 'constant', constant_values=(0, 0)))\n",
    "selected_users['event_type'] = selected_users['event_type'].apply(lambda x: [list_of_events.index(i) for i in x])\n",
    "selected_users['pass-fail'] = selected_users['pass-fail'].apply(lambda x: 1 if x[0]=='Passed' else 0)\n",
    "\n",
    "P_users = []\n",
    "y_users = []\n",
    "for index, row in selected_users.iterrows():\n",
    "    user_id = row['user_id']\n",
    "    time = row['timestamp']\n",
    "    events = row['event_type']\n",
    "    arr = np.zeros([max_len, len(list_of_events)+1])\n",
    "    arr[np.arange(len(events)), events] = row['arr_value']\n",
    "    #arr[:, [0, 1, 2, 3, 4, 5, 6, 7, 8]] = arr[:, [0, 1, 2, 3, 4, 5, 6, 7, 9]]\n",
    "    arr[:, 8] = 0\n",
    "    arr[:, 9] = 0\n",
    "    arr[:row['video_length'], len(list_of_events)-2] = row['video_id']\n",
    "    arr[row['video_length']:row['video_length']+row['prob_length'], len(list_of_events)-1] = row['problem_id']\n",
    "    arr[row['video_length']:row['video_length']+row['prob_length'], len(list_of_events)] = row['submission_number']\n",
    "    \n",
    "    indices = np.argsort(time)\n",
    "    \n",
    "    P_users.append({'id': user_id, \n",
    "                    'static': tuple([0, 0, 0, 0, 0, 0]), \n",
    "                    'extended_static': [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                    'arr': arr[indices],\n",
    "                    'time': np.reshape(time[indices], (max_len, 1)), \n",
    "                    'length': len(events)\n",
    "                    })\n",
    "    \n",
    "    y_users.append(row['pass-fail'])\n",
    "y_users = np.reshape(np.array(y_users), (len(y_users), 1))\n",
    "\n",
    "np.save(f\"{filename.split('.')[0]}_data.npy\", P_users)\n",
    "np.save(f\"{filename.split('.')[0]}_y.npy\", y_users)\n",
    "\n",
    "print(f\"Dataset: {filename} \\n Users: {len(P_users)} \\n 1/all ratio: {sum(y_users)/len(y_users)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f6967c",
   "metadata": {},
   "source": [
    "# Getting first x%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ee29d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"progfun-002.csv\"\n",
    "coursetype = \"mooc/coursera\"\n",
    "saved_filename = filename.split('.')[0]\n",
    "\n",
    "meta_dataset = pd.read_csv('F:\\SeFT4ED\\DATASETS\\mooc\\metadata.csv')\n",
    "meta_dataset = meta_dataset[meta_dataset.course_id==saved_filename]\n",
    "\n",
    "start_timestamp = datetime.datetime.strptime(meta_dataset.start_date.to_numpy()[0], '%Y-%m-%d %H:%M:%S')\n",
    "start_timestamp = start_timestamp.timestamp()\n",
    "end_timestamp = datetime.datetime.strptime(meta_dataset.end_date.to_numpy()[0], '%Y-%m-%d %H:%M:%S')\n",
    "end_timestamp = end_timestamp.timestamp()\n",
    "\n",
    "x_percent = 0.4\n",
    "x_deadline = start_timestamp + x_percent*(end_timestamp-start_timestamp)\n",
    "\n",
    "video_dataset = pd.read_csv(f\"F:/SeFT4ED/DATASETS/{coursetype}/video_event/{filename}\")\n",
    "outcome_dataset = pd.read_csv(f\"F:/SeFT4ED/DATASETS/{coursetype}/grade/{filename}\")\n",
    "\n",
    "prob_dataset = pd.read_csv(f\"F:/SeFT4ED/DATASETS/{coursetype}/problem_event/{filename}\")\n",
    "\n",
    "video_dataset = video_dataset[(video_dataset.timestamp >= start_timestamp) & (video_dataset.timestamp <= x_deadline)]\n",
    "prob_dataset = prob_dataset[(prob_dataset.timestamp >= start_timestamp) & (prob_dataset.timestamp <= x_deadline)]\n",
    "\n",
    "video_dataset = video_dataset.merge(outcome_dataset, on='user_id', how='inner')\n",
    "if 'XXX-' in filename:\n",
    "    dataset['pass-fail'] = dataset['grade'].apply(lambda x: 'Passed' if x>=4 else 'Failed')\n",
    "\n",
    "min_len = 5\n",
    "max_len = 215\n",
    "\n",
    "#======\n",
    "#Adding numbers to arr_value, VIDEO\n",
    "list_of_video_events = list(np.unique(video_dataset['event_type']))\n",
    "temp_datasets = []\n",
    "for event_type in list_of_video_events:\n",
    "    temp_dataset = video_dataset[video_dataset.event_type == event_type].copy()\n",
    "    if event_type in ['Video.Error', 'Video.Pause', 'Video.Play']:\n",
    "        temp_dataset['arr_value'] = temp_dataset['current_time']\n",
    "    elif event_type in ['Video.Seek', 'Video.Stalled']:\n",
    "        temp_dataset['arr_value'] = temp_dataset['current_time'] - temp_dataset['old_time']\n",
    "    elif event_type == 'Video.SpeedChange':\n",
    "        temp_dataset['arr_value'] = temp_dataset['new_speed']\n",
    "    else:\n",
    "        temp_dataset['arr_value'] = 1\n",
    "    temp_datasets.append(temp_dataset)\n",
    "    \n",
    "merged_video = pd.concat(temp_datasets)\n",
    "video_dataset = merged_video[['user_id', 'video_id', 'event_type', 'timestamp', 'pass-fail', 'arr_value']].dropna()\n",
    "#======\n",
    "\n",
    "\n",
    "#======\n",
    "#Adding numbers to arr_value, PROBLEM\n",
    "list_of_prob_types = list(np.unique(prob_dataset['problem_type']))\n",
    "temp_datasets = []\n",
    "for prob_type in list_of_prob_types:\n",
    "    temp_dataset = prob_dataset[prob_dataset.problem_type == prob_type].copy()\n",
    "    \n",
    "    if prob_type == 'Assignment Part':\n",
    "        temp_dataset = temp_dataset.dropna()\n",
    "        temp_dataset['arr_value'] = temp_dataset['grade']\n",
    "    elif prob_type == 'Quiz':\n",
    "        temp_dataset['arr_value'] = 1\n",
    "    else:\n",
    "        print(\"New Problem type Found!!\")\n",
    "    temp_datasets.append(temp_dataset)\n",
    "    \n",
    "    \n",
    "merged_prob = pd.concat(temp_datasets)\n",
    "prob_dataset = merged_prob[['user_id', 'problem_id', 'problem_type', 'timestamp', 'submission_number', 'arr_value']].dropna()\n",
    "#======\n",
    "\n",
    "\n",
    "prob_group = prob_dataset.groupby('user_id').agg(list)\n",
    "video_group = video_dataset.groupby('user_id').agg(list)\n",
    "\n",
    "merged = video_group.merge(prob_group, on='user_id', how='inner', suffixes=['_video', '_prob'])\n",
    "\n",
    "\n",
    "merged['video_length'] = merged['timestamp_video'].apply(len)\n",
    "merged['prob_length'] = merged['timestamp_prob'].apply(len)\n",
    "merged['timestamp'] = merged['timestamp_video'] + merged['timestamp_prob']\n",
    "merged['event_type'] = merged['event_type'] + merged['problem_type']\n",
    "merged['arr_value'] = merged['arr_value_video'] + merged['arr_value_prob']\n",
    "\n",
    "dataset = merged.drop(columns=['timestamp_video', 'timestamp_prob', 'arr_value_video', 'arr_value_prob', 'problem_type'])\n",
    "\n",
    "list_of_events = list_of_video_events + list_of_prob_types\n",
    "selected_users = dataset[(dataset['video_length'] + dataset['prob_length'] > min_len) & (dataset['video_length'] + dataset['prob_length'] < max_len)]\n",
    "selected_users = selected_users.reset_index(level=0)\n",
    "selected_users['timestamp'] = selected_users['timestamp'].apply(lambda x: np.pad(x, (0, max_len-len(x)), 'constant', constant_values=(0, 0)))\n",
    "selected_users['event_type'] = selected_users['event_type'].apply(lambda x: [list_of_events.index(i) for i in x])\n",
    "selected_users['pass-fail'] = selected_users['pass-fail'].apply(lambda x: 1 if x[0]=='Passed' else 0)\n",
    "\n",
    "P_users = []\n",
    "y_users = []\n",
    "for index, row in selected_users.iterrows():\n",
    "    user_id = row['user_id']\n",
    "    time = row['timestamp']\n",
    "    events = row['event_type']\n",
    "    arr = np.zeros([max_len, len(list_of_events)+3])\n",
    "    arr[np.arange(len(events)), events] = row['arr_value']\n",
    "    arr[:row['video_length'], len(list_of_events)] = row['video_id']\n",
    "    arr[row['video_length']:row['video_length']+row['prob_length'], len(list_of_events)+1] = row['problem_id']\n",
    "    arr[row['video_length']:row['video_length']+row['prob_length'], len(list_of_events)+2] = row['submission_number']\n",
    "    \n",
    "    indices = np.argsort(time)\n",
    "    \n",
    "    P_users.append({'id': user_id, \n",
    "                    'static': tuple([0, 0, 0, 0, 0, 0]), \n",
    "                    'extended_static': [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                    'arr': arr[indices],\n",
    "                    'time': np.reshape(time[indices], (max_len, 1)), \n",
    "                    'length': len(events)\n",
    "                    })\n",
    "    \n",
    "    y_users.append(row['pass-fail'])\n",
    "y_users = np.reshape(np.array(y_users), (len(y_users), 1))\n",
    "\n",
    "np.save(f\"{filename.split('.')[0]}_data.npy\", P_users)\n",
    "np.save(f\"{filename.split('.')[0]}_y.npy\", y_users)\n",
    "\n",
    "print(f\"Dataset: {filename} \\n Users: {len(P_users)} \\n 1/all ratio: {sum(y_users)/len(y_users)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f2d628",
   "metadata": {},
   "source": [
    "# Getting first x% without assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7be9c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"progfun-002.csv\"\n",
    "coursetype = \"mooc/coursera\"\n",
    "saved_filename = filename.split('.')[0]\n",
    "\n",
    "meta_dataset = pd.read_csv('F:\\SeFT4ED\\DATASETS\\mooc\\metadata.csv')\n",
    "meta_dataset = meta_dataset[meta_dataset.course_id==saved_filename]\n",
    "\n",
    "start_timestamp = datetime.datetime.strptime(meta_dataset.start_date.to_numpy()[0], '%Y-%m-%d %H:%M:%S')\n",
    "start_timestamp = start_timestamp.timestamp()\n",
    "end_timestamp = datetime.datetime.strptime(meta_dataset.end_date.to_numpy()[0], '%Y-%m-%d %H:%M:%S')\n",
    "end_timestamp = end_timestamp.timestamp()\n",
    "\n",
    "x_percent = 1\n",
    "x_deadline = start_timestamp + x_percent*(end_timestamp-start_timestamp)\n",
    "\n",
    "video_dataset = pd.read_csv(f\"F:/SeFT4ED/DATASETS/{coursetype}/video_event/{filename}\")\n",
    "outcome_dataset = pd.read_csv(f\"F:/SeFT4ED/DATASETS/{coursetype}/grade/{filename}\")\n",
    "\n",
    "prob_dataset = pd.read_csv(f\"F:/SeFT4ED/DATASETS/{coursetype}/problem_event/{filename}\")\n",
    "\n",
    "video_dataset = video_dataset[(video_dataset.timestamp >= start_timestamp) & (video_dataset.timestamp <= x_deadline)]\n",
    "prob_dataset = prob_dataset[(prob_dataset.timestamp >= start_timestamp) & (prob_dataset.timestamp <= x_deadline)]\n",
    "\n",
    "video_dataset = video_dataset.merge(outcome_dataset, on='user_id', how='inner')\n",
    "if 'XXX-' in filename:\n",
    "    dataset['pass-fail'] = dataset['grade'].apply(lambda x: 'Passed' if x>=4 else 'Failed')\n",
    "\n",
    "min_len = 5\n",
    "max_len = 215\n",
    "\n",
    "#======\n",
    "#Adding numbers to arr_value, VIDEO\n",
    "list_of_video_events = list(np.unique(video_dataset['event_type']))\n",
    "temp_datasets = []\n",
    "for event_type in list_of_video_events:\n",
    "    temp_dataset = video_dataset[video_dataset.event_type == event_type].copy()\n",
    "    if event_type in ['Video.Error', 'Video.Pause', 'Video.Play']:\n",
    "        temp_dataset['arr_value'] = temp_dataset['current_time']\n",
    "    elif event_type in ['Video.Seek', 'Video.Stalled']:\n",
    "        temp_dataset['arr_value'] = temp_dataset['current_time'] - temp_dataset['old_time']\n",
    "    elif event_type == 'Video.SpeedChange':\n",
    "        temp_dataset['arr_value'] = temp_dataset['new_speed']\n",
    "    else:\n",
    "        temp_dataset['arr_value'] = 1\n",
    "    temp_datasets.append(temp_dataset)\n",
    "    \n",
    "merged_video = pd.concat(temp_datasets)\n",
    "video_dataset = merged_video[['user_id', 'video_id', 'event_type', 'timestamp', 'pass-fail', 'arr_value']].dropna()\n",
    "#======\n",
    "\n",
    "\n",
    "#======\n",
    "#Adding numbers to arr_value, PROBLEM\n",
    "list_of_prob_types = list(np.unique(prob_dataset['problem_type'])[0:])\n",
    "temp_datasets = []\n",
    "for prob_type in list_of_prob_types:\n",
    "    temp_dataset = prob_dataset[prob_dataset.problem_type == prob_type].copy()\n",
    "    \n",
    "    if prob_type == 'Assignment Part':\n",
    "        temp_dataset = temp_dataset.dropna()\n",
    "        temp_dataset['arr_value'] = temp_dataset['grade']\n",
    "    elif prob_type == 'Quiz':\n",
    "        temp_dataset['arr_value'] = 1\n",
    "    else:\n",
    "        print(\"New Problem type Found!!\")\n",
    "    temp_datasets.append(temp_dataset)\n",
    "    \n",
    "    \n",
    "merged_prob = pd.concat(temp_datasets)\n",
    "prob_dataset = merged_prob[['user_id', 'problem_id', 'problem_type', 'timestamp', 'submission_number', 'arr_value']].dropna()\n",
    "#======\n",
    "\n",
    "\n",
    "prob_group = prob_dataset.groupby('user_id').agg(list)\n",
    "video_group = video_dataset.groupby('user_id').agg(list)\n",
    "\n",
    "merged = video_group.merge(prob_group, on='user_id', how='inner', suffixes=['_video', '_prob'])\n",
    "\n",
    "merged['video_length'] = merged['timestamp_video'].apply(len)\n",
    "merged['prob_length'] = merged['timestamp_prob'].apply(len)\n",
    "merged['timestamp'] = merged['timestamp_video'] + merged['timestamp_prob']\n",
    "merged['event_type'] = merged['event_type'] + merged['problem_type']\n",
    "merged['arr_value'] = merged['arr_value_video'] + merged['arr_value_prob']\n",
    "\n",
    "dataset = merged.drop(columns=['timestamp_video', 'timestamp_prob', 'arr_value_video', 'arr_value_prob', 'problem_type'])\n",
    "\n",
    "list_of_events = list_of_video_events + list_of_prob_types\n",
    "\n",
    "selected_users = dataset[(dataset['video_length'] + dataset['prob_length'] > min_len) & (dataset['video_length'] + dataset['prob_length'] < max_len)]\n",
    "selected_users = selected_users.reset_index(level=0)\n",
    "selected_users['timestamp'] = selected_users['timestamp'].apply(lambda x: np.pad(x, (0, max_len-len(x)), 'constant', constant_values=(0, 0)))\n",
    "selected_users['event_type'] = selected_users['event_type'].apply(lambda x: [list_of_events.index(i) for i in x])\n",
    "selected_users['pass-fail'] = selected_users['pass-fail'].apply(lambda x: 1 if x[0]=='Passed' else 0)\n",
    "\n",
    "P_users = []\n",
    "y_users = []\n",
    "for index, row in selected_users.iterrows():\n",
    "    user_id = row['user_id']\n",
    "    time = row['timestamp']\n",
    "    events = row['event_type']\n",
    "    arr = np.zeros([max_len, len(list_of_events)+2])\n",
    "    arr[np.arange(len(events)), events] = row['arr_value']\n",
    "    arr[:, [0, 1, 2, 3, 4, 5, 6, 7, 8]] = arr[:, [0, 1, 2, 3, 4, 5, 6, 7, 9]]\n",
    "    arr[:, 9] = 0\n",
    "    arr[:row['video_length'], len(list_of_events)-1] = row['video_id']\n",
    "    arr[row['video_length']:row['video_length']+row['prob_length'], len(list_of_events)] = row['problem_id']\n",
    "    arr[row['video_length']:row['video_length']+row['prob_length'], len(list_of_events)+1] = row['submission_number']\n",
    "    \n",
    "    indices = np.argsort(time)\n",
    "    \n",
    "    P_users.append({'id': user_id, \n",
    "                    'static': tuple([0, 0, 0, 0, 0, 0]), \n",
    "                    'extended_static': [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                    'arr': arr[indices],\n",
    "                    'time': np.reshape(time[indices], (max_len, 1)), \n",
    "                    'length': len(events)\n",
    "                    })\n",
    "    \n",
    "    y_users.append(row['pass-fail'])\n",
    "y_users = np.reshape(np.array(y_users), (len(y_users), 1))\n",
    "\n",
    "np.save(f\"{filename.split('.')[0]}_data.npy\", P_users)\n",
    "np.save(f\"{filename.split('.')[0]}_y.npy\", y_users)\n",
    "\n",
    "print(f\"Dataset: {filename} \\n Users: {len(P_users)} \\n 1/all ratio: {sum(y_users)/len(y_users)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1f60b9",
   "metadata": {},
   "source": [
    "# Getting first x% with removing easy-fails (every event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6e7d58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filename = \"progfun-002.csv\"\n",
    "coursetype = \"mooc/coursera\"\n",
    "saved_filename = filename.split('.')[0]\n",
    "\n",
    "marras_feats = pd.read_csv(f'F:/SeFT4ED_2/data/result/easy-fail/eq_week-marras_et_al-{saved_filename.replace(\"-\", \"_\")}/feature_labels.csv')\n",
    "number_id_mapping = pd.read_csv(f'F:/SeFT4ED_2/data/result/easy-fail/user_id_mapping-{saved_filename.replace(\"-\", \"_\")}.csv')\n",
    "hard_fail = marras_feats.merge(number_id_mapping, on='Unnamed: 0', how='inner')['user_id']\n",
    "\n",
    "\n",
    "meta_dataset = pd.read_csv('F:\\SeFT4ED\\DATASETS\\mooc\\metadata.csv')\n",
    "meta_dataset = meta_dataset[meta_dataset.course_id==saved_filename]\n",
    "\n",
    "start_timestamp = datetime.datetime.strptime(meta_dataset.start_date.to_numpy()[0], '%Y-%m-%d %H:%M:%S')\n",
    "start_timestamp = start_timestamp.timestamp()\n",
    "end_timestamp = datetime.datetime.strptime(meta_dataset.end_date.to_numpy()[0], '%Y-%m-%d %H:%M:%S')\n",
    "end_timestamp = end_timestamp.timestamp()\n",
    "\n",
    "x_percent = 0.4\n",
    "x_deadline = start_timestamp + x_percent*(end_timestamp-start_timestamp)\n",
    "\n",
    "video_dataset = pd.read_csv(f\"F:/SeFT4ED_2/data/course/{coursetype}/video_event/{filename}\")\n",
    "video_dataset = video_dataset.merge(hard_fail, on='user_id', how='inner')\n",
    "outcome_dataset = pd.read_csv(f\"F:/SeFT4ED_2/data/course/{coursetype}/grade/{filename}\")\n",
    "outcome_dataset = outcome_dataset.merge(hard_fail, on='user_id', how='inner')\n",
    "\n",
    "prob_dataset = pd.read_csv(f\"F:/SeFT4ED_2/data/course/{coursetype}/problem_event/{filename}\")\n",
    "prob_dataset = prob_dataset.merge(hard_fail, on='user_id', how='inner')\n",
    "\n",
    "video_dataset = video_dataset[(video_dataset.timestamp >= start_timestamp) & (video_dataset.timestamp <= x_deadline)]\n",
    "prob_dataset = prob_dataset[(prob_dataset.timestamp >= start_timestamp) & (prob_dataset.timestamp <= x_deadline)]\n",
    "\n",
    "video_dataset = video_dataset.merge(outcome_dataset, on='user_id', how='inner')\n",
    "if 'XXX-' in filename:\n",
    "    dataset['pass-fail'] = dataset['grade'].apply(lambda x: 'Passed' if x>=4 else 'Failed')\n",
    "\n",
    "min_len = 5\n",
    "max_len = 1000\n",
    "\n",
    "#======\n",
    "#Adding numbers to arr_value, VIDEO\n",
    "list_of_video_events = list(np.unique(video_dataset['event_type']))\n",
    "temp_datasets = []\n",
    "for event_type in list_of_video_events:\n",
    "    temp_dataset = video_dataset[video_dataset.event_type == event_type].copy()\n",
    "    if event_type in ['Video.Error', 'Video.Pause', 'Video.Play']:\n",
    "        temp_dataset['arr_value'] = temp_dataset['current_time']\n",
    "    elif event_type in ['Video.Seek', 'Video.Stalled']:\n",
    "        temp_dataset['arr_value'] = temp_dataset['current_time'] - temp_dataset['old_time']\n",
    "    elif event_type == 'Video.SpeedChange':\n",
    "        temp_dataset['arr_value'] = temp_dataset['new_speed']\n",
    "    else:\n",
    "        temp_dataset['arr_value'] = 1\n",
    "    temp_datasets.append(temp_dataset)\n",
    "    \n",
    "merged_video = pd.concat(temp_datasets)\n",
    "video_dataset = merged_video[['user_id', 'video_id', 'event_type', 'timestamp', 'pass-fail', 'arr_value']].dropna()\n",
    "#======\n",
    "\n",
    "\n",
    "#======\n",
    "#Adding numbers to arr_value, PROBLEM\n",
    "list_of_prob_types = list(np.unique(prob_dataset['problem_type']))\n",
    "temp_datasets = []\n",
    "for prob_type in list_of_prob_types:\n",
    "    temp_dataset = prob_dataset[prob_dataset.problem_type == prob_type].copy()\n",
    "    \n",
    "    if prob_type == 'Assignment Part':\n",
    "        temp_dataset = temp_dataset.dropna()\n",
    "        temp_dataset['arr_value'] = temp_dataset['grade']\n",
    "    elif prob_type == 'Quiz':\n",
    "        temp_dataset['arr_value'] = 1\n",
    "    else:\n",
    "        print(\"New Problem type Found!!\")\n",
    "    temp_datasets.append(temp_dataset)\n",
    "    \n",
    "    \n",
    "merged_prob = pd.concat(temp_datasets)\n",
    "prob_dataset = merged_prob[['user_id', 'problem_id', 'problem_type', 'timestamp', 'submission_number', 'arr_value']].dropna()\n",
    "#======\n",
    "\n",
    "\n",
    "prob_group = prob_dataset.groupby('user_id').agg(list)\n",
    "video_group = video_dataset.groupby('user_id').agg(list)\n",
    "\n",
    "merged = video_group.merge(prob_group, on='user_id', how='inner', suffixes=['_video', '_prob'])\n",
    "\n",
    "\n",
    "merged['video_length'] = merged['timestamp_video'].apply(len)\n",
    "merged['prob_length'] = merged['timestamp_prob'].apply(len)\n",
    "merged['timestamp'] = merged['timestamp_video'] + merged['timestamp_prob']\n",
    "merged['event_type'] = merged['event_type'] + merged['problem_type']\n",
    "merged['arr_value'] = merged['arr_value_video'] + merged['arr_value_prob']\n",
    "\n",
    "dataset = merged.drop(columns=['timestamp_video', 'timestamp_prob', 'arr_value_video', 'arr_value_prob', 'problem_type'])\n",
    "\n",
    "list_of_events = list_of_video_events + list_of_prob_types\n",
    "\n",
    "selected_users = dataset[(dataset['video_length'] + dataset['prob_length'] > min_len) & (dataset['video_length'] + dataset['prob_length'] < max_len)]\n",
    "selected_users = selected_users.reset_index(level=0)\n",
    "selected_users['timestamp'] = selected_users['timestamp'].apply(lambda x: np.pad(x, (0, max_len-len(x)), 'constant', constant_values=(0, 0)))\n",
    "selected_users['event_type'] = selected_users['event_type'].apply(lambda x: [list_of_events.index(i) for i in x])\n",
    "selected_users['pass-fail'] = selected_users['pass-fail'].apply(lambda x: 1 if x[0]=='Passed' else 0)\n",
    "\n",
    "P_users = []\n",
    "y_users = []\n",
    "for index, row in selected_users.iterrows():\n",
    "    user_id = row['user_id']\n",
    "    time = row['timestamp']\n",
    "    events = row['event_type']\n",
    "    arr = np.zeros([max_len, len(list_of_events)+3])\n",
    "    arr[np.arange(len(events)), events] = row['arr_value']\n",
    "    arr[:row['video_length'], len(list_of_events)] = row['video_id']\n",
    "    arr[row['video_length']:row['video_length']+row['prob_length'], len(list_of_events)+1] = row['problem_id']\n",
    "    arr[row['video_length']:row['video_length']+row['prob_length'], len(list_of_events)+2] = row['submission_number']\n",
    "    \n",
    "    indices = np.argsort(time)\n",
    "    \n",
    "    P_users.append({'id': user_id, \n",
    "                    'static': tuple([0, 0, 0, 0, 0, 0]), \n",
    "                    'extended_static': [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                    'arr': arr[indices],\n",
    "                    'time': np.reshape(time[indices], (max_len, 1)), \n",
    "                    'length': len(events)\n",
    "                    })\n",
    "    \n",
    "    y_users.append(row['pass-fail'])\n",
    "y_users = np.reshape(np.array(y_users), (len(y_users), 1))\n",
    "\n",
    "np.save(f\"{filename.split('.')[0]}_data_hard_fail.npy\", P_users)\n",
    "np.save(f\"{filename.split('.')[0]}_y_hard_fail.npy\", y_users)\n",
    "\n",
    "print(f\"Dataset: {filename} \\n Users: {len(P_users)} \\n 1/all ratio: {sum(y_users)/len(y_users)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a26cda4",
   "metadata": {},
   "source": [
    "# First x%, easy-fail removed, truncate to max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526261d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE\n",
    "# Note, for now I'm assuming prob_length is always less than max_len\n",
    "\n",
    "filename = \"progfun-002.csv\"\n",
    "coursetype = \"mooc/coursera\"\n",
    "saved_filename = filename.split('.')[0]\n",
    "\n",
    "marras_feats = pd.read_csv(f'F:/SeFT4ED_2/data/result/easy-fail/eq_week-marras_et_al-{saved_filename.replace(\"-\", \"_\")}/feature_labels.csv')\n",
    "number_id_mapping = pd.read_csv(f'F:/SeFT4ED_2/data/result/easy-fail/user_id_mapping-{saved_filename.replace(\"-\", \"_\")}.csv')\n",
    "hard_fail = marras_feats.merge(number_id_mapping, on='Unnamed: 0', how='inner')['user_id']\n",
    "\n",
    "\n",
    "meta_dataset = pd.read_csv('F:\\SeFT4ED\\DATASETS\\mooc\\metadata.csv')\n",
    "meta_dataset = meta_dataset[meta_dataset.course_id==saved_filename]\n",
    "\n",
    "start_timestamp = datetime.datetime.strptime(meta_dataset.start_date.to_numpy()[0], '%Y-%m-%d %H:%M:%S')\n",
    "start_timestamp = start_timestamp.timestamp()\n",
    "end_timestamp = datetime.datetime.strptime(meta_dataset.end_date.to_numpy()[0], '%Y-%m-%d %H:%M:%S')\n",
    "end_timestamp = end_timestamp.timestamp()\n",
    "\n",
    "x_percent = 1\n",
    "x_deadline = start_timestamp + x_percent*(end_timestamp-start_timestamp)\n",
    "\n",
    "video_dataset = pd.read_csv(f\"F:/SeFT4ED_2/data/course/{coursetype}/video_event/{filename}\")\n",
    "video_dataset = video_dataset.merge(hard_fail, on='user_id', how='inner')\n",
    "outcome_dataset = pd.read_csv(f\"F:/SeFT4ED_2/data/course/{coursetype}/grade/{filename}\")\n",
    "outcome_dataset = outcome_dataset.merge(hard_fail, on='user_id', how='inner')\n",
    "\n",
    "prob_dataset = pd.read_csv(f\"F:/SeFT4ED_2/data/course/{coursetype}/problem_event/{filename}\")\n",
    "prob_dataset = prob_dataset.merge(hard_fail, on='user_id', how='inner')\n",
    "\n",
    "video_dataset = video_dataset[(video_dataset.timestamp >= start_timestamp) & (video_dataset.timestamp <= x_deadline)]\n",
    "prob_dataset = prob_dataset[(prob_dataset.timestamp >= start_timestamp) & (prob_dataset.timestamp <= x_deadline)]\n",
    "\n",
    "#video_dataset = video_dataset.merge(outcome_dataset, on='user_id', how='outer')\n",
    "if 'XXX-' in filename:\n",
    "    dataset['pass-fail'] = dataset['grade'].apply(lambda x: 'Passed' if x>=4 else 'Failed')\n",
    "\n",
    "min_len = 5\n",
    "max_len = 215\n",
    "\n",
    "#======\n",
    "#Adding numbers to arr_value, VIDEO\n",
    "list_of_video_events = list(np.unique(video_dataset['event_type']))\n",
    "temp_datasets = []\n",
    "for event_type in list_of_video_events:\n",
    "    temp_dataset = video_dataset[video_dataset.event_type == event_type].copy()\n",
    "    if event_type in ['Video.Error', 'Video.Pause', 'Video.Play']:\n",
    "        temp_dataset['arr_value'] = temp_dataset['current_time']\n",
    "    elif event_type in ['Video.Seek', 'Video.Stalled']:\n",
    "        temp_dataset['arr_value'] = temp_dataset['current_time'] - temp_dataset['old_time']\n",
    "    elif event_type == 'Video.SpeedChange':\n",
    "        temp_dataset['arr_value'] = temp_dataset['new_speed']\n",
    "    else:\n",
    "        temp_dataset['arr_value'] = 1\n",
    "    temp_datasets.append(temp_dataset)\n",
    "    \n",
    "merged_video = pd.concat(temp_datasets)\n",
    "#video_dataset = merged_video[['user_id', 'video_id', 'event_type', 'timestamp', 'pass-fail', 'arr_value']].dropna()\n",
    "video_dataset = merged_video[['user_id', 'video_id', 'event_type', 'timestamp', 'arr_value']].dropna()\n",
    "#======\n",
    "\n",
    "\n",
    "#======\n",
    "#Adding numbers to arr_value, PROBLEM\n",
    "list_of_prob_types = list(np.unique(prob_dataset['problem_type']))\n",
    "temp_datasets = []\n",
    "for prob_type in list_of_prob_types:\n",
    "    temp_dataset = prob_dataset[prob_dataset.problem_type == prob_type].copy()\n",
    "    \n",
    "    if prob_type == 'Assignment Part':\n",
    "        temp_dataset = temp_dataset.dropna()\n",
    "        temp_dataset['arr_value'] = temp_dataset['grade']\n",
    "    elif prob_type == 'Quiz':\n",
    "        temp_dataset['arr_value'] = 1\n",
    "    else:\n",
    "        print(\"New Problem type Found!!\")\n",
    "    temp_datasets.append(temp_dataset)\n",
    "    \n",
    "    \n",
    "merged_prob = pd.concat(temp_datasets)\n",
    "prob_dataset = merged_prob[['user_id', 'problem_id', 'problem_type', 'timestamp', 'submission_number', 'arr_value']].dropna()\n",
    "#======\n",
    "\n",
    "\n",
    "prob_group = prob_dataset.groupby('user_id').agg(list)\n",
    "video_group = video_dataset.groupby('user_id').agg(list)\n",
    "\n",
    "merged = video_group.merge(prob_group, on='user_id', how='outer', suffixes=['_video', '_prob'])\n",
    "\n",
    "\n",
    "merged['video_length'] = merged['timestamp_video'].apply(lambda x: len(x) if type(x) == list else 0)\n",
    "merged['prob_length'] = merged['timestamp_prob'].apply(lambda x: len(x) if type(x) == list else 0)\n",
    "for column in merged.columns:\n",
    "    merged[column] = merged[column].apply(lambda x: [] if type(x) not in [list, int] else x)\n",
    "\n",
    "merged['timestamp'] = merged['timestamp_video'] + merged['timestamp_prob']\n",
    "merged['event_type'] = merged['event_type'] + merged['problem_type']\n",
    "merged['arr_value'] = merged['arr_value_video'] + merged['arr_value_prob']\n",
    "\n",
    "merged = merged.merge(outcome_dataset[['user_id', 'pass-fail']], on='user_id', how='inner')\n",
    "#TODO: What to do for students without any video or problem interactions in the specified period?\n",
    "#For now, delete them, because what is the model supposed to learn from?\n",
    "\n",
    "dataset = merged.drop(columns=['timestamp_video', 'timestamp_prob', 'arr_value_video', 'arr_value_prob', 'problem_type'])\n",
    "\n",
    "list_of_events = list_of_video_events + list_of_prob_types\n",
    "\n",
    "#selected_users = dataset[(dataset['video_length'] + dataset['prob_length'] > min_len) & (dataset['video_length'] + dataset['prob_length'] < max_len)]\n",
    "selected_users = dataset.apply(trunc, max_len=max_len, axis=1)\n",
    "selected_users = selected_users.reset_index(level=0)\n",
    "selected_users['timestamp'] = selected_users['timestamp'].apply(lambda x: np.pad(x, (0, max_len-len(x)), 'constant', constant_values=(0, 0)))\n",
    "selected_users['event_type'] = selected_users['event_type'].apply(lambda x: [list_of_events.index(i) for i in x])\n",
    "selected_users['pass-fail'] = selected_users['pass-fail'].apply(lambda x: 1 if x=='Passed' else 0)\n",
    "\n",
    "P_users = []\n",
    "y_users = []\n",
    "for index, row in selected_users.iterrows():\n",
    "    user_id = row['user_id']\n",
    "    time = row['timestamp']\n",
    "    events = row['event_type']\n",
    "    arr = np.zeros([max_len, len(list_of_events)+3])\n",
    "    arr[np.arange(len(events)), events] = row['arr_value']\n",
    "    arr[:row['video_length'], len(list_of_events)] = row['video_id']\n",
    "    arr[row['video_length']:row['video_length']+row['prob_length'], len(list_of_events)+1] = row['problem_id']\n",
    "    arr[row['video_length']:row['video_length']+row['prob_length'], len(list_of_events)+2] = row['submission_number']\n",
    "    \n",
    "    indices = np.argsort(time)\n",
    "    \n",
    "    P_users.append({'id': user_id, \n",
    "                    'static': tuple([0, 0, 0, 0, 0, 0]), \n",
    "                    'extended_static': [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                    'arr': arr[indices],\n",
    "                    'time': np.reshape(time[indices], (max_len, 1)), \n",
    "                    'length': len(events)\n",
    "                    })\n",
    "    \n",
    "    y_users.append(row['pass-fail'])\n",
    "y_users = np.reshape(np.array(y_users), (len(y_users), 1))\n",
    "\n",
    "np.save(f\"{filename.split('.')[0]}_data_hard_fail.npy\", P_users)\n",
    "np.save(f\"{filename.split('.')[0]}_y_hard_fail.npy\", y_users)\n",
    "\n",
    "print(f\"Dataset: {filename} \\n Users: {len(P_users)} \\n 1/all ratio: {sum(y_users)/len(y_users)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0b52c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b06bf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be53257b",
   "metadata": {},
   "source": [
    "# For all courses, not removing students with no interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54d63a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MOOCs_list = [\n",
    "'villesafricaines_002.csv',\n",
    " 'villesafricaines_003.csv',\n",
    " 'microcontroleurs_004.csv',\n",
    " 'dsp_004.csv',\n",
    " 'hwts_001.csv',\n",
    " 'dsp_001.csv',\n",
    " 'progfun_002.csv',\n",
    " 'microcontroleurs_003.csv',\n",
    " 'geomatique_003.csv',\n",
    " 'villesafricaines_001.csv',\n",
    " 'progfun_003.csv',\n",
    " 'dsp_002.csv',\n",
    " 'structures_002.csv',\n",
    " 'initprogcpp_001.csv',\n",
    " 'analysenumerique_003.csv',\n",
    " 'microcontroleurs_006.csv',\n",
    " 'dsp_005.csv',\n",
    " 'hwts_002.csv',\n",
    " 'dsp_006.csv',\n",
    " 'analysenumerique_002.csv',\n",
    " 'structures_003.csv',\n",
    " 'microcontroleurs_005.csv',\n",
    " 'venture_001.csv',\n",
    " 'analysenumerique_001.csv',\n",
    " 'cpp_fr_001.csv',\n",
    " 'structures_001.csv'\n",
    "]\n",
    "#MOOCs_list = [i.split('.')[0] for i in MOOCs_list]\n",
    "MOOCs_list = [i.replace(\"_\", \"-\") for i in MOOCs_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be9d967c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ../raindrop_data/prep_data\n",
    "!mkdir ../raindrop_data/split_args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "050216c5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: villesafricaines-002.csv \n",
      " Users: 3000 \n",
      " 1/all ratio: [0.078]\n",
      "Dataset: villesafricaines-003.csv \n",
      " Users: 2153 \n",
      " 1/all ratio: [0.10496981]\n",
      "Dataset: microcontroleurs-004.csv \n",
      " Users: 2827 \n",
      " 1/all ratio: [0.08206579]\n",
      "Dataset: dsp-004.csv \n",
      " Users: 1735 \n",
      " 1/all ratio: [0.16311239]\n",
      "Dataset: hwts-001.csv \n",
      " Users: 1400 \n",
      " 1/all ratio: [0.45714286]\n",
      "Dataset: dsp-001.csv \n",
      " Users: 5611 \n",
      " 1/all ratio: [0.2696489]\n",
      "Dataset: progfun-002.csv \n",
      " Users: 7840 \n",
      " 1/all ratio: [0.81747449]\n",
      "Dataset: microcontroleurs-003.csv \n",
      " Users: 567 \n",
      " 1/all ratio: [0.49382716]\n",
      "Dataset: geomatique-003.csv \n",
      " Users: 452 \n",
      " 1/all ratio: [0.45132743]\n",
      "Dataset: villesafricaines-001.csv \n",
      " Users: 4941 \n",
      " 1/all ratio: [0.11353977]\n",
      "Dataset: progfun-003.csv \n",
      " Users: 10862 \n",
      " 1/all ratio: [0.52071442]\n",
      "Dataset: dsp-002.csv \n",
      " Users: 3974 \n",
      " 1/all ratio: [0.23351787]\n",
      "Dataset: structures-002.csv \n",
      " Users: 97 \n",
      " 1/all ratio: [0.84536082]\n",
      "Dataset: initprogcpp-001.csv \n",
      " Users: 727 \n",
      " 1/all ratio: [0.63411279]\n",
      "Dataset: analysenumerique-003.csv \n",
      " Users: 459 \n",
      " 1/all ratio: [0.74727669]\n",
      "Dataset: microcontroleurs-006.csv \n",
      " Users: 1470 \n",
      " 1/all ratio: [0.10884354]\n",
      "Dataset: dsp-005.csv \n",
      " Users: 2605 \n",
      " 1/all ratio: [0.17159309]\n",
      "Dataset: hwts-002.csv \n",
      " Users: 1023 \n",
      " 1/all ratio: [0.49364614]\n",
      "Dataset: dsp-006.csv \n",
      " Users: 1469 \n",
      " 1/all ratio: [0.24029952]\n",
      "Dataset: analysenumerique-002.csv \n",
      " Users: 504 \n",
      " 1/all ratio: [0.71626984]\n",
      "Dataset: structures-003.csv \n",
      " Users: 173 \n",
      " 1/all ratio: [0.31213873]\n",
      "Dataset: microcontroleurs-005.csv \n",
      " Users: 2639 \n",
      " 1/all ratio: [0.07995453]\n",
      "Dataset: venture-001.csv \n",
      " Users: 3208 \n",
      " 1/all ratio: [0.03023691]\n",
      "Dataset: analysenumerique-001.csv \n",
      " Users: 505 \n",
      " 1/all ratio: [0.08514851]\n",
      "Dataset: cpp-fr-001.csv \n",
      " Users: 790 \n",
      " 1/all ratio: [0.38481013]\n",
      "Dataset: structures-001.csv \n",
      " Users: 95 \n",
      " 1/all ratio: [0.66315789]\n"
     ]
    }
   ],
   "source": [
    "# NOTE\n",
    "# Note, for now I'm assuming prob_length is always less than max_len\n",
    "\n",
    "dims = []\n",
    "info_dict = {}\n",
    "for filename in MOOCs_list:\n",
    "\n",
    "    #filename = \"progfun-002.csv\"\n",
    "    coursetype = \"mooc/coursera\"\n",
    "    saved_filename = filename.split('.')[0]\n",
    "    \n",
    "    marras_feats = pd.read_csv(f'../raindrop_data/easy-fail/eq_week-marras_et_al-{saved_filename.replace(\"-\", \"_\")}/feature_labels.csv')\n",
    "    number_id_mapping = pd.read_csv(f'../raindrop_data/easy-fail/user_id_mapping-{saved_filename.replace(\"-\", \"_\")}.csv')\n",
    "    hard_fail = marras_feats.merge(number_id_mapping, on='Unnamed: 0', how='inner')['user_id']\n",
    "\n",
    "\n",
    "    meta_dataset = pd.read_csv('../raindrop_data/mooc/metadata.csv')\n",
    "    meta_dataset = meta_dataset[meta_dataset.course_id==saved_filename]\n",
    "\n",
    "    start_timestamp = datetime.datetime.strptime(meta_dataset.start_date.to_numpy()[0], '%Y-%m-%d %H:%M:%S')\n",
    "    start_timestamp = start_timestamp.timestamp()\n",
    "    end_timestamp = datetime.datetime.strptime(meta_dataset.end_date.to_numpy()[0], '%Y-%m-%d %H:%M:%S')\n",
    "    end_timestamp = end_timestamp.timestamp()\n",
    "\n",
    "    x_percent = 0.6\n",
    "    x_deadline = start_timestamp + x_percent*(end_timestamp-start_timestamp)\n",
    "\n",
    "    video_dataset = pd.read_csv(f\"../raindrop_data/{coursetype}/video_event/{filename}\")\n",
    "    video_dataset = video_dataset.merge(hard_fail, on='user_id', how='right')\n",
    "    outcome_dataset = pd.read_csv(f\"../raindrop_data/{coursetype}/grade/{filename}\")\n",
    "    outcome_dataset = outcome_dataset.merge(hard_fail, on='user_id', how='right')\n",
    "\n",
    "    prob_dataset = pd.read_csv(f\"../raindrop_data/{coursetype}/problem_event/{filename}\")\n",
    "    prob_dataset = prob_dataset.merge(hard_fail, on='user_id', how='right')\n",
    "\n",
    "    \n",
    "    video_dataset = video_dataset[(video_dataset.timestamp >= start_timestamp) & (video_dataset.timestamp <= x_deadline)]\n",
    "    prob_dataset = prob_dataset[(prob_dataset.timestamp >= start_timestamp) & (prob_dataset.timestamp <= x_deadline)]\n",
    "    \n",
    "    #video_dataset = video_dataset.merge(outcome_dataset, on='user_id', how='outer')\n",
    "    if 'XXX-' in filename:\n",
    "        dataset['pass-fail'] = dataset['grade'].apply(lambda x: 'Passed' if x>=4 else 'Failed')\n",
    "\n",
    "    min_len = 0\n",
    "    max_len = 1000\n",
    "\n",
    "    #======\n",
    "    #Adding numbers to arr_value, VIDEO\n",
    "    list_of_video_events = list(np.unique(video_dataset['event_type']))\n",
    "    temp_datasets = []\n",
    "    for event_type in list_of_video_events:\n",
    "        temp_dataset = video_dataset[video_dataset.event_type == event_type].copy()\n",
    "        if event_type in ['Video.Error', 'Video.Pause', 'Video.Play']:\n",
    "            temp_dataset['arr_value'] = temp_dataset['current_time']\n",
    "        elif event_type in ['Video.Seek', 'Video.Stalled']:\n",
    "            temp_dataset['arr_value'] = temp_dataset['current_time'] - temp_dataset['old_time']\n",
    "        elif event_type == 'Video.SpeedChange':\n",
    "            temp_dataset['arr_value'] = temp_dataset['new_speed']\n",
    "        else:\n",
    "            temp_dataset['arr_value'] = 1\n",
    "        temp_datasets.append(temp_dataset)\n",
    "\n",
    "    merged_video = pd.concat(temp_datasets)\n",
    "    #video_dataset = merged_video[['user_id', 'video_id', 'event_type', 'timestamp', 'pass-fail', 'arr_value']].dropna()\n",
    "    video_dataset = merged_video[['user_id', 'video_id', 'event_type', 'timestamp', 'arr_value']].dropna()\n",
    "    #======\n",
    "\n",
    "\n",
    "    #======\n",
    "    #Adding numbers to arr_value, PROBLEM\n",
    "    list_of_prob_types = list(np.unique(prob_dataset['problem_type']))\n",
    "    temp_datasets = []\n",
    "    for prob_type in list_of_prob_types:\n",
    "        temp_dataset = prob_dataset[prob_dataset.problem_type == prob_type].copy()\n",
    "\n",
    "        if prob_type == 'Assignment Part':\n",
    "            temp_dataset = temp_dataset.dropna()\n",
    "            temp_dataset['arr_value'] = temp_dataset['grade']\n",
    "        elif prob_type == 'Quiz':\n",
    "            temp_dataset['arr_value'] = 1\n",
    "        else:\n",
    "            print(\"New Problem type Found!!\")\n",
    "        temp_datasets.append(temp_dataset)\n",
    "\n",
    "\n",
    "    merged_prob = pd.concat(temp_datasets)\n",
    "    prob_dataset = merged_prob[['user_id', 'problem_id', 'problem_type', 'timestamp', 'submission_number', 'arr_value']].dropna()\n",
    "    #======\n",
    "\n",
    "\n",
    "    prob_group = prob_dataset.groupby('user_id').agg(list)\n",
    "    video_group = video_dataset.groupby('user_id').agg(list)\n",
    "\n",
    "    merged = video_group.merge(prob_group, on='user_id', how='outer', suffixes=['_video', '_prob'])\n",
    "\n",
    "\n",
    "    merged['video_length'] = merged['timestamp_video'].apply(lambda x: len(x) if type(x) == list else 0)\n",
    "    merged['prob_length'] = merged['timestamp_prob'].apply(lambda x: len(x) if type(x) == list else 0)\n",
    "    for column in merged.columns:\n",
    "        merged[column] = merged[column].apply(lambda x: [] if type(x) not in [list, int] else x)\n",
    "\n",
    "    merged['timestamp'] = merged['timestamp_video'] + merged['timestamp_prob']\n",
    "    merged['event_type'] = merged['event_type'] + merged['problem_type']\n",
    "    merged['arr_value'] = merged['arr_value_video'] + merged['arr_value_prob']\n",
    "\n",
    "    merged = merged.merge(outcome_dataset[['user_id', 'pass-fail']], on='user_id', how='right')\n",
    "    #TODO: What to do for students without any video or problem interactions in the specified period?\n",
    "    #For now, delete them, because what is the model supposed to learn from?\n",
    "    \n",
    "    merged['video_length'] = merged['video_length'].fillna(0).apply(int)\n",
    "    merged['prob_length'] = merged['prob_length'].fillna(0).apply(int)\n",
    "    for column in merged.columns:\n",
    "        merged[column] = merged[column].apply(lambda x: [] if type(x) not in [list, int, str] else x)\n",
    "    \n",
    "\n",
    "    dataset = merged.drop(columns=['timestamp_video', 'timestamp_prob', 'arr_value_video', 'arr_value_prob', 'problem_type'])\n",
    "    \n",
    "    list_of_events = list_of_video_events + list_of_prob_types\n",
    "\n",
    "    #selected_users = dataset[(dataset['video_length'] + dataset['prob_length'] > min_len) & (dataset['video_length'] + dataset['prob_length'] < max_len)]\n",
    "    selected_users = dataset.apply(trunc, max_len=max_len, axis=1)\n",
    "    selected_users = selected_users.reset_index(level=0)\n",
    "    selected_users['timestamp'] = selected_users['timestamp'].apply(lambda x: np.pad(x, (0, max_len-len(x)), 'constant', constant_values=(0, 0)))\n",
    "    selected_users['event_type'] = selected_users['event_type'].apply(lambda x: [list_of_events.index(i) for i in x])\n",
    "    selected_users['pass-fail'] = selected_users['pass-fail'].apply(lambda x: 1 if x=='Passed' else 0)\n",
    "\n",
    "    P_users = []\n",
    "    y_users = []\n",
    "    for index, row in selected_users.iterrows():\n",
    "        user_id = row['user_id']\n",
    "        time = row['timestamp']\n",
    "        events = row['event_type']\n",
    "        arr = np.zeros([max_len, len(list_of_events)+3])\n",
    "        arr[np.arange(len(events)), events] = row['arr_value']\n",
    "        arr[:row['video_length'], len(list_of_events)] = row['video_id']\n",
    "        arr[row['video_length']:row['video_length']+row['prob_length'], len(list_of_events)+1] = row['problem_id']\n",
    "        arr[row['video_length']:row['video_length']+row['prob_length'], len(list_of_events)+2] = row['submission_number']\n",
    "\n",
    "        indices = np.argsort(time)\n",
    "\n",
    "        P_users.append({'id': user_id, \n",
    "                        'static': tuple([0, 0, 0, 0, 0, 0]), \n",
    "                        'extended_static': [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                        'arr': arr[indices],\n",
    "                        'time': np.reshape(time[indices], (max_len, 1)), \n",
    "                        'length': len(events)\n",
    "                        })\n",
    "\n",
    "        y_users.append(row['pass-fail'])\n",
    "    y_users = np.reshape(np.array(y_users), (len(y_users), 1))\n",
    "\n",
    "    \n",
    "    np.save(os.path.join('../raindrop_data/prep_data', f\"{filename.split('.')[0]}_{int(x_percent*100)}_data_hard_fail.npy\"), P_users)\n",
    "    np.save(os.path.join('../raindrop_data/prep_data', f\"{filename.split('.')[0]}_{int(x_percent*100)}_y_hard_fail.npy\"), y_users)\n",
    "\n",
    "    print(f\"Dataset: {filename} \\n Users: {len(P_users)} \\n 1/all ratio: {sum(y_users)/len(y_users)}\")\n",
    "    dims.append(len(list_of_events)+3)\n",
    "    info_dict[saved_filename] = [len(P_users), sum(y_users)/len(y_users)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1454a416",
   "metadata": {},
   "outputs": [],
   "source": [
    "info_dict = {key: [info_dict[key][0], float(info_dict[key][1])] for key in info_dict.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d135ea5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "info_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33af262",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb586d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6218fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6de0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "['Video.Download',\n",
    " 'Video.Error',\n",
    " 'Video.Load',\n",
    " 'Video.Pause',\n",
    " 'Video.Play',\n",
    " 'Video.Seek',\n",
    " 'Video.SpeedChange',\n",
    " 'Video.Stalled',\n",
    " 'Assignment Part',\n",
    " 'Quiz',\n",
    " 'video_id',\n",
    " 'problem_id',\n",
    " 'submission_number']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db23f63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_prob_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f0551e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dims4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0713d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "dims6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a79ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dims4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327b08bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9586e70b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094441bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aab109d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40b84de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171692e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb343c29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b04e124",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe4d712",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de8f67a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1df924",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b225a0fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379d9317",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328eacdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e0b70f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf5dcdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9248025d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152901e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782460ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1c7b7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392157b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de03d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged[.loc[merged.isnull()] = merged.loc[merged.isnull()].apply(lambda x: [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65a6a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "for motherfuckingcolumn in merged.columns:\n",
    "    merged[motherfuckingcolumn] = merged[motherfuckingcolumn].apply(lambda x: [] if type(x) != list else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6729c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da20fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged['timestamp_video'].apply(lambda x: [] if type(x) != list else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb54f377",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb81e17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb87676",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cf12ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1300fa9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d12f01e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f319fdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_group.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7484157d",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_group.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f46e7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = video_group.merge(prob_group, on='user_id', how='outer', suffixes=['_video', '_prob'])\n",
    "\n",
    "\n",
    "merged['video_length'] = merged['timestamp_video'].apply(lambda x: len(x) if type(x) == list else 0)\n",
    "merged['prob_length'] = merged['timestamp_prob'].apply(lambda x: len(x) if type(x) == list else 0)\n",
    "merged['timestamp'] = merged['timestamp_video'] + merged['timestamp_prob']\n",
    "merged['event_type'] = merged['event_type'] + merged['problem_type']\n",
    "merged['arr_value'] = merged['arr_value_video'] + merged['arr_value_prob']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1371e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa578f2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b756cf07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a60c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged[merged.index == 3257038]['video_id'].isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b85111",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a51216",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged['timestamp_video']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129ae1f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e84349",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46ec219",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955e9ce5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598b2b90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daaeec98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e497b1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c422dffa",
   "metadata": {},
   "source": [
    "# Temp exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1875a956",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c381a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"progfun-002.csv\"\n",
    "coursetype = \"mooc/coursera\"\n",
    "saved_filename = filename.split('.')[0]\n",
    "\n",
    "marras_feats = pd.read_csv(f'F:/SeFT4ED_2/data/result/easy-fail/eq_week-marras_et_al-{saved_filename.replace(\"-\", \"_\")}/feature_labels.csv')\n",
    "number_id_mapping = pd.read_csv(f'F:/SeFT4ED_2/data/result/easy-fail/user_id_mapping-{saved_filename.replace(\"-\", \"_\")}.csv')\n",
    "hard_fail = marras_feats.merge(number_id_mapping, on='Unnamed: 0', how='inner')['user_id']\n",
    "\n",
    "\n",
    "meta_dataset = pd.read_csv('F:\\SeFT4ED\\DATASETS\\mooc\\metadata.csv')\n",
    "meta_dataset = meta_dataset[meta_dataset.course_id==saved_filename]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49a973b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_dataset = pd.read_csv(f\"F:/SeFT4ED_2/data/course/{coursetype}/problem_event/{filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3a4553",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7ffba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(prob_dataset['problem_type'], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169ecaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_dataset = prob_dataset.merge(hard_fail, on='user_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb513296",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c3daeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(prob_dataset['problem_type'], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443a54c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529f1914",
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = (dataset['video_length'] + dataset['prob_length']).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ca1825",
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e00cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths[-500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1bfe21",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49efdd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3143fcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = dataset['prob_length'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40604f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c56bef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[-500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627a9a23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
