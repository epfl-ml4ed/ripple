{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f85bfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7475120d",
   "metadata": {},
   "source": [
    "## Standardized Raindrop-compatible data matrices\n",
    "The original MOOC data is provided in several forms of spreadsheets. Here we convert them from various forms into Raindrop-compatible matrices of interactions. These matrices are then populated with cleaned data. The cleaning includes:\n",
    "- Combining `Video` and `Problem` events and extracting various event types of each one.\n",
    "- Extracting data only specific to pre-defined time invtervals (e.g. early 40%, 60%)\n",
    "- Changing qualitative phrases used in some courses' log to qunatitative data\n",
    "- Balancing the length of `Video`/`Problem` events in case there is a dimensionality limit due to memory shortage.\n",
    "Here we also keep track of the course information for further references."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fca0d44",
   "metadata": {},
   "source": [
    "### Auxiliary functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77074066",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_bytes(x, distinction=False):\n",
    "    x = str(x)\n",
    "    if \"b'\" not in x:\n",
    "        val = x\n",
    "    else:\n",
    "        val = x.replace(\"b'\", \"\").split(\"\\\\x00\")[0]\n",
    "        if \"'\" in val:\n",
    "            val = val[:-1]\n",
    "    if distinction:\n",
    "        if val == \"\":\n",
    "            val = False\n",
    "        else:\n",
    "            val = True\n",
    "    return val\n",
    "\n",
    "# If running short on memory, one can truncate the interaction data to decrease the matrix's number of rows.\n",
    "def trunc(x_input, max_len):\n",
    "    x = x_input.copy()\n",
    "    x_video_len = x['video_length']\n",
    "    x_prob_len = x['prob_length']\n",
    "    x_len = x_video_len + x_prob_len\n",
    "    \n",
    "    diff = x_len - max_len\n",
    "    \n",
    "    if diff > 0:\n",
    "        x['video_id'] = x['video_id'][:-diff]\n",
    "        # Since there are usually much less problem events available than video events, \n",
    "        #  and for not losing all the problem data information, we reduce only the length of video interactions.\n",
    "        x['event_type'] = x['event_type'][:x_video_len-diff] + x['event_type'][x_video_len:]\n",
    "        x['pass-fail'] = x['pass-fail']\n",
    "        x['timestamp'] = x['timestamp'][:x_video_len-diff] + x['timestamp'][x_video_len:]\n",
    "        x['arr_value'] = x['arr_value'][:x_video_len-diff] + x['arr_value'][x_video_len:]\n",
    "        x['video_length'] = x['video_length'] - diff\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54d63a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MOOCs_list = [\n",
    "'villesafricaines_002.csv',\n",
    " 'villesafricaines_003.csv',\n",
    " 'microcontroleurs_004.csv',\n",
    " 'dsp_004.csv',\n",
    " 'hwts_001.csv',\n",
    " 'dsp_001.csv',\n",
    " 'progfun_002.csv',\n",
    " 'microcontroleurs_003.csv',\n",
    " 'geomatique_003.csv',\n",
    " 'villesafricaines_001.csv',\n",
    " 'progfun_003.csv',\n",
    " 'dsp_002.csv',\n",
    " 'structures_002.csv',\n",
    " 'initprogcpp_001.csv',\n",
    " 'analysenumerique_003.csv',\n",
    " 'microcontroleurs_006.csv',\n",
    " 'dsp_005.csv',\n",
    " 'hwts_002.csv',\n",
    " 'dsp_006.csv',\n",
    " 'analysenumerique_002.csv',\n",
    " 'structures_003.csv',\n",
    " 'microcontroleurs_005.csv',\n",
    " 'venture_001.csv',\n",
    " 'analysenumerique_001.csv',\n",
    " 'cpp_fr_001.csv',\n",
    " 'structures_001.csv'\n",
    "]\n",
    "MOOCs_list = [i.replace(\"_\", \"-\") for i in MOOCs_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be9d967c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ../data/prep_data\n",
    "!mkdir ../data/split_args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "050216c5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: villesafricaines-002.csv \n",
      " Users: 3000 \n",
      " 1/all ratio: [0.078]\n",
      "Dataset: villesafricaines-003.csv \n",
      " Users: 2153 \n",
      " 1/all ratio: [0.10496981]\n",
      "Dataset: microcontroleurs-004.csv \n",
      " Users: 2827 \n",
      " 1/all ratio: [0.08206579]\n",
      "Dataset: dsp-004.csv \n",
      " Users: 1735 \n",
      " 1/all ratio: [0.16311239]\n",
      "Dataset: hwts-001.csv \n",
      " Users: 1400 \n",
      " 1/all ratio: [0.45714286]\n",
      "Dataset: dsp-001.csv \n",
      " Users: 5611 \n",
      " 1/all ratio: [0.2696489]\n",
      "Dataset: progfun-002.csv \n",
      " Users: 7840 \n",
      " 1/all ratio: [0.81747449]\n",
      "Dataset: microcontroleurs-003.csv \n",
      " Users: 567 \n",
      " 1/all ratio: [0.49382716]\n",
      "Dataset: geomatique-003.csv \n",
      " Users: 452 \n",
      " 1/all ratio: [0.45132743]\n",
      "Dataset: villesafricaines-001.csv \n",
      " Users: 4941 \n",
      " 1/all ratio: [0.11353977]\n",
      "Dataset: progfun-003.csv \n",
      " Users: 10862 \n",
      " 1/all ratio: [0.52071442]\n",
      "Dataset: dsp-002.csv \n",
      " Users: 3974 \n",
      " 1/all ratio: [0.23351787]\n",
      "Dataset: structures-002.csv \n",
      " Users: 97 \n",
      " 1/all ratio: [0.84536082]\n",
      "Dataset: initprogcpp-001.csv \n",
      " Users: 727 \n",
      " 1/all ratio: [0.63411279]\n",
      "Dataset: analysenumerique-003.csv \n",
      " Users: 459 \n",
      " 1/all ratio: [0.74727669]\n",
      "Dataset: microcontroleurs-006.csv \n",
      " Users: 1470 \n",
      " 1/all ratio: [0.10884354]\n",
      "Dataset: dsp-005.csv \n",
      " Users: 2605 \n",
      " 1/all ratio: [0.17159309]\n",
      "Dataset: hwts-002.csv \n",
      " Users: 1023 \n",
      " 1/all ratio: [0.49364614]\n",
      "Dataset: dsp-006.csv \n",
      " Users: 1469 \n",
      " 1/all ratio: [0.24029952]\n",
      "Dataset: analysenumerique-002.csv \n",
      " Users: 504 \n",
      " 1/all ratio: [0.71626984]\n",
      "Dataset: structures-003.csv \n",
      " Users: 173 \n",
      " 1/all ratio: [0.31213873]\n",
      "Dataset: microcontroleurs-005.csv \n",
      " Users: 2639 \n",
      " 1/all ratio: [0.07995453]\n",
      "Dataset: venture-001.csv \n",
      " Users: 3208 \n",
      " 1/all ratio: [0.03023691]\n",
      "Dataset: analysenumerique-001.csv \n",
      " Users: 505 \n",
      " 1/all ratio: [0.08514851]\n",
      "Dataset: cpp-fr-001.csv \n",
      " Users: 790 \n",
      " 1/all ratio: [0.38481013]\n",
      "Dataset: structures-001.csv \n",
      " Users: 95 \n",
      " 1/all ratio: [0.66315789]\n"
     ]
    }
   ],
   "source": [
    "dims = []\n",
    "info_dict = {}\n",
    "for filename in MOOCs_list:\n",
    "\n",
    "    \n",
    "    coursetype = \"mooc/coursera\"\n",
    "    saved_filename = filename.split('.')[0]\n",
    "    \n",
    "    # We use the user ids extracted in Marras, et. al. to exclude the `easy_fail` students, in favor of fairness\n",
    "    marras_feats = pd.read_csv(f'../data/extracting_concepts/eq_week-marras_et_al-{saved_filename.replace(\"-\", \"_\")}/feature_labels.csv')\n",
    "    number_id_mapping = pd.read_csv(f'../data/extracting_concepts/user_id_mapping-{saved_filename.replace(\"-\", \"_\")}.csv')\n",
    "    hard_fail = marras_feats.merge(number_id_mapping, on='Unnamed: 0', how='inner')['user_id']\n",
    "\n",
    "\n",
    "    meta_dataset = pd.read_csv('../data/mooc/metadata.csv')\n",
    "    meta_dataset = meta_dataset[meta_dataset.course_id==saved_filename]\n",
    "\n",
    "    start_timestamp = datetime.datetime.strptime(meta_dataset.start_date.to_numpy()[0], '%Y-%m-%d %H:%M:%S')\n",
    "    start_timestamp = start_timestamp.timestamp()\n",
    "    end_timestamp = datetime.datetime.strptime(meta_dataset.end_date.to_numpy()[0], '%Y-%m-%d %H:%M:%S')\n",
    "    end_timestamp = end_timestamp.timestamp()\n",
    "\n",
    "    x_percent = 0.6\n",
    "    x_deadline = start_timestamp + x_percent*(end_timestamp-start_timestamp)\n",
    "\n",
    "    # There are cases where a student does not have any interactions for some type whatsoever\n",
    "    # But in order not to omit those students, we merge using the `hard_fail` dataframe's indices (right merge)\n",
    "    video_dataset = pd.read_csv(f\"../data/{coursetype}/video_event/{filename}\")\n",
    "    video_dataset = video_dataset.merge(hard_fail, on='user_id', how='right')\n",
    "    outcome_dataset = pd.read_csv(f\"../data/{coursetype}/grade/{filename}\")\n",
    "    outcome_dataset = outcome_dataset.merge(hard_fail, on='user_id', how='right')\n",
    "\n",
    "    prob_dataset = pd.read_csv(f\"../data/{coursetype}/problem_event/{filename}\")\n",
    "    prob_dataset = prob_dataset.merge(hard_fail, on='user_id', how='right')\n",
    "\n",
    "\n",
    "    # Cutting off interactions before x% of the course has passed.\n",
    "    video_dataset = video_dataset[(video_dataset.timestamp >= start_timestamp) & (video_dataset.timestamp <= x_deadline)]\n",
    "    prob_dataset = prob_dataset[(prob_dataset.timestamp >= start_timestamp) & (prob_dataset.timestamp <= x_deadline)]\n",
    "    \n",
    "    # Adjusting pass/fail quantization for XXX initiated courses.\n",
    "    if 'XXX-' in filename:\n",
    "        dataset['pass-fail'] = dataset['grade'].apply(lambda x: 'Passed' if x>=4 else 'Failed')\n",
    "\n",
    "    # Cutting off interactions after max_len if running low on memory\n",
    "    min_len = 0\n",
    "    max_len = 1000\n",
    "\n",
    "    #======\n",
    "    # Adding values to arr_value, VIDEO\n",
    "    list_of_video_events = list(np.unique(video_dataset['event_type']))\n",
    "    temp_datasets = []\n",
    "    # For each subtype of `Video` interactions, we set the value accordingly.\n",
    "    for event_type in list_of_video_events:\n",
    "        temp_dataset = video_dataset[video_dataset.event_type == event_type].copy()\n",
    "        if event_type in ['Video.Error', 'Video.Pause', 'Video.Play']:\n",
    "            temp_dataset['arr_value'] = temp_dataset['current_time']\n",
    "        elif event_type in ['Video.Seek', 'Video.Stalled']:\n",
    "            temp_dataset['arr_value'] = temp_dataset['current_time'] - temp_dataset['old_time']\n",
    "        elif event_type == 'Video.SpeedChange':\n",
    "            temp_dataset['arr_value'] = temp_dataset['new_speed']\n",
    "        else:\n",
    "            temp_dataset['arr_value'] = 1\n",
    "        temp_datasets.append(temp_dataset)\n",
    "\n",
    "    merged_video = pd.concat(temp_datasets)\n",
    "    video_dataset = merged_video[['user_id', 'video_id', 'event_type', 'timestamp', 'arr_value']].dropna()\n",
    "    #======\n",
    "\n",
    "\n",
    "    #======\n",
    "    # Adding numbers to arr_value, PROBLEM\n",
    "    list_of_prob_types = list(np.unique(prob_dataset['problem_type']))\n",
    "    temp_datasets = []\n",
    "    for prob_type in list_of_prob_types:\n",
    "        temp_dataset = prob_dataset[prob_dataset.problem_type == prob_type].copy()\n",
    "\n",
    "        if prob_type == 'Assignment Part':\n",
    "            temp_dataset = temp_dataset.dropna()\n",
    "            temp_dataset['arr_value'] = temp_dataset['grade']\n",
    "        elif prob_type == 'Quiz':\n",
    "            temp_dataset['arr_value'] = 1\n",
    "        else:\n",
    "            print(\"New Problem type Found!!\")\n",
    "        temp_datasets.append(temp_dataset)\n",
    "\n",
    "\n",
    "    merged_prob = pd.concat(temp_datasets)\n",
    "    prob_dataset = merged_prob[['user_id', 'problem_id', 'problem_type', 'timestamp', 'submission_number', 'arr_value']].dropna()\n",
    "    #======\n",
    "\n",
    "\n",
    "    prob_group = prob_dataset.groupby('user_id').agg(list)\n",
    "    video_group = video_dataset.groupby('user_id').agg(list)\n",
    "\n",
    "    merged = video_group.merge(prob_group, on='user_id', how='outer', suffixes=['_video', '_prob'])\n",
    "\n",
    "\n",
    "    merged['video_length'] = merged['timestamp_video'].apply(lambda x: len(x) if type(x) == list else 0)\n",
    "    merged['prob_length'] = merged['timestamp_prob'].apply(lambda x: len(x) if type(x) == list else 0)\n",
    "    \n",
    "    # In case there was no `Video` or `Problem` event at all, replace an empty list instead\n",
    "    for column in merged.columns:\n",
    "        merged[column] = merged[column].apply(lambda x: [] if type(x) not in [list, int] else x)\n",
    "\n",
    "    merged['timestamp'] = merged['timestamp_video'] + merged['timestamp_prob']\n",
    "    merged['event_type'] = merged['event_type'] + merged['problem_type']\n",
    "    merged['arr_value'] = merged['arr_value_video'] + merged['arr_value_prob']\n",
    "\n",
    "    merged = merged.merge(outcome_dataset[['user_id', 'pass-fail']], on='user_id', how='right')\n",
    "    \n",
    "    merged['video_length'] = merged['video_length'].fillna(0).apply(int)\n",
    "    merged['prob_length'] = merged['prob_length'].fillna(0).apply(int)\n",
    "    for column in merged.columns:\n",
    "        merged[column] = merged[column].apply(lambda x: [] if type(x) not in [list, int, str] else x)\n",
    "    \n",
    "\n",
    "    dataset = merged.drop(columns=['timestamp_video', 'timestamp_prob', 'arr_value_video', 'arr_value_prob', 'problem_type'])\n",
    "    \n",
    "    # Keep track of what events and subevents exactly were used, for further discussion in the paper\n",
    "    list_of_events = list_of_video_events + list_of_prob_types\n",
    "\n",
    "    # The `selected_users` dataframe is used to:\n",
    "    #    - Truncate the dataframe and keep those who have interactions in the selected period of the course\n",
    "    #    - Pad the dataframe to the maximum length (all matrices should be of the same shape)\n",
    "    #    - One-hot encode various event types\n",
    "    #    - Binarize Passing and Failing \n",
    "    selected_users = dataset.apply(trunc, max_len=max_len, axis=1)\n",
    "    selected_users = selected_users.reset_index(level=0)\n",
    "    selected_users['timestamp'] = selected_users['timestamp'].apply(lambda x: np.pad(x, (0, max_len-len(x)), 'constant', constant_values=(0, 0)))\n",
    "    selected_users['event_type'] = selected_users['event_type'].apply(lambda x: [list_of_events.index(i) for i in x])\n",
    "    selected_users['pass-fail'] = selected_users['pass-fail'].apply(lambda x: 1 if x=='Passed' else 0)\n",
    "\n",
    "    # For each user in the course, the matrix consists of usual interactions, problem_id, and number of submissions for `Problems`\n",
    "    P_users = []\n",
    "    y_users = []\n",
    "    for index, row in selected_users.iterrows():\n",
    "        user_id = row['user_id']\n",
    "        time = row['timestamp']\n",
    "        events = row['event_type']\n",
    "        arr = np.zeros([max_len, len(list_of_events)+3])\n",
    "        arr[np.arange(len(events)), events] = row['arr_value']\n",
    "        arr[:row['video_length'], len(list_of_events)] = row['video_id']\n",
    "        arr[row['video_length']:row['video_length']+row['prob_length'], len(list_of_events)+1] = row['problem_id']\n",
    "        arr[row['video_length']:row['video_length']+row['prob_length'], len(list_of_events)+2] = row['submission_number']\n",
    "\n",
    "        indices = np.argsort(time)\n",
    "\n",
    "        # Transforming the matrices to Raindrop-compatible array\n",
    "        P_users.append({'id': user_id, \n",
    "                        'static': tuple([0, 0, 0, 0, 0, 0]), \n",
    "                        'extended_static': [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                        'arr': arr[indices],\n",
    "                        'time': np.reshape(time[indices], (max_len, 1)), \n",
    "                        'length': len(events)\n",
    "                        })\n",
    "\n",
    "        y_users.append(row['pass-fail'])\n",
    "    y_users = np.reshape(np.array(y_users), (len(y_users), 1))\n",
    "\n",
    "    \n",
    "    np.save(os.path.join('../data/prep_data', f\"{filename.split('.')[0]}_{int(x_percent*100)}_data_hard_fail.npy\"), P_users)\n",
    "    np.save(os.path.join('../data/prep_data', f\"{filename.split('.')[0]}_{int(x_percent*100)}_y_hard_fail.npy\"), y_users)\n",
    "\n",
    "    print(f\"Dataset: {filename} \\n Users: {len(P_users)} \\n 1/all ratio: {sum(y_users)/len(y_users)}\")\n",
    "    dims.append(len(list_of_events)+3)\n",
    "    info_dict[saved_filename] = [len(P_users), sum(y_users)/len(y_users)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1454a416",
   "metadata": {},
   "outputs": [],
   "source": [
    "info_dict = {key: [info_dict[key][0], float(info_dict[key][1])] for key in info_dict.keys()}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
