{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44b48407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/beerslaw/ML4ED/Raindrop/code\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix, average_precision_score, precision_score, recall_score, f1_score, balanced_accuracy_score\n",
    "%cd Raindrop/code\n",
    "from models_rd import *\n",
    "from utils_rd import *\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0423c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MOOCs_list = [\n",
    "#'villesafricaines_002.csv',\n",
    "# 'villesafricaines_003.csv',\n",
    "# 'microcontroleurs_004.csv',\n",
    "# 'dsp_004.csv',\n",
    "# 'hwts_001.csv',\n",
    "# 'dsp_001.csv',\n",
    "# 'progfun_002.csv',\n",
    "# 'microcontroleurs_003.csv',\n",
    "# 'geomatique_003.csv',\n",
    "# 'villesafricaines_001.csv',\n",
    "# 'progfun_003.csv',\n",
    "# 'dsp_002.csv',\n",
    "# 'structures_002.csv',\n",
    "# 'initprogcpp_001.csv',\n",
    "# 'analysenumerique_003.csv',\n",
    "# 'microcontroleurs_006.csv',\n",
    "# 'dsp_005.csv',\n",
    "# 'hwts_002.csv',\n",
    "# 'dsp_006.csv',\n",
    "# 'analysenumerique_002.csv',\n",
    "# 'structures_003.csv',\n",
    "# 'microcontroleurs_005.csv',\n",
    " 'venture_001.csv',\n",
    "# 'analysenumerique_001.csv',\n",
    "# 'cpp_fr_001.csv',\n",
    "# 'structures_001.csv'\n",
    "]\n",
    "MOOCs_list = [i.replace(\"_\", \"-\").split('.')[0] for i in MOOCs_list]\n",
    "\n",
    "dims4 = [\n",
    "# 12,\n",
    "# 12,\n",
    "# 13,\n",
    "# 12,\n",
    "# 12,\n",
    "# 6,\n",
    "# 12,\n",
    "# 13,\n",
    "# 12,\n",
    "# 12,\n",
    "# 12,\n",
    "# 12,\n",
    "# 13,\n",
    "# 13,\n",
    "# 6,\n",
    "# 13,\n",
    "# 12,\n",
    "# 12,\n",
    "# 12,\n",
    "# 12,\n",
    "# 13,\n",
    "# 13,\n",
    " 12,\n",
    "# 6,\n",
    "# 13,\n",
    "# 12\n",
    "]\n",
    "\n",
    "dims6 = [\n",
    "# 12,\n",
    "# 12,\n",
    "# 13,\n",
    "# 12,\n",
    "# 12,\n",
    "# 6,\n",
    "# 12,\n",
    "# 13,\n",
    "# 12,\n",
    " 12,\n",
    "# 12,\n",
    "# 12,\n",
    "# 13,\n",
    "# 13,\n",
    "# 6,\n",
    "# 13,\n",
    "# 12,\n",
    "# 12,\n",
    "# 13,\n",
    "# 12,\n",
    "# 13,\n",
    "# 13,\n",
    " 12,\n",
    "# 6,\n",
    "# 13,\n",
    "# 12\n",
    "]\n",
    "dims = {40: dims4, 60: dims6}\n",
    "raindrop_data_path = '/beerslaw/raindrop_data'\n",
    "percentile = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "736b86ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop epochs: 25, Batches/epoch: 1, Total batches: 25\n",
      "[[  0 128]\n",
      " [  0 128]]\n",
      "Validation: Epoch 0,  val_loss:0.7485, aupr_val: 26.86, auc_val: 89.83\n",
      "**[S] Epoch 0, aupr_val: 26.8567, auc_val: 89.8336 **\n",
      "Validation: Epoch 1,  val_loss:0.7448, aupr_val: 24.61, auc_val: 89.74\n",
      "Validation: Epoch 2,  val_loss:0.7412, aupr_val: 24.92, auc_val: 89.47\n",
      "Epoch 00003: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Validation: Epoch 3,  val_loss:0.7408, aupr_val: 24.72, auc_val: 89.43\n",
      "Validation: Epoch 4,  val_loss:0.7404, aupr_val: 30.27, auc_val: 89.47\n",
      "Validation: Epoch 5,  val_loss:0.7401, aupr_val: 30.27, auc_val: 89.47\n",
      "Validation: Epoch 6,  val_loss:0.7397, aupr_val: 30.09, auc_val: 89.43\n",
      "Epoch 00007: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Validation: Epoch 7,  val_loss:0.7397, aupr_val: 30.09, auc_val: 89.43\n",
      "Validation: Epoch 8,  val_loss:0.7396, aupr_val: 30.09, auc_val: 89.43\n",
      "Epoch 00009: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Validation: Epoch 9,  val_loss:0.7396, aupr_val: 30.09, auc_val: 89.43\n",
      "Validation: Epoch 10,  val_loss:0.7396, aupr_val: 30.09, auc_val: 89.43\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Validation: Epoch 11,  val_loss:0.7396, aupr_val: 30.09, auc_val: 89.43\n",
      "Validation: Epoch 12,  val_loss:0.7396, aupr_val: 30.09, auc_val: 89.43\n",
      "Validation: Epoch 13,  val_loss:0.7396, aupr_val: 30.09, auc_val: 89.43\n",
      "Validation: Epoch 14,  val_loss:0.7396, aupr_val: 30.09, auc_val: 89.43\n",
      "Validation: Epoch 15,  val_loss:0.7396, aupr_val: 30.09, auc_val: 89.43\n",
      "Validation: Epoch 16,  val_loss:0.7396, aupr_val: 30.09, auc_val: 89.43\n",
      "Validation: Epoch 17,  val_loss:0.7396, aupr_val: 30.09, auc_val: 89.43\n",
      "Validation: Epoch 18,  val_loss:0.7396, aupr_val: 30.09, auc_val: 89.43\n",
      "Validation: Epoch 19,  val_loss:0.7396, aupr_val: 30.09, auc_val: 89.43\n",
      "Validation: Epoch 20,  val_loss:0.7396, aupr_val: 30.09, auc_val: 89.43\n",
      "Validation: Epoch 21,  val_loss:0.7396, aupr_val: 30.09, auc_val: 89.43\n",
      "Validation: Epoch 22,  val_loss:0.7396, aupr_val: 30.09, auc_val: 89.43\n",
      "Validation: Epoch 23,  val_loss:0.7396, aupr_val: 30.09, auc_val: 89.43\n",
      "[[  0 128]\n",
      " [  0 128]]\n",
      "Validation: Epoch 24,  val_loss:0.7396, aupr_val: 30.09, auc_val: 89.43\n",
      "Total Time elapsed: 0.920 mins\n",
      "        course  percentile       acc       bac        f1       auc    auprc\n",
      "0  venture-001          40  0.311526  0.644695  0.082988  0.868167  0.21828\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "for MOOC_idx, MOOC in enumerate(MOOCs_list):\n",
    "    \n",
    "    d_inp = dims[percentile][MOOC_idx]\n",
    "    max_len = 1000\n",
    "    n_classes = 2\n",
    "    global_structure = torch.ones(d_inp, d_inp)\n",
    "    saved_model_path = f\"../models/n1_mlp2_best_raindrop_{MOOC}_{percentile}.pt\"\n",
    "    batch_size = 256\n",
    "    n_splits = 1\n",
    "\n",
    "    d_static = 9\n",
    "    static_info = 1\n",
    "    d_ob = 4\n",
    "    d_model = d_inp * d_ob\n",
    "    nhid = 2 * d_model\n",
    "    nlayers = 1\n",
    "    nhead = 2\n",
    "    dropout = 0.2\n",
    "    sensor_wise_mask = False\n",
    "    MAX = 100\n",
    "    aggreg = 'mean'\n",
    "    n_runs=1\n",
    "\n",
    "    learning_rate = 0.0001\n",
    "    num_epochs = 25\n",
    "\n",
    "    acc_arr = np.zeros((n_splits, n_runs))\n",
    "    auprc_arr = np.zeros((n_splits, n_runs))\n",
    "    auroc_arr = np.zeros((n_splits, n_runs))\n",
    "    precision_arr = np.zeros((n_splits, n_runs))\n",
    "    recall_arr = np.zeros((n_splits, n_runs))\n",
    "    F1_arr = np.zeros((n_splits, n_runs))\n",
    "\n",
    "\n",
    "    base_path = '/beerslaw/raindrop_data/prep_data'\n",
    "    Pdict_list = np.load(os.path.join(base_path, f\"{MOOC}_{percentile}_data_hard_fail.npy\"), allow_pickle=True)\n",
    "    arr_outcomes = np.load(os.path.join(base_path, f\"{MOOC}_{percentile}_y_hard_fail.npy\"), allow_pickle=True)\n",
    "\n",
    "    #Ptrain, Ptest, ytrain, ytest = train_test_split(Pdict_list, arr_outcomes, test_size=0.1, random_state=1)\n",
    "    #Ptrain, Pval, ytrain, yval = train_test_split(Ptrain, ytrain, test_size=1/9, random_state=1)\n",
    "    args_train, args_val, args_test = np.load(os.path.join(raindrop_data_path, \n",
    "                                                                   'split_args', f\"split_{MOOC.replace('-', '_')}.npy\"),\n",
    "                                                     allow_pickle=True)\n",
    "    Ptrain = Pdict_list[args_train]\n",
    "    Pval = Pdict_list[args_val]\n",
    "    Ptest = Pdict_list[args_test]\n",
    "    ytrain = arr_outcomes[args_train, :]\n",
    "    yval = arr_outcomes[args_val, :]\n",
    "    ytest = arr_outcomes[args_test, :]\n",
    "    \n",
    "    \n",
    "    zero_indices = [i for i, item in enumerate(Ptrain) if item['length'] == 0]\n",
    "    #zero_Ptrain = Ptrain[zero_indices]\n",
    "    Ptrain = np.delete(Ptrain, zero_indices, axis=0)\n",
    "    ytrain = np.delete(ytrain, zero_indices, axis=0)\n",
    "\n",
    "    zero_indices = [i for i, item in enumerate(Pval) if item['length'] == 0]\n",
    "    #zero_Ptrain = Ptrain[zero_indices]\n",
    "    Pval = np.delete(Pval, zero_indices, axis=0)\n",
    "    yval = np.delete(yval, zero_indices, axis=0)\n",
    "\n",
    "    zero_indices = [i for i, item in enumerate(Ptest) if item['length'] == 0]\n",
    "    zero_ytest = ytest[zero_indices]\n",
    "    Ptest = np.delete(Ptest, zero_indices, axis=0)  \n",
    "    ytest = np.delete(ytest, zero_indices, axis=0)\n",
    "    \n",
    "\n",
    "    T, F = Ptrain[0]['arr'].shape\n",
    "    D = len(Ptrain[0]['extended_static'])\n",
    "\n",
    "    Ptrain_tensor = np.zeros((len(Ptrain), T, F))\n",
    "    Ptrain_static_tensor = np.zeros((len(Ptrain), D))\n",
    "\n",
    "    for i in range(len(Ptrain)):\n",
    "        Ptrain_tensor[i] = Ptrain[i]['arr']\n",
    "        Ptrain_static_tensor[i] = Ptrain[i]['extended_static']\n",
    "\n",
    "    mf, stdf = getStats(Ptrain_tensor)\n",
    "    ms, ss = getStats_static(Ptrain_static_tensor, dataset='P12')\n",
    "\n",
    "    Ptrain_tensor, Ptrain_static_tensor, Ptrain_time_tensor, ytrain_tensor = tensorize_normalize(Ptrain, ytrain, mf,\n",
    "                                                                                                 stdf, ms, ss)\n",
    "    Pval_tensor, Pval_static_tensor, Pval_time_tensor, yval_tensor = tensorize_normalize(Pval, yval, mf, stdf, ms, ss)\n",
    "\n",
    "    Ptest_tensor, Ptest_static_tensor, Ptest_time_tensor, ytest_tensor = tensorize_normalize(Ptest, ytest, mf, stdf, ms, ss)\n",
    "\n",
    "    Ptrain_tensor = Ptrain_tensor.permute(1, 0, 2)\n",
    "    Pval_tensor = Pval_tensor.permute(1, 0, 2)\n",
    "    Ptest_tensor = Ptest_tensor.permute(1, 0, 2)\n",
    "\n",
    "    Ptrain_time_tensor = Ptrain_time_tensor.squeeze(2).permute(1, 0)\n",
    "    Pval_time_tensor = Pval_time_tensor.squeeze(2).permute(1, 0)\n",
    "    Ptest_time_tensor = Ptest_time_tensor.squeeze(2).permute(1, 0)\n",
    "    \n",
    "    model = Raindrop_v2(d_inp, d_model, nhead, nhid, nlayers, dropout, max_len,\n",
    "                                        d_static, MAX, 0.5, aggreg, n_classes, global_structure,\n",
    "                                        sensor_wise_mask=sensor_wise_mask)\n",
    "    #torch.save(model, '../models/raw_raindrop_model.pt')\n",
    "\n",
    "    model = model.cuda()\n",
    "    criterion = torch.nn.CrossEntropyLoss().cuda()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.1,\n",
    "                                                           patience=1, threshold=0.0001, threshold_mode='rel',\n",
    "                                                           cooldown=0, min_lr=1e-8, eps=1e-08, verbose=True)\n",
    "\n",
    "\n",
    "    idx_0 = np.where(ytrain == 0)[0]\n",
    "    idx_1 = np.where(ytrain == 1)[0]\n",
    "\n",
    "    n0, n1 = len(idx_0), len(idx_1)\n",
    "    expanded_idx_1 = np.concatenate([idx_1, idx_1, idx_1], axis=0)\n",
    "    expanded_n1 = len(expanded_idx_1)\n",
    "\n",
    "    K0 = n0 // int(batch_size / 2)\n",
    "    K1 = expanded_n1 // int(batch_size / 2)\n",
    "    n_batches = np.min([K0, K1])\n",
    "\n",
    "    best_aupr_val = best_auc_val = 0.0\n",
    "    best_loss_val = 100.0\n",
    "\n",
    "    print('Stop epochs: %d, Batches/epoch: %d, Total batches: %d' % (num_epochs, n_batches, num_epochs * n_batches))\n",
    "    start = time.time()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "\n",
    "\n",
    "        np.random.shuffle(expanded_idx_1)\n",
    "        I1 = expanded_idx_1\n",
    "        np.random.shuffle(idx_0)\n",
    "        I0 = idx_0\n",
    "\n",
    "        for n in range(n_batches):\n",
    "\n",
    "            idx0_batch = I0[n * int(batch_size / 2):(n + 1) * int(batch_size / 2)]\n",
    "            idx1_batch = I1[n * int(batch_size / 2):(n + 1) * int(batch_size / 2)]\n",
    "            idx = np.concatenate([idx0_batch, idx1_batch], axis=0)\n",
    "\n",
    "\n",
    "            P, Ptime, Pstatic, y = Ptrain_tensor[:, idx, :].cuda(), Ptrain_time_tensor[:, idx].cuda(), \\\n",
    "                                   Ptrain_static_tensor[idx].cuda(), ytrain_tensor[idx].cuda()\n",
    "\n",
    "\n",
    "            lengths = torch.sum(Ptime > 0, dim=0)\n",
    "            #outputs, local_structure_regularization, _ = model.forward(P, Pstatic, Ptime, lengths)\n",
    "            outputs = model.forward(P, Pstatic, Ptime, lengths)\n",
    "            outputs = torch.nan_to_num(outputs)\n",
    "            \n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "        train_probs = torch.squeeze(torch.sigmoid(outputs))\n",
    "        train_probs = train_probs.cpu().detach().numpy()\n",
    "        train_y = y.cpu().detach().numpy()\n",
    "        train_auroc = roc_auc_score(train_y, train_probs[:, 1])\n",
    "        train_auprc = average_precision_score(train_y, train_probs[:, 1])\n",
    "\n",
    "\n",
    "        if epoch == 0 or epoch == num_epochs - 1:\n",
    "            print(confusion_matrix(train_y, np.argmax(train_probs, axis=1), labels=[0, 1]))\n",
    "\n",
    "        \"\"\"Validation\"\"\"\n",
    "        model.eval()\n",
    "        if epoch == 0 or epoch % 1 == 0:\n",
    "            with torch.no_grad():\n",
    "                #out_val = evaluate_standard(model, Pval_tensor, Pval_time_tensor, Pval_static_tensor, static=static_info)\n",
    "                #Changed into:\n",
    "                out_val = evaluate(model, Pval_tensor, Pval_time_tensor, Pval_static_tensor,batch_size=batch_size, static=static_info)\n",
    "                out_val = torch.squeeze(torch.sigmoid(out_val))\n",
    "                out_val = out_val.detach().cpu().numpy()\n",
    "\n",
    "                val_loss = criterion(torch.from_numpy(out_val), torch.from_numpy(yval.squeeze(1)).long())\n",
    "\n",
    "\n",
    "                auc_val = roc_auc_score(yval, out_val[:, 1])\n",
    "                aupr_val = average_precision_score(yval, out_val[:, 1])\n",
    "\n",
    "                print(\"Validation: Epoch %d,  val_loss:%.4f, aupr_val: %.2f, auc_val: %.2f\" % (epoch,\n",
    "                                                                                                val_loss.item(),\n",
    "                                                                                                aupr_val * 100,\n",
    "                                                                                                auc_val * 100))\n",
    "\n",
    "                scheduler.step(aupr_val)\n",
    "                if auc_val > best_auc_val:\n",
    "                    best_auc_val = auc_val\n",
    "                    print(\n",
    "                        \"**[S] Epoch %d, aupr_val: %.4f, auc_val: %.4f **\" % (\n",
    "                        epoch, aupr_val * 100, auc_val * 100))\n",
    "                    torch.save(model.state_dict(), saved_model_path)\n",
    "\n",
    "    end = time.time()\n",
    "    time_elapsed = end - start\n",
    "    print('Total Time elapsed: %.3f mins' % (time_elapsed / 60.0))\n",
    "    \n",
    "    # Test\n",
    "    model.load_state_dict(torch.load(saved_model_path))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "                    out_test = evaluate(model, Ptest_tensor, Ptest_time_tensor, Ptest_static_tensor, n_classes=n_classes, static=static_info, batch_size=batch_size).numpy()\n",
    "                    ypred = np.argmax(out_test, axis=1)\n",
    "                \n",
    "                    # Adding zero interaction students\n",
    "                    ytest = np.append(ytest, zero_ytest, axis=0)\n",
    "                    ypred = np.append(ypred, np.zeros([1, len(zero_ytest)]))\n",
    "                    \n",
    "\n",
    "                    denoms = np.sum(np.exp(out_test), axis=1).reshape((-1, 1))\n",
    "                    probs = np.exp(out_test) / denoms\n",
    "                    \n",
    "                    # Adding zero interaction students\n",
    "                    probs = np.append(probs, np.zeros([len(zero_ytest), 2]), axis=0)\n",
    "\n",
    "                    acc = np.sum(ytest.ravel() == ypred.ravel()) / ytest.shape[0]\n",
    "                    bac = balanced_accuracy_score(ytest.ravel(), ypred.ravel())\n",
    "                    f1 = f1_score(ytest.ravel(), ypred.ravel())\n",
    "\n",
    "                    auc = roc_auc_score(ytest, probs[:, 1])\n",
    "                    aupr = average_precision_score(ytest, probs[:, 1])\n",
    "\n",
    "                    #print('Testing: AUROC = %.2f | AUPRC = %.2f | Accuracy = %.2f' % (auc * 100, aupr * 100, acc * 100))\n",
    "                    #print('classification report', classification_report(ytest, ypred))\n",
    "                    #print(confusion_matrix(ytest, ypred, labels=list(range(2))))\n",
    "                    results = pd.DataFrame(columns=['course', 'percentile', 'acc', 'bac', 'f1', 'auc', 'auprc'])\n",
    "                    results.loc[0] = [MOOC, percentile, acc, bac, f1, auc, aupr]\n",
    "                    results.to_csv(f\"../raindrop_results/{MOOC}_{percentile}.csv\")\n",
    "                    print(results)\n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26853333",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f9c1be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bafee4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f52e775",
   "metadata": {},
   "source": [
    "DSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65e6c4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MOOCs_list = [\n",
    "#'villesafricaines_002.csv',\n",
    "# 'villesafricaines_003.csv',\n",
    "# 'microcontroleurs_004.csv',\n",
    "# 'dsp_004.csv',\n",
    "# 'hwts_001.csv',\n",
    " 'dsp_001.csv',\n",
    "# 'progfun_002.csv',\n",
    "# 'microcontroleurs_003.csv',\n",
    "# 'geomatique_003.csv',\n",
    "# 'villesafricaines_001.csv',\n",
    "# 'progfun_003.csv',\n",
    "# 'dsp_002.csv',\n",
    "# 'structures_002.csv',\n",
    "# 'initprogcpp_001.csv',\n",
    "# 'analysenumerique_003.csv',\n",
    "# 'microcontroleurs_006.csv',\n",
    "# 'dsp_005.csv',\n",
    "# 'hwts_002.csv',\n",
    "# 'dsp_006.csv',\n",
    "# 'analysenumerique_002.csv',\n",
    "# 'structures_003.csv',\n",
    "# 'microcontroleurs_005.csv',\n",
    "# 'venture_001.csv',\n",
    "# 'analysenumerique_001.csv',\n",
    "# 'cpp_fr_001.csv',\n",
    "# 'structures_001.csv'\n",
    "]\n",
    "MOOCs_list = [i.replace(\"_\", \"-\").split('.')[0] for i in MOOCs_list]\n",
    "\n",
    "dims4 = [\n",
    "# 12,\n",
    "# 12,\n",
    "# 13,\n",
    "# 12,\n",
    "# 12,\n",
    " 6,\n",
    "# 12,\n",
    "# 13,\n",
    "# 12,\n",
    "# 12,\n",
    "# 12,\n",
    "# 12,\n",
    "# 13,\n",
    "# 13,\n",
    "# 6,\n",
    "# 13,\n",
    "# 12,\n",
    "# 12,\n",
    "# 12,\n",
    "# 12,\n",
    "# 13,\n",
    "# 13,\n",
    "# 12,\n",
    "# 6,\n",
    "# 13,\n",
    " 12\n",
    "]\n",
    "\n",
    "dims6 = [\n",
    "# 12,\n",
    "# 12,\n",
    "# 13,\n",
    "# 12,\n",
    "# 12,\n",
    " 6,\n",
    "# 12,\n",
    "# 13,\n",
    "# 12,\n",
    "# 12,\n",
    "# 12,\n",
    "# 12,\n",
    "# 13,\n",
    "# 13,\n",
    "# 6,\n",
    "# 13,\n",
    "# 12,\n",
    "# 12,\n",
    "# 13,\n",
    "# 12,\n",
    "# 13,\n",
    "# 13,\n",
    "# 12,\n",
    "# 6,\n",
    "# 13,\n",
    " 12\n",
    "]\n",
    "dims = {40: dims4, 60: dims6}\n",
    "raindrop_data_path = '/beerslaw/raindrop_data'\n",
    "percentile = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09bb7085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop epochs: 25, Batches/epoch: 24, Total batches: 600\n",
      "[[ 46  82]\n",
      " [  3 125]]\n",
      "Validation: Epoch 0,  val_loss:0.6929, aupr_val: 71.08, auc_val: 87.03\n",
      "**[S] Epoch 0, aupr_val: 71.0777, auc_val: 87.0309 **\n",
      "Validation: Epoch 1,  val_loss:0.6759, aupr_val: 69.92, auc_val: 86.73\n",
      "Validation: Epoch 2,  val_loss:0.6424, aupr_val: 71.72, auc_val: 87.50\n",
      "**[S] Epoch 2, aupr_val: 71.7168, auc_val: 87.4953 **\n",
      "Validation: Epoch 3,  val_loss:0.6166, aupr_val: 77.54, auc_val: 89.39\n",
      "**[S] Epoch 3, aupr_val: 77.5407, auc_val: 89.3904 **\n",
      "Validation: Epoch 4,  val_loss:0.5969, aupr_val: 82.15, auc_val: 91.20\n",
      "**[S] Epoch 4, aupr_val: 82.1465, auc_val: 91.2005 **\n",
      "Validation: Epoch 5,  val_loss:0.5839, aupr_val: 84.65, auc_val: 92.34\n",
      "**[S] Epoch 5, aupr_val: 84.6465, auc_val: 92.3417 **\n",
      "Validation: Epoch 6,  val_loss:0.5617, aupr_val: 85.80, auc_val: 93.07\n",
      "**[S] Epoch 6, aupr_val: 85.7961, auc_val: 93.0650 **\n",
      "Validation: Epoch 7,  val_loss:0.5510, aupr_val: 86.58, auc_val: 93.63\n",
      "**[S] Epoch 7, aupr_val: 86.5849, auc_val: 93.6271 **\n",
      "Validation: Epoch 8,  val_loss:0.5419, aupr_val: 86.91, auc_val: 93.71\n",
      "**[S] Epoch 8, aupr_val: 86.9092, auc_val: 93.7069 **\n",
      "Validation: Epoch 9,  val_loss:0.5323, aupr_val: 87.36, auc_val: 93.90\n",
      "**[S] Epoch 9, aupr_val: 87.3598, auc_val: 93.9022 **\n",
      "Validation: Epoch 10,  val_loss:0.5289, aupr_val: 87.52, auc_val: 93.99\n",
      "**[S] Epoch 10, aupr_val: 87.5183, auc_val: 93.9854 **\n",
      "Validation: Epoch 11,  val_loss:0.5329, aupr_val: 87.40, auc_val: 93.93\n",
      "Validation: Epoch 12,  val_loss:0.5306, aupr_val: 87.45, auc_val: 94.01\n",
      "Epoch 00013: reducing learning rate of group 0 to 1.0000e-05.\n",
      "**[S] Epoch 12, aupr_val: 87.4478, auc_val: 94.0143 **\n",
      "Validation: Epoch 13,  val_loss:0.5233, aupr_val: 87.42, auc_val: 94.03\n",
      "**[S] Epoch 13, aupr_val: 87.4151, auc_val: 94.0329 **\n",
      "Validation: Epoch 14,  val_loss:0.5233, aupr_val: 87.41, auc_val: 94.03\n",
      "Epoch 00015: reducing learning rate of group 0 to 1.0000e-06.\n",
      "**[S] Epoch 14, aupr_val: 87.4078, auc_val: 94.0329 **\n",
      "Validation: Epoch 15,  val_loss:0.5235, aupr_val: 87.41, auc_val: 94.03\n",
      "**[S] Epoch 15, aupr_val: 87.4118, auc_val: 94.0346 **\n",
      "Validation: Epoch 16,  val_loss:0.5235, aupr_val: 87.43, auc_val: 94.04\n",
      "Epoch 00017: reducing learning rate of group 0 to 1.0000e-07.\n",
      "**[S] Epoch 16, aupr_val: 87.4303, auc_val: 94.0414 **\n",
      "Validation: Epoch 17,  val_loss:0.5235, aupr_val: 87.43, auc_val: 94.04\n",
      "Validation: Epoch 18,  val_loss:0.5235, aupr_val: 87.43, auc_val: 94.04\n",
      "Epoch 00019: reducing learning rate of group 0 to 1.0000e-08.\n",
      "**[S] Epoch 18, aupr_val: 87.4314, auc_val: 94.0431 **\n",
      "Validation: Epoch 19,  val_loss:0.5235, aupr_val: 87.43, auc_val: 94.04\n",
      "Validation: Epoch 20,  val_loss:0.5235, aupr_val: 87.43, auc_val: 94.04\n",
      "Validation: Epoch 21,  val_loss:0.5235, aupr_val: 87.43, auc_val: 94.04\n",
      "Validation: Epoch 22,  val_loss:0.5235, aupr_val: 87.43, auc_val: 94.04\n",
      "Validation: Epoch 23,  val_loss:0.5235, aupr_val: 87.43, auc_val: 94.04\n",
      "[[118  10]\n",
      " [ 15 113]]\n",
      "Validation: Epoch 24,  val_loss:0.5235, aupr_val: 87.43, auc_val: 94.04\n",
      "Total Time elapsed: 16.950 mins\n",
      "    course  percentile       acc       bac        f1       auc     auprc\n",
      "0  dsp-001          40  0.859431  0.856049  0.765579  0.924422  0.805626\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "for MOOC_idx, MOOC in enumerate(MOOCs_list):\n",
    "    \n",
    "    d_inp = dims[percentile][MOOC_idx]\n",
    "    max_len = 1000\n",
    "    n_classes = 2\n",
    "    global_structure = torch.ones(d_inp, d_inp)\n",
    "    saved_model_path = f\"../models/n1_mlp2_best_raindrop_{MOOC}_{percentile}.pt\"\n",
    "    batch_size = 256\n",
    "    n_splits = 1\n",
    "\n",
    "    d_static = 9\n",
    "    static_info = 1\n",
    "    d_ob = 4\n",
    "    d_model = d_inp * d_ob\n",
    "    nhid = 2 * d_model\n",
    "    nlayers = 1\n",
    "    nhead = 2\n",
    "    dropout = 0.2\n",
    "    sensor_wise_mask = False\n",
    "    MAX = 100\n",
    "    aggreg = 'mean'\n",
    "    n_runs=1\n",
    "\n",
    "    learning_rate = 0.0001\n",
    "    num_epochs = 25\n",
    "\n",
    "    acc_arr = np.zeros((n_splits, n_runs))\n",
    "    auprc_arr = np.zeros((n_splits, n_runs))\n",
    "    auroc_arr = np.zeros((n_splits, n_runs))\n",
    "    precision_arr = np.zeros((n_splits, n_runs))\n",
    "    recall_arr = np.zeros((n_splits, n_runs))\n",
    "    F1_arr = np.zeros((n_splits, n_runs))\n",
    "\n",
    "\n",
    "    base_path = '/beerslaw/raindrop_data/prep_data'\n",
    "    Pdict_list = np.load(os.path.join(base_path, f\"{MOOC}_{percentile}_data_hard_fail.npy\"), allow_pickle=True)\n",
    "    arr_outcomes = np.load(os.path.join(base_path, f\"{MOOC}_{percentile}_y_hard_fail.npy\"), allow_pickle=True)\n",
    "\n",
    "    #Ptrain, Ptest, ytrain, ytest = train_test_split(Pdict_list, arr_outcomes, test_size=0.1, random_state=1)\n",
    "    #Ptrain, Pval, ytrain, yval = train_test_split(Ptrain, ytrain, test_size=1/9, random_state=1)\n",
    "    args_train, args_val, args_test = np.load(os.path.join(raindrop_data_path, \n",
    "                                                                   'split_args', f\"split_{MOOC.replace('-', '_')}.npy\"),\n",
    "                                                     allow_pickle=True)\n",
    "    Ptrain = Pdict_list[args_train]\n",
    "    Pval = Pdict_list[args_val]\n",
    "    Ptest = Pdict_list[args_test]\n",
    "    ytrain = arr_outcomes[args_train, :]\n",
    "    yval = arr_outcomes[args_val, :]\n",
    "    ytest = arr_outcomes[args_test, :]\n",
    "    \n",
    "    \n",
    "    zero_indices = [i for i, item in enumerate(Ptrain) if item['length'] == 0]\n",
    "    #zero_Ptrain = Ptrain[zero_indices]\n",
    "    Ptrain = np.delete(Ptrain, zero_indices, axis=0)\n",
    "    ytrain = np.delete(ytrain, zero_indices, axis=0)\n",
    "\n",
    "    zero_indices = [i for i, item in enumerate(Pval) if item['length'] == 0]\n",
    "    #zero_Ptrain = Ptrain[zero_indices]\n",
    "    Pval = np.delete(Pval, zero_indices, axis=0)\n",
    "    yval = np.delete(yval, zero_indices, axis=0)\n",
    "\n",
    "    zero_indices = [i for i, item in enumerate(Ptest) if item['length'] == 0]\n",
    "    zero_ytest = ytest[zero_indices]\n",
    "    Ptest = np.delete(Ptest, zero_indices, axis=0)  \n",
    "    ytest = np.delete(ytest, zero_indices, axis=0)\n",
    "    \n",
    "\n",
    "    T, F = Ptrain[0]['arr'].shape\n",
    "    D = len(Ptrain[0]['extended_static'])\n",
    "\n",
    "    Ptrain_tensor = np.zeros((len(Ptrain), T, F))\n",
    "    Ptrain_static_tensor = np.zeros((len(Ptrain), D))\n",
    "\n",
    "    for i in range(len(Ptrain)):\n",
    "        Ptrain_tensor[i] = Ptrain[i]['arr']\n",
    "        Ptrain_static_tensor[i] = Ptrain[i]['extended_static']\n",
    "\n",
    "    mf, stdf = getStats(Ptrain_tensor)\n",
    "    ms, ss = getStats_static(Ptrain_static_tensor, dataset='P12')\n",
    "\n",
    "    Ptrain_tensor, Ptrain_static_tensor, Ptrain_time_tensor, ytrain_tensor = tensorize_normalize(Ptrain, ytrain, mf,\n",
    "                                                                                                 stdf, ms, ss)\n",
    "    Pval_tensor, Pval_static_tensor, Pval_time_tensor, yval_tensor = tensorize_normalize(Pval, yval, mf, stdf, ms, ss)\n",
    "\n",
    "    Ptest_tensor, Ptest_static_tensor, Ptest_time_tensor, ytest_tensor = tensorize_normalize(Ptest, ytest, mf, stdf, ms, ss)\n",
    "\n",
    "    Ptrain_tensor = Ptrain_tensor.permute(1, 0, 2)\n",
    "    Pval_tensor = Pval_tensor.permute(1, 0, 2)\n",
    "    Ptest_tensor = Ptest_tensor.permute(1, 0, 2)\n",
    "\n",
    "    Ptrain_time_tensor = Ptrain_time_tensor.squeeze(2).permute(1, 0)\n",
    "    Pval_time_tensor = Pval_time_tensor.squeeze(2).permute(1, 0)\n",
    "    Ptest_time_tensor = Ptest_time_tensor.squeeze(2).permute(1, 0)\n",
    "    \n",
    "    model = Raindrop_v2(d_inp, d_model, nhead, nhid, nlayers, dropout, max_len,\n",
    "                                        d_static, MAX, 0.5, aggreg, n_classes, global_structure,\n",
    "                                        sensor_wise_mask=sensor_wise_mask)\n",
    "    #torch.save(model, '../models/raw_raindrop_model.pt')\n",
    "\n",
    "    model = model.cuda()\n",
    "    criterion = torch.nn.CrossEntropyLoss().cuda()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.1,\n",
    "                                                           patience=1, threshold=0.0001, threshold_mode='rel',\n",
    "                                                           cooldown=0, min_lr=1e-8, eps=1e-08, verbose=True)\n",
    "\n",
    "\n",
    "    idx_0 = np.where(ytrain == 0)[0]\n",
    "    idx_1 = np.where(ytrain == 1)[0]\n",
    "\n",
    "    n0, n1 = len(idx_0), len(idx_1)\n",
    "    expanded_idx_1 = np.concatenate([idx_1, idx_1, idx_1], axis=0)\n",
    "    expanded_n1 = len(expanded_idx_1)\n",
    "\n",
    "    K0 = n0 // int(batch_size / 2)\n",
    "    K1 = expanded_n1 // int(batch_size / 2)\n",
    "    n_batches = np.min([K0, K1])\n",
    "\n",
    "    best_aupr_val = best_auc_val = 0.0\n",
    "    best_loss_val = 100.0\n",
    "\n",
    "    print('Stop epochs: %d, Batches/epoch: %d, Total batches: %d' % (num_epochs, n_batches, num_epochs * n_batches))\n",
    "    start = time.time()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "\n",
    "\n",
    "        np.random.shuffle(expanded_idx_1)\n",
    "        I1 = expanded_idx_1\n",
    "        np.random.shuffle(idx_0)\n",
    "        I0 = idx_0\n",
    "\n",
    "        for n in range(n_batches):\n",
    "\n",
    "            idx0_batch = I0[n * int(batch_size / 2):(n + 1) * int(batch_size / 2)]\n",
    "            idx1_batch = I1[n * int(batch_size / 2):(n + 1) * int(batch_size / 2)]\n",
    "            idx = np.concatenate([idx0_batch, idx1_batch], axis=0)\n",
    "\n",
    "\n",
    "            P, Ptime, Pstatic, y = Ptrain_tensor[:, idx, :].cuda(), Ptrain_time_tensor[:, idx].cuda(), \\\n",
    "                                   Ptrain_static_tensor[idx].cuda(), ytrain_tensor[idx].cuda()\n",
    "\n",
    "\n",
    "            lengths = torch.sum(Ptime > 0, dim=0)\n",
    "            #outputs, local_structure_regularization, _ = model.forward(P, Pstatic, Ptime, lengths)\n",
    "            outputs = model.forward(P, Pstatic, Ptime, lengths)\n",
    "            outputs = torch.nan_to_num(outputs)\n",
    "            \n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "        train_probs = torch.squeeze(torch.sigmoid(outputs))\n",
    "        train_probs = train_probs.cpu().detach().numpy()\n",
    "        train_y = y.cpu().detach().numpy()\n",
    "        train_auroc = roc_auc_score(train_y, train_probs[:, 1])\n",
    "        train_auprc = average_precision_score(train_y, train_probs[:, 1])\n",
    "\n",
    "\n",
    "        if epoch == 0 or epoch == num_epochs - 1:\n",
    "            print(confusion_matrix(train_y, np.argmax(train_probs, axis=1), labels=[0, 1]))\n",
    "\n",
    "        \"\"\"Validation\"\"\"\n",
    "        model.eval()\n",
    "        if epoch == 0 or epoch % 1 == 0:\n",
    "            with torch.no_grad():\n",
    "                #out_val = evaluate_standard(model, Pval_tensor, Pval_time_tensor, Pval_static_tensor, static=static_info)\n",
    "                #Changed into:\n",
    "                out_val = evaluate(model, Pval_tensor, Pval_time_tensor, Pval_static_tensor,batch_size=batch_size, static=static_info)\n",
    "                out_val = torch.squeeze(torch.sigmoid(out_val))\n",
    "                out_val = out_val.detach().cpu().numpy()\n",
    "\n",
    "                val_loss = criterion(torch.from_numpy(out_val), torch.from_numpy(yval.squeeze(1)).long())\n",
    "\n",
    "\n",
    "                auc_val = roc_auc_score(yval, out_val[:, 1])\n",
    "                aupr_val = average_precision_score(yval, out_val[:, 1])\n",
    "\n",
    "                print(\"Validation: Epoch %d,  val_loss:%.4f, aupr_val: %.2f, auc_val: %.2f\" % (epoch,\n",
    "                                                                                                val_loss.item(),\n",
    "                                                                                                aupr_val * 100,\n",
    "                                                                                                auc_val * 100))\n",
    "\n",
    "                scheduler.step(aupr_val)\n",
    "                if auc_val > best_auc_val:\n",
    "                    best_auc_val = auc_val\n",
    "                    print(\n",
    "                        \"**[S] Epoch %d, aupr_val: %.4f, auc_val: %.4f **\" % (\n",
    "                        epoch, aupr_val * 100, auc_val * 100))\n",
    "                    torch.save(model.state_dict(), saved_model_path)\n",
    "\n",
    "    end = time.time()\n",
    "    time_elapsed = end - start\n",
    "    print('Total Time elapsed: %.3f mins' % (time_elapsed / 60.0))\n",
    "    \n",
    "    # Test\n",
    "    model.load_state_dict(torch.load(saved_model_path))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "                    out_test = evaluate(model, Ptest_tensor, Ptest_time_tensor, Ptest_static_tensor, n_classes=n_classes, static=static_info, batch_size=batch_size).numpy()\n",
    "                    ypred = np.argmax(out_test, axis=1)\n",
    "                \n",
    "                    # Adding zero interaction students\n",
    "                    ytest = np.append(ytest, zero_ytest, axis=0)\n",
    "                    ypred = np.append(ypred, np.zeros([1, len(zero_ytest)]))\n",
    "                    \n",
    "\n",
    "                    denoms = np.sum(np.exp(out_test), axis=1).reshape((-1, 1))\n",
    "                    probs = np.exp(out_test) / denoms\n",
    "                    \n",
    "                    # Adding zero interaction students\n",
    "                    probs = np.append(probs, np.zeros([len(zero_ytest), 2]), axis=0)\n",
    "\n",
    "                    acc = np.sum(ytest.ravel() == ypred.ravel()) / ytest.shape[0]\n",
    "                    bac = balanced_accuracy_score(ytest.ravel(), ypred.ravel())\n",
    "                    f1 = f1_score(ytest.ravel(), ypred.ravel())\n",
    "\n",
    "                    auc = roc_auc_score(ytest, probs[:, 1])\n",
    "                    aupr = average_precision_score(ytest, probs[:, 1])\n",
    "\n",
    "                    #print('Testing: AUROC = %.2f | AUPRC = %.2f | Accuracy = %.2f' % (auc * 100, aupr * 100, acc * 100))\n",
    "                    #print('classification report', classification_report(ytest, ypred))\n",
    "                    #print(confusion_matrix(ytest, ypred, labels=list(range(2))))\n",
    "                    results = pd.DataFrame(columns=['course', 'percentile', 'acc', 'bac', 'f1', 'auc', 'auprc'])\n",
    "                    results.loc[0] = [MOOC, percentile, acc, bac, f1, auc, aupr]\n",
    "                    results.to_csv(f\"../raindrop_results/{MOOC}_{percentile}.csv\")\n",
    "                    print(results)\n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba70571c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359b8f81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f91a0ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009f9cc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b24df1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f213c2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0322d313",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e568f3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(73,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ptrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b27cc74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([43, 26, 21, 39, 38, 17, 66,  4, 42, 37, 54, 13, 71,  6, 57, 12, 35,\n",
       "       45, 14, 70, 18, 30, 47])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8195c61a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  5,  7,  8,  9, 10, 11, 15, 16, 19, 20, 22, 23, 24,\n",
       "       25, 27, 28, 29, 31, 32, 33, 34, 36, 40, 41, 44, 46, 48, 49, 50, 51,\n",
       "       52, 53, 55, 56, 58, 59, 60, 61, 62, 63, 64, 65, 67, 68, 69, 72])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d535fac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n0 // int(batch_size / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "247ceafa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca288afb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a3acdec",
   "metadata": {},
   "source": [
    "# Just for 3 courses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67283b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MOOCs_list = [\n",
    "#'villesafricaines_002.csv',\n",
    "# 'villesafricaines_003.csv',\n",
    "# 'microcontroleurs_004.csv',\n",
    "# 'dsp_004.csv',\n",
    "# 'hwts_001.csv',\n",
    " 'dsp_001.csv',\n",
    "# 'progfun_002.csv',\n",
    "# 'microcontroleurs_003.csv',\n",
    "# 'geomatique_003.csv',\n",
    "# 'villesafricaines_001.csv',\n",
    "# 'progfun_003.csv',\n",
    "# 'dsp_002.csv',\n",
    "# 'structures_002.csv',\n",
    "# 'initprogcpp_001.csv',\n",
    "# 'analysenumerique_003.csv',\n",
    "# 'microcontroleurs_006.csv',\n",
    "# 'dsp_005.csv',\n",
    "# 'hwts_002.csv',\n",
    "# 'dsp_006.csv',\n",
    "# 'analysenumerique_002.csv',\n",
    "# 'structures_003.csv',\n",
    "# 'microcontroleurs_005.csv',\n",
    "# 'venture_001.csv',\n",
    "# 'analysenumerique_001.csv',\n",
    "# 'cpp_fr_001.csv',\n",
    "# 'structures_001.csv'\n",
    "]\n",
    "MOOCs_list = [i.replace(\"_\", \"-\").split('.')[0] for i in MOOCs_list]\n",
    "\n",
    "dims4 = [\n",
    "# 12,\n",
    "# 12,\n",
    "# 13,\n",
    "# 12,\n",
    "# 12,\n",
    " 6,\n",
    "# 12,\n",
    "# 13,\n",
    "# 12,\n",
    "# 12,\n",
    "# 12,\n",
    "# 12,\n",
    "# 13,\n",
    "# 13,\n",
    "# 6,\n",
    "# 13,\n",
    "# 12,\n",
    "# 12,\n",
    "# 12,\n",
    "# 12,\n",
    "# 13,\n",
    "# 13,\n",
    "# 12,\n",
    "# 6,\n",
    "# 13,\n",
    "# 12\n",
    "]\n",
    "\n",
    "dims6 = [\n",
    "# 12,\n",
    "# 12,\n",
    "# 13,\n",
    "# 12,\n",
    "# 12,\n",
    " 6,\n",
    "# 12,\n",
    "# 13,\n",
    "# 12,\n",
    "# 12,\n",
    "# 12,\n",
    "# 12,\n",
    "# 13,\n",
    "# 13,\n",
    "# 6,\n",
    "# 13,\n",
    "# 12,\n",
    "# 12,\n",
    "# 13,\n",
    "# 12,\n",
    "# 13,\n",
    "# 13,\n",
    "# 12,\n",
    "# 6,\n",
    "# 13,\n",
    "# 12\n",
    "]\n",
    "dims = {40: dims4, 60: dims6}\n",
    "raindrop_data_path = '/beerslaw/raindrop_data'\n",
    "percentile = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d813e3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for MOOC_idx, MOOC in enumerate(MOOCs_list):\n",
    "    \n",
    "    d_inp = dims[percentile][MOOC_idx]\n",
    "    max_len = 1000\n",
    "    n_classes = 2\n",
    "    global_structure = torch.ones(d_inp, d_inp)\n",
    "    saved_model_path = f\"../models/best_raindrop_{MOOC}_{percentile}.pt\"\n",
    "    batch_size = 256\n",
    "    n_splits = 1\n",
    "\n",
    "    d_static = 9\n",
    "    static_info = 1\n",
    "    d_ob = 4\n",
    "    d_model = d_inp * d_ob\n",
    "    nhid = 2 * d_model\n",
    "    nlayers = 2\n",
    "    nhead = 2\n",
    "    dropout = 0.2\n",
    "    sensor_wise_mask = False\n",
    "    MAX = 100\n",
    "    aggreg = 'mean'\n",
    "    n_runs=1\n",
    "\n",
    "    learning_rate = 0.0001\n",
    "    num_epochs = 30\n",
    "\n",
    "    acc_arr = np.zeros((n_splits, n_runs))\n",
    "    auprc_arr = np.zeros((n_splits, n_runs))\n",
    "    auroc_arr = np.zeros((n_splits, n_runs))\n",
    "    precision_arr = np.zeros((n_splits, n_runs))\n",
    "    recall_arr = np.zeros((n_splits, n_runs))\n",
    "    F1_arr = np.zeros((n_splits, n_runs))\n",
    "\n",
    "\n",
    "    base_path = '/beerslaw/raindrop_data/prep_data'\n",
    "    Pdict_list = np.load(os.path.join(base_path, f\"{MOOC}_{percentile}_data_hard_fail.npy\"), allow_pickle=True)\n",
    "    arr_outcomes = np.load(os.path.join(base_path, f\"{MOOC}_{percentile}_y_hard_fail.npy\"), allow_pickle=True)\n",
    "\n",
    "    #Ptrain, Ptest, ytrain, ytest = train_test_split(Pdict_list, arr_outcomes, test_size=0.1, random_state=1)\n",
    "    #Ptrain, Pval, ytrain, yval = train_test_split(Ptrain, ytrain, test_size=1/9, random_state=1)\n",
    "    args_train, args_val, args_test = np.load(os.path.join(raindrop_data_path, \n",
    "                                                                   'split_args', f\"split_{MOOC.replace('-', '_')}.npy\"),\n",
    "                                                     allow_pickle=True)\n",
    "    Ptrain = Pdict_list[args_train]\n",
    "    Pval = Pdict_list[args_val]\n",
    "    Ptest = Pdict_list[args_test]\n",
    "    ytrain = arr_outcomes[args_train, :]\n",
    "    yval = arr_outcomes[args_val, :]\n",
    "    ytest = arr_outcomes[args_test, :]\n",
    "    \n",
    "    \n",
    "    zero_indices = [i for i, item in enumerate(Ptrain) if item['length'] == 0]\n",
    "    #zero_Ptrain = Ptrain[zero_indices]\n",
    "    Ptrain = np.delete(Ptrain, zero_indices, axis=0)\n",
    "    ytrain = np.delete(ytrain, zero_indices, axis=0)\n",
    "\n",
    "    zero_indices = [i for i, item in enumerate(Pval) if item['length'] == 0]\n",
    "    #zero_Ptrain = Ptrain[zero_indices]\n",
    "    Pval = np.delete(Pval, zero_indices, axis=0)\n",
    "    yval = np.delete(yval, zero_indices, axis=0)\n",
    "\n",
    "    zero_indices = [i for i, item in enumerate(Ptest) if item['length'] == 0]\n",
    "    zero_ytest = ytest[zero_indices]\n",
    "    Ptest = np.delete(Ptest, zero_indices, axis=0)  \n",
    "    ytest = np.delete(ytest, zero_indices, axis=0)\n",
    "    \n",
    "\n",
    "    T, F = Ptrain[0]['arr'].shape\n",
    "    D = len(Ptrain[0]['extended_static'])\n",
    "\n",
    "    Ptrain_tensor = np.zeros((len(Ptrain), T, F))\n",
    "    Ptrain_static_tensor = np.zeros((len(Ptrain), D))\n",
    "\n",
    "    for i in range(len(Ptrain)):\n",
    "        Ptrain_tensor[i] = Ptrain[i]['arr']\n",
    "        Ptrain_static_tensor[i] = Ptrain[i]['extended_static']\n",
    "\n",
    "    mf, stdf = getStats(Ptrain_tensor)\n",
    "    ms, ss = getStats_static(Ptrain_static_tensor, dataset='P12')\n",
    "\n",
    "    Ptrain_tensor, Ptrain_static_tensor, Ptrain_time_tensor, ytrain_tensor = tensorize_normalize(Ptrain, ytrain, mf,\n",
    "                                                                                                 stdf, ms, ss)\n",
    "    Pval_tensor, Pval_static_tensor, Pval_time_tensor, yval_tensor = tensorize_normalize(Pval, yval, mf, stdf, ms, ss)\n",
    "\n",
    "    Ptest_tensor, Ptest_static_tensor, Ptest_time_tensor, ytest_tensor = tensorize_normalize(Ptest, ytest, mf, stdf, ms, ss)\n",
    "\n",
    "    Ptrain_tensor = Ptrain_tensor.permute(1, 0, 2)\n",
    "    Pval_tensor = Pval_tensor.permute(1, 0, 2)\n",
    "    Ptest_tensor = Ptest_tensor.permute(1, 0, 2)\n",
    "\n",
    "    Ptrain_time_tensor = Ptrain_time_tensor.squeeze(2).permute(1, 0)\n",
    "    Pval_time_tensor = Pval_time_tensor.squeeze(2).permute(1, 0)\n",
    "    Ptest_time_tensor = Ptest_time_tensor.squeeze(2).permute(1, 0)\n",
    "    \n",
    "    model = Raindrop_v2(d_inp, d_model, nhead, nhid, nlayers, dropout, max_len,\n",
    "                                        d_static, MAX, 0.5, aggreg, n_classes, global_structure,\n",
    "                                        sensor_wise_mask=sensor_wise_mask)\n",
    "    #torch.save(model, '../models/raw_raindrop_model.pt')\n",
    "\n",
    "    model = model.cuda()\n",
    "    criterion = torch.nn.CrossEntropyLoss().cuda()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.1,\n",
    "                                                           patience=1, threshold=0.0001, threshold_mode='rel',\n",
    "                                                           cooldown=0, min_lr=1e-8, eps=1e-08, verbose=True)\n",
    "\n",
    "\n",
    "    idx_0 = np.where(ytrain == 0)[0]\n",
    "    idx_1 = np.where(ytrain == 1)[0]\n",
    "\n",
    "    n0, n1 = len(idx_0), len(idx_1)\n",
    "    expanded_idx_1 = np.concatenate([idx_1, idx_1, idx_1], axis=0)\n",
    "    expanded_n1 = len(expanded_idx_1)\n",
    "\n",
    "    K0 = n0 // int(batch_size / 2)\n",
    "    K1 = expanded_n1 // int(batch_size / 2)\n",
    "    n_batches = np.min([K0, K1])\n",
    "\n",
    "    best_aupr_val = best_auc_val = 0.0\n",
    "    best_loss_val = 100.0\n",
    "\n",
    "    print('Stop epochs: %d, Batches/epoch: %d, Total batches: %d' % (num_epochs, n_batches, num_epochs * n_batches))\n",
    "    start = time.time()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "\n",
    "\n",
    "        np.random.shuffle(expanded_idx_1)\n",
    "        I1 = expanded_idx_1\n",
    "        np.random.shuffle(idx_0)\n",
    "        I0 = idx_0\n",
    "\n",
    "        for n in range(n_batches):\n",
    "\n",
    "            idx0_batch = I0[n * int(batch_size / 2):(n + 1) * int(batch_size / 2)]\n",
    "            idx1_batch = I1[n * int(batch_size / 2):(n + 1) * int(batch_size / 2)]\n",
    "            idx = np.concatenate([idx0_batch, idx1_batch], axis=0)\n",
    "\n",
    "\n",
    "            P, Ptime, Pstatic, y = Ptrain_tensor[:, idx, :].cuda(), Ptrain_time_tensor[:, idx].cuda(), \\\n",
    "                                   Ptrain_static_tensor[idx].cuda(), ytrain_tensor[idx].cuda()\n",
    "\n",
    "\n",
    "            lengths = torch.sum(Ptime > 0, dim=0)\n",
    "            #outputs, local_structure_regularization, _ = model.forward(P, Pstatic, Ptime, lengths)\n",
    "            outputs = model.forward(P, Pstatic, Ptime, lengths)\n",
    "            outputs = torch.nan_to_num(outputs)\n",
    "            \n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "        train_probs = torch.squeeze(torch.sigmoid(outputs))\n",
    "        train_probs = train_probs.cpu().detach().numpy()\n",
    "        train_y = y.cpu().detach().numpy()\n",
    "        train_auroc = roc_auc_score(train_y, train_probs[:, 1])\n",
    "        train_auprc = average_precision_score(train_y, train_probs[:, 1])\n",
    "\n",
    "\n",
    "        if epoch == 0 or epoch == num_epochs - 1:\n",
    "            print(confusion_matrix(train_y, np.argmax(train_probs, axis=1), labels=[0, 1]))\n",
    "\n",
    "        \"\"\"Validation\"\"\"\n",
    "        model.eval()\n",
    "        if epoch == 0 or epoch % 1 == 0:\n",
    "            with torch.no_grad():\n",
    "                #out_val = evaluate_standard(model, Pval_tensor, Pval_time_tensor, Pval_static_tensor, static=static_info)\n",
    "                #Changed into:\n",
    "                out_val = evaluate(model, Pval_tensor, Pval_time_tensor, Pval_static_tensor,batch_size=batch_size, static=static_info)\n",
    "                out_val = torch.squeeze(torch.sigmoid(out_val))\n",
    "                out_val = out_val.detach().cpu().numpy()\n",
    "\n",
    "                val_loss = criterion(torch.from_numpy(out_val), torch.from_numpy(yval.squeeze(1)).long())\n",
    "\n",
    "\n",
    "                auc_val = roc_auc_score(yval, out_val[:, 1])\n",
    "                aupr_val = average_precision_score(yval, out_val[:, 1])\n",
    "\n",
    "                print(\"Validation: Epoch %d,  val_loss:%.4f, aupr_val: %.2f, auc_val: %.2f\" % (epoch,\n",
    "                                                                                                val_loss.item(),\n",
    "                                                                                                aupr_val * 100,\n",
    "                                                                                                auc_val * 100))\n",
    "\n",
    "                scheduler.step(aupr_val)\n",
    "                if auc_val > best_auc_val:\n",
    "                    best_auc_val = auc_val\n",
    "                    print(\n",
    "                        \"**[S] Epoch %d, aupr_val: %.4f, auc_val: %.4f **\" % (\n",
    "                        epoch, aupr_val * 100, auc_val * 100))\n",
    "                    torch.save(model.state_dict(), saved_model_path)\n",
    "\n",
    "    end = time.time()\n",
    "    time_elapsed = end - start\n",
    "    print('Total Time elapsed: %.3f mins' % (time_elapsed / 60.0))\n",
    "    \n",
    "    # Test\n",
    "    model.load_state_dict(torch.load(saved_model_path))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "                    out_test = evaluate(model, Ptest_tensor, Ptest_time_tensor, Ptest_static_tensor, n_classes=n_classes, static=static_info, batch_size=batch_size).numpy()\n",
    "                    ypred = np.argmax(out_test, axis=1)\n",
    "                \n",
    "                    # Adding zero interaction students\n",
    "                    ytest = np.append(ytest, zero_ytest, axis=0)\n",
    "                    ypred = np.append(ypred, np.zeros([1, len(zero_ytest)]))\n",
    "                    \n",
    "\n",
    "                    denoms = np.sum(np.exp(out_test), axis=1).reshape((-1, 1))\n",
    "                    probs = np.exp(out_test) / denoms\n",
    "                    \n",
    "                    # Adding zero interaction students\n",
    "                    probs = np.append(probs, np.zeros([len(zero_ytest), 2]), axis=0)\n",
    "\n",
    "                    acc = np.sum(ytest.ravel() == ypred.ravel()) / ytest.shape[0]\n",
    "                    bac = balanced_accuracy_score(ytest.ravel(), ypred.ravel())\n",
    "                    f1 = f1_score(ytest.ravel(), ypred.ravel())\n",
    "\n",
    "                    auc = roc_auc_score(ytest, probs[:, 1])\n",
    "                    aupr = average_precision_score(ytest, probs[:, 1])\n",
    "\n",
    "                    #print('Testing: AUROC = %.2f | AUPRC = %.2f | Accuracy = %.2f' % (auc * 100, aupr * 100, acc * 100))\n",
    "                    #print('classification report', classification_report(ytest, ypred))\n",
    "                    #print(confusion_matrix(ytest, ypred, labels=list(range(2))))\n",
    "                    results = pd.DataFrame(columns=['course', 'percentile', 'acc', 'bac', 'f1', 'auc', 'auprc'])\n",
    "                    results.loc[0] = [MOOC, percentile, acc, bac, f1, auc, aupr]\n",
    "                    results.to_csv(f\"../raindrop_results/{MOOC}_{percentile}.csv\")\n",
    "                    print(results)\n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0679f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5ed4e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e050040",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2334d854",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aaa10fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f27653",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efff5a7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3c599b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1ff61d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1022f80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff26659",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872a477e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
